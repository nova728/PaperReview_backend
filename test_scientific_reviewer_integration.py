#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯• Hammer_review_backend ä¸ ScientificReviewer-7B æ¨¡å‹çš„é›†æˆ
"""

import requests
import json
import time

def test_backend_health():
    """æµ‹è¯•åç«¯å¥åº·çŠ¶æ€"""
    try:
        response = requests.get('http://localhost:5000/api/papers/health')
        if response.status_code == 200:
            print("âœ… åç«¯æœåŠ¡æ­£å¸¸è¿è¡Œ")
            return True
        else:
            print(f"âŒ åç«¯æœåŠ¡å¼‚å¸¸ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ°åç«¯æœåŠ¡: {str(e)}")
        return False

def test_scientific_reviewer_request():
    """æµ‹è¯•ScientificReviewer-7Bè¯·æ±‚æ ¼å¼"""
    
    # æ„å»ºæµ‹è¯•è¯·æ±‚
    test_request = {
        "paper_json": {
            "title": "Quantifying the Influence of Evaluation Aspects on Long-Form Response Assessment",
            "abstract": "Evaluating the outputs of large language models (LLMs) on long-form generative tasks remains challenging. While fine-grained, aspectwise evaluations provide valuable diagnostic information, they are difficult to design exhaustively, and each aspect's contribution to the overall acceptability of an answer is unclear. In this study, we propose a method to compute an overall quality score as a weighted average of three key aspects: factuality, informativeness, and formality. This approach achieves stronger correlations with human judgments compared to previous metrics. Our analysis identifies factuality as the most predictive aspect of overall quality. Additionally, we release a dataset of 1.2k long-form QA answers annotated with both absolute judgments and relative preferences in overall and aspect-wise schemes, to aid future research in evaluation practices."
        },
        "temperature": 0.7,
        "max_tokens": 1024,
        "use_chunking": True,
        "include_authors": False,
        "stream": False
    }
    
    try:
        print("ğŸš€ æµ‹è¯•ScientificReviewer-7Bé›†æˆ...")
        start_time = time.time()
        
        response = requests.post(
            'http://localhost:5000/api/papers/peer-review',
            headers={'Content-Type': 'application/json'},
            json=test_request,
            timeout=300
        )
        
        end_time = time.time()
        request_time = end_time - start_time
        
        print(f"â±ï¸  è¯·æ±‚è€—æ—¶: {request_time:.2f}ç§’")
        
        if response.status_code == 200:
            result = response.json()
            print("âœ… é›†æˆæµ‹è¯•æˆåŠŸï¼")
            print(f"ğŸ“Š å“åº”ç»Ÿè®¡: {result.get('stats', {})}")
            
            if result.get('success'):
                review_content = result.get('response', '')
                print(f"ğŸ“ è¯„å®¡å†…å®¹é•¿åº¦: {len(review_content)} å­—ç¬¦")
                print("ğŸ“„ è¯„å®¡å†…å®¹é¢„è§ˆ:")
                print("-" * 50)
                print(review_content[:500] + "..." if len(review_content) > 500 else review_content)
                print("-" * 50)
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
        else:
            print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            print("é”™è¯¯ä¿¡æ¯:", response.text)
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")

def test_direct_vllm_api():
    """ç›´æ¥æµ‹è¯•vLLM API"""
    print("\nğŸ”¬ ç›´æ¥æµ‹è¯•vLLM API...")
    
    direct_request = {
        "model": "scientific-reviewer-7b",
        "messages": [
            {
                "role": "user",
                "content": "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ çš„åŠŸèƒ½ã€‚"
            }
        ],
        "temperature": 0.7,
        "max_tokens": 200
    }
    
    try:
        response = requests.post(
            'http://localhost:8000/v1/chat/completions',
            headers={'Content-Type': 'application/json'},
            json=direct_request,
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content']
            print("âœ… vLLM APIç›´æ¥æµ‹è¯•æˆåŠŸ:")
            print(content)
        else:
            print(f"âŒ vLLM APIæµ‹è¯•å¤±è´¥: {response.status_code}")
            
    except Exception as e:
        print(f"âŒ vLLM APIæµ‹è¯•é”™è¯¯: {str(e)}")

if __name__ == "__main__":
    print("ğŸ”¬ Hammer_review_backend é›†æˆæµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•åç«¯å¥åº·çŠ¶æ€
    if test_backend_health():
        # æµ‹è¯•ScientificReviewer-7Bé›†æˆ
        test_scientific_reviewer_request()
    
    # ç›´æ¥æµ‹è¯•vLLM API
    test_direct_vllm_api()
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼") 