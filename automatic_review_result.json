{"paper_length":35597,"processing_time":15.752795219421387,"result":{"aspects":["Novelty","Contribution of the research","Clarity and Presentation","Theoretical Soundness","Experimental Validation","Reproducibility"],"content":"**Summary:**\nThis paper explores the evaluation of long-form responses by examining the relationships between various aspects such as factuality, informativeness, formality, and acceptability. The authors propose a novel approach to compute an overall quality score as a weighted average of these aspects, validated using a dataset of 1.2K long-form QA answers annotated with both absolute judgments and relative preferences. The study also includes extensive experiments comparing multiple evaluation metrics, focusing on their correlations with human judgments. While the paper aims to address a significant challenge in the evaluation of long-form responses, particularly in the context of large language models (LLMs), it primarily focuses on the ELI5 dataset and employs a relatively small sample size, raising concerns about generalizability and robustness. The paper is generally well-written, but certain terminological choices and methodological details require clarification to improve its accessibility and reproducibility.\n\n**Strengths:**\nThe paper addresses a timely and relevant problem in the field of natural language processing, particularly in the evaluation of long-form responses generated by LLMs. By proposing a weighted average approach to evaluate responses across multiple dimensions, the authors contribute a potentially impactful methodology. The introduction of a dataset with 1.2K annotations, which includes both absolute judgments and relative preferences, is a notable resource for the community. The inclusion of extensive experiments and ablation studies adds value, providing insights into the strengths and weaknesses of various evaluation metrics. The paper demonstrates strong alignment with human judgments and highlights the importance of factuality as the most predictive factor for overall response acceptability. Furthermore, the overall presentation is clear, with well-organized content and a logical flow that makes the study accessible to readers. The study’s attempt to systematically evaluate multiple aspects of response quality sets a promising foundation for further exploration in this domain.\n\n**Weaknesses:**\nDespite its contributions, the paper exhibits several limitations that need to be addressed. The novelty of the work is somewhat constrained, as the concept of aggregating evaluation metrics into an overall score is not entirely new. The motivation for defining 'acceptability' as a distinct aspect and the rationale behind selecting the three proposed dimensions—factuality, informativeness, and formality—are not sufficiently justified. There is also a lack of clarity regarding the differences between 'informativeness' and 'information amount,' as well as the distinction between 'formality' and 'grammaticality.' Additionally, the paper does not adequately explore alternative approaches beyond the weighted average, limiting the scope of its conclusions. Methodologically, the reliance on a single dataset (ELI5) and a relatively small sample size raises concerns about the generalizability and robustness of the findings. Key details, such as the specific prompts used for generating and evaluating responses, are missing, which hinders reproducibility. The absence of an ablation study to assess the impact of individual components within the weighted average further weakens the empirical validation. Finally, the paper’s terminology, particularly in sections like 5.3, and the lack of discussion on future directions leave room for improvement in terms of clarity and comprehensiveness.\n\n**Decision**\nReject","source":"Automatic_Review","type":"automatic_review"},"status":"success"}
