{
  "title": "LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods",
  "publication": {
    "publisher": {},
    "date": "2024-12-10"
  },
  "author": [
    {
      "forename": "Haitao",
      "surname": "Li",
      "name": "Haitao Li",
      "email": ""
    },
    {
      "forename": "Qian",
      "surname": "Dong",
      "name": "Qian Dong",
      "email": ""
    },
    {
      "forename": "Junjie",
      "surname": "Chen",
      "name": "Junjie Chen",
      "email": ""
    },
    {
      "forename": "Huixue",
      "surname": "Su",
      "name": "Huixue Su",
      "email": "suhuixue@ruc.edu.cn"
    },
    {
      "forename": "Yujia",
      "surname": "Zhou",
      "name": "Yujia Zhou",
      "email": ""
    },
    {
      "forename": "Qingyao",
      "surname": "Ai",
      "name": "Qingyao Ai",
      "email": ""
    },
    {
      "forename": "Ziyi",
      "surname": "Ye",
      "name": "Ziyi Ye",
      "email": ""
    },
    {
      "forename": "Yiqun",
      "surname": "Liu",
      "name": "Yiqun Liu",
      "email": "yiqunliu@tsinghua.edu.cn"
    }
  ],
  "abstract": [
    [
      "The rapid advancement of Large Language Models (LLMs) has driven their expanding application across various fields. One of the most promising applications is their role as evaluators based on natural language responses, referred to as \"LLMs-as-judges\". This framework has attracted growing attention from both academia and industry due to their excellent effectiveness, ability to generalize across tasks, and interpretability in the form of natural language. This paper presents a comprehensive survey of the LLMs-as-judges paradigm from five key perspectives: Functionality, Methodology, Applications, Meta-evaluation, and Limitations. We begin by providing a systematic definition of LLMs-as-Judges and introduce their functionality (Why use LLM judges?). Then we address methodology to construct an evaluation system with LLMs (How to use LLM judges?). Additionally, we investigate the potential domains for their application (Where to use LLM judges?) and discuss methods for evaluating them in various contexts (How to evaluate LLM judges?). Finally, we provide a detailed analysis of the limitations of LLM judges and discuss potential future directions."
    ]
  ],
  "body": [
    {
      "section": {
        "index": -1,
        "name": ""
      },
      "p": [
        {
          "text": "Despite its great potential and significant advantages, LLMs-as-judges also face several critical challenges. For example, the evaluation results of LLMs are often influenced by the prompt template, which can lead to biased or inconsistent assessments [260]. Considering that LLMs are trained on extensive text corpus, they may also inherit various implicit biases, impacting the fairness and reliability of their assessments [267]. Moreover, distinct tasks and domains require specific evaluation criteria, making it difficult for LLMs to adapt their standards dynamically to specific contexts.",
          "quote": [
            {
              "text": "[260]",
              "target": "#b262",
              "type": "bibr",
              "context": "sessments ",
              "index": 252
            },
            {
              "text": "[267]",
              "target": "#b269",
              "type": "bibr",
              "context": "sessments ",
              "index": 426
            }
          ]
        },
        {
          "text": "Considering the vast potential of this field, this survey aims to systematically review and analyze the current state and key challenges of the LLMs-as-judges. As shown in Figures , we discuss existing research across five key perspectives: 1) Functionality: Why use LLM judges, 2) Methodology: How to use LLM judges, 3) Application: Where to use LLM judges, 4) Metaevaluation: How to evaluate LLM judges and 5) Limitation: Existing problems of LLM judges. We explore the key challenges confronting LLMs-as-judges and hope to provide a clearer guideline for their future development.",
          "quote": []
        },
        {
          "text": "In summary, the main contributions of this paper are as follows:",
          "quote": []
        },
        {
          "text": "(1) Comprehensive and Timely Survey: We present the extensive survey on the emerging paradigm of LLMs-as-judges, systematically reviewing the current state of research and developments in this field. By examining LLMs as performance evaluators based on their generated natural language, we highlight the unique role of LLMs in shaping the future of AI evaluation.",
          "quote": []
        },
        {
          "text": "(2) Systematic Analysis Across Five Key Perspectives: We organize our survey around five critical aspects: Functionality, Methodology, Application, Meta-evaluation, and Limitation. This structured approach allows for a nuanced understanding of how and why LLMs are utilized as evaluators, their practical implementations, and reliability concerns. (3) Current Challenges and Future Research Directions: We discuss the existing challenges for adopting LLMs-as-judges, highlighting potential research opportunities and directions while offering a forward-looking perspective on the future development of this paradigm, encouraging researchers to delve deeper into this exciting area. We also provide an opensource repository at https://github.com/CSHaitao/Awesome-LLMs-as-Judges, with the goal of fostering a collaborative community and advancing best practices in this area.",
          "quote": []
        },
        {
          "text": "The organization of this paper is as follows. In Section ( §2), we provide the formal definition of LLMs-as-judges. Then, Section ( §3) reviews existing work from the perspective of \"Why use LLM judges\". Following that, Section ( §4) covers \"How to use LLM judges\", summarizing the current technical developments in LLMs-as-judges. Section ( §5) discusses \"Where to use LLM judges\", focusing on their application domains. In Section ( §6), we review the metrics and benchmarks used for evaluating LLMs-as-judges. Section ( §7) discusses the limitations and challenges of LLM judges. We discuss major future work in Sections ( §8) and ( §9) to conclude the paper.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "2",
        "name": "PRELIMINARIES"
      },
      "p": [
        {
          "text": "In this section, we will provide a formal definition of LLMs-as-judges, aiming to encompass all current evaluation paradigms and methods, thereby offering readers a clear and thorough understanding. Figure  presents an overview of the LLMs-as-judges system.",
          "quote": []
        },
        {
          "text": "The LLMs-as-judges paradigm is a flexible and powerful evaluation framework where LLMs are employed as evaluative tools, responsible for assessing the quality, relevance, and effectiveness of generated outputs based on defined evaluation criteria. This framework leverages the extensive knowledge and deep contextual understanding of LLMs, enabling it to flexibly adapt to various LLM-Eval [141], Wang et al. [228], Zhou et al. [297], ARES [188], SELF-RAG [3], Lei et al. [124] Model Evaluation ( §3.1.2) Auto-Arena [153, 286], LMExam [10], KIEval [273] Model Enhancement ( §3.2)",
          "quote": [
            {
              "text": "[141]",
              "target": "#b140",
              "type": "bibr",
              "context": " LLM-Eval ",
              "index": 390
            },
            {
              "text": "[228]",
              "target": "#b230",
              "type": "bibr",
              "context": "ng et al. ",
              "index": 409
            },
            {
              "text": "[297]",
              "target": "#b299",
              "type": "bibr",
              "context": "ou et al. ",
              "index": 428
            },
            {
              "text": "[188]",
              "target": "#b188",
              "type": "bibr",
              "context": "97], ARES ",
              "index": 440
            },
            {
              "text": "[3]",
              "target": "#b2",
              "type": "bibr",
              "context": " SELF-RAG ",
              "index": 456
            },
            {
              "text": "[124]",
              "target": "#b123",
              "type": "bibr",
              "context": "ei et al. ",
              "index": 472
            },
            {
              "text": "[153,",
              "target": "#b153",
              "type": "bibr",
              "context": "uto-Arena ",
              "index": 516
            },
            {
              "text": "286]",
              "target": "#b288",
              "type": "bibr",
              "context": "ena [153, ",
              "index": 522
            },
            {
              "text": "[10]",
              "target": "#b9",
              "type": "bibr",
              "context": "], LMExam ",
              "index": 535
            },
            {
              "text": "[273]",
              "target": "#b275",
              "type": "bibr",
              "context": "], KIEval ",
              "index": 548
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": -1,
        "name": "Reward Modeling"
      },
      "p": [
        {
          "text": "During Training ( §3.2.1)",
          "quote": []
        },
        {
          "text": "LLMs-as-judges LIMITATION ( §7)",
          "quote": []
        },
        {
          "text": "Biases ( §7.1)",
          "quote": []
        },
        {
          "text": "Presentation-Related ( §7.1.1) Position bias [16, 25, 87, 107, 111, 127, 130, 132, 134, 151, 182, 184, 197, 208, 230, 234, 288, 291, 292, 296], Verbosity bias [163, 267, 270] Social-Related ( §7.1.2) Authority bias [25, 267, 289], Bandwagon-effect bias [112, 267], Compassion-fade bias [112, 267], Diversity bias [25, 267] Content-Related ( §7. 1.3) Sentiment bias [267], Token Bias [98, 127, 178, 184], Contextual Bias [62, 78, 179, 290, 296, 300] Cognitive-Related ( §7. 1.4) Overconfidence bias [103, 107], Self-enhancement bias [8, 19, 132, 145, 145, 267, 292], Refinement-aware bias [259, 267] Distraction bias [112, 195, 267], Fallacy-oversight bias [25, 267] Adversarial Attacks ( §7.2)",
          "quote": [
            {
              "text": "[16,",
              "target": "#b15",
              "type": "bibr",
              "context": "tion bias ",
              "index": 45
            },
            {
              "text": "25,",
              "target": "#b24",
              "type": "bibr",
              "context": "bias [16, ",
              "index": 50
            },
            {
              "text": "87,",
              "target": "#b86",
              "type": "bibr",
              "context": " [16, 25, ",
              "index": 54
            },
            {
              "text": "107,",
              "target": "#b106",
              "type": "bibr",
              "context": ", 25, 87, ",
              "index": 58
            },
            {
              "text": "111,",
              "target": "#b110",
              "type": "bibr",
              "context": " 87, 107, ",
              "index": 63
            },
            {
              "text": "127,",
              "target": "#b126",
              "type": "bibr",
              "context": "107, 111, ",
              "index": 68
            },
            {
              "text": "130,",
              "target": "#b129",
              "type": "bibr",
              "context": "111, 127, ",
              "index": 73
            },
            {
              "text": "132,",
              "target": "#b131",
              "type": "bibr",
              "context": "127, 130, ",
              "index": 78
            },
            {
              "text": "134,",
              "target": "#b133",
              "type": "bibr",
              "context": "130, 132, ",
              "index": 83
            },
            {
              "text": "151,",
              "target": "#b151",
              "type": "bibr",
              "context": "132, 134, ",
              "index": 88
            },
            {
              "text": "182,",
              "target": "#b182",
              "type": "bibr",
              "context": "134, 151, ",
              "index": 93
            },
            {
              "text": "184,",
              "target": "#b184",
              "type": "bibr",
              "context": "151, 182, ",
              "index": 98
            },
            {
              "text": "197,",
              "target": "#b199",
              "type": "bibr",
              "context": "182, 184, ",
              "index": 103
            },
            {
              "text": "208,",
              "target": "#b210",
              "type": "bibr",
              "context": "184, 197, ",
              "index": 108
            },
            {
              "text": "230,",
              "target": "#b232",
              "type": "bibr",
              "context": "197, 208, ",
              "index": 113
            },
            {
              "text": "234,",
              "target": "#b236",
              "type": "bibr",
              "context": "208, 230, ",
              "index": 118
            },
            {
              "text": "288,",
              "target": "#b290",
              "type": "bibr",
              "context": "230, 234, ",
              "index": 123
            },
            {
              "text": "291,",
              "target": "#b293",
              "type": "bibr",
              "context": "234, 288, ",
              "index": 128
            },
            {
              "text": "292,",
              "target": "#b294",
              "type": "bibr",
              "context": "288, 291, ",
              "index": 133
            },
            {
              "text": "296]",
              "target": "#b298",
              "type": "bibr",
              "context": "291, 292, ",
              "index": 138
            },
            {
              "text": "[163,",
              "target": "#b163",
              "type": "bibr",
              "context": "sity bias ",
              "index": 159
            },
            {
              "text": "267,",
              "target": "#b269",
              "type": "bibr",
              "context": "ias [163, ",
              "index": 165
            },
            {
              "text": "270]",
              "target": "#b272",
              "type": "bibr",
              "context": "163, 267, ",
              "index": 170
            },
            {
              "text": "[25,",
              "target": "#b24",
              "type": "bibr",
              "context": "rity bias ",
              "index": 215
            },
            {
              "text": "267,",
              "target": "#b269",
              "type": "bibr",
              "context": "bias [25, ",
              "index": 220
            },
            {
              "text": "289]",
              "target": "#b291",
              "type": "bibr",
              "context": "[25, 267, ",
              "index": 225
            },
            {
              "text": "[112,",
              "target": "#b111",
              "type": "bibr",
              "context": "fect bias ",
              "index": 253
            },
            {
              "text": "267]",
              "target": "#b269",
              "type": "bibr",
              "context": "ias [112, ",
              "index": 259
            },
            {
              "text": "[112,",
              "target": "#b111",
              "type": "bibr",
              "context": "fade bias ",
              "index": 286
            },
            {
              "text": "267]",
              "target": "#b269",
              "type": "bibr",
              "context": "ias [112, ",
              "index": 292
            },
            {
              "text": "[25,",
              "target": "#b24",
              "type": "bibr",
              "context": "sity bias ",
              "index": 313
            },
            {
              "text": "267]",
              "target": "#b269",
              "type": "bibr",
              "context": "bias [25, ",
              "index": 318
            },
            {
              "text": "1.3)",
              "target": "",
              "type": "bibr",
              "context": "ted ( §7. ",
              "index": 345
            },
            {
              "text": "[267]",
              "target": "#b269",
              "type": "bibr",
              "context": "ment bias ",
              "index": 365
            },
            {
              "text": "[98,",
              "target": "#b97",
              "type": "bibr",
              "context": "oken Bias ",
              "index": 383
            },
            {
              "text": "127,",
              "target": "#b126",
              "type": "bibr",
              "context": "Bias [98, ",
              "index": 388
            },
            {
              "text": "178,",
              "target": "#b178",
              "type": "bibr",
              "context": "[98, 127, ",
              "index": 393
            },
            {
              "text": "184]",
              "target": "#b184",
              "type": "bibr",
              "context": "127, 178, ",
              "index": 398
            },
            {
              "text": "[62,",
              "target": "#b61",
              "type": "bibr",
              "context": "tual Bias ",
              "index": 420
            },
            {
              "text": "78,",
              "target": "#b77",
              "type": "bibr",
              "context": "Bias [62, ",
              "index": 425
            },
            {
              "text": "179,",
              "target": "#b179",
              "type": "bibr",
              "context": " [62, 78, ",
              "index": 429
            },
            {
              "text": "290,",
              "target": "#b292",
              "type": "bibr",
              "context": " 78, 179, ",
              "index": 434
            },
            {
              "text": "296,",
              "target": "#b298",
              "type": "bibr",
              "context": "179, 290, ",
              "index": 439
            },
            {
              "text": "300]",
              "target": "#b302",
              "type": "bibr",
              "context": "290, 296, ",
              "index": 444
            },
            {
              "text": "1.4)",
              "target": "",
              "type": "bibr",
              "context": "ted ( §7. ",
              "index": 473
            },
            {
              "text": "[103,",
              "target": "#b102",
              "type": "bibr",
              "context": "ence bias ",
              "index": 498
            },
            {
              "text": "107]",
              "target": "#b106",
              "type": "bibr",
              "context": "ias [103, ",
              "index": 504
            },
            {
              "text": "[8,",
              "target": "#b7",
              "type": "bibr",
              "context": "ment bias ",
              "index": 532
            },
            {
              "text": "19,",
              "target": "#b18",
              "type": "bibr",
              "context": " bias [8, ",
              "index": 536
            },
            {
              "text": "132,",
              "target": "#b131",
              "type": "bibr",
              "context": "s [8, 19, ",
              "index": 540
            },
            {
              "text": "145,",
              "target": "#b145",
              "type": "bibr",
              "context": " 19, 132, ",
              "index": 545
            },
            {
              "text": "145,",
              "target": "#b145",
              "type": "bibr",
              "context": "132, 145, ",
              "index": 550
            },
            {
              "text": "267,",
              "target": "#b269",
              "type": "bibr",
              "context": "145, 145, ",
              "index": 555
            },
            {
              "text": "292]",
              "target": "#b294",
              "type": "bibr",
              "context": "145, 267, ",
              "index": 560
            },
            {
              "text": "[259,",
              "target": "#b261",
              "type": "bibr",
              "context": "ware bias ",
              "index": 588
            },
            {
              "text": "267]",
              "target": "#b269",
              "type": "bibr",
              "context": "ias [259, ",
              "index": 594
            },
            {
              "text": "[112,",
              "target": "#b111",
              "type": "bibr",
              "context": "tion bias ",
              "index": 616
            },
            {
              "text": "195,",
              "target": "#b197",
              "type": "bibr",
              "context": "ias [112, ",
              "index": 622
            },
            {
              "text": "267]",
              "target": "#b269",
              "type": "bibr",
              "context": "112, 195, ",
              "index": 627
            },
            {
              "text": "[25,",
              "target": "#b24",
              "type": "bibr",
              "context": "ight bias ",
              "index": 656
            },
            {
              "text": "267]",
              "target": "#b269",
              "type": "bibr",
              "context": "bias [25, ",
              "index": 661
            }
          ]
        },
        {
          "text": "Adversarial Attacks on LLMs ( §7.2.1) Text-Level Manipulations [18, 57, 100, 177], Structural and Semantic Distortions [260], Optimization-Based Attacks [120, 210, 211] Adversarial Attacks on LLMs-as-Judges ( §7.2.2)",
          "quote": [
            {
              "text": "[18,",
              "target": "#b17",
              "type": "bibr",
              "context": "pulations ",
              "index": 63
            },
            {
              "text": "57,",
              "target": "#b56",
              "type": "bibr",
              "context": "ions [18, ",
              "index": 68
            },
            {
              "text": "100,",
              "target": "#b99",
              "type": "bibr",
              "context": " [18, 57, ",
              "index": 72
            },
            {
              "text": "177]",
              "target": "#b177",
              "type": "bibr",
              "context": " 57, 100, ",
              "index": 77
            },
            {
              "text": "[260]",
              "target": "#b262",
              "type": "bibr",
              "context": "stortions ",
              "index": 119
            },
            {
              "text": "[120,",
              "target": "#b119",
              "type": "bibr",
              "context": "d Attacks ",
              "index": 153
            },
            {
              "text": "210,",
              "target": "#b212",
              "type": "bibr",
              "context": "cks [120, ",
              "index": 159
            },
            {
              "text": "211]",
              "target": "#b213",
              "type": "bibr",
              "context": "120, 210, ",
              "index": 164
            }
          ]
        },
        {
          "text": "Zheng et al. [293], Doddapaneni et al. [51], MT-Bench [292], Raina et al. [184], Shi et al. [196] Inherent Weaknesses ( §7.3) Knowledge Recency ( §7.3.1) Zhao et al. [287], Luo et al. [152], Gao et al. [69], Lewis et al. [125], Wu et al. [244], Dierickx et al. [48] Hallucination ( §7.3.2) Dierickx et al. [48], Ji et al. [97], Tonmoy et al. [216] Domain-Specific Knowledge Gaps ( §7.3.3)",
          "quote": [
            {
              "text": "[293]",
              "target": "#b295",
              "type": "bibr",
              "context": "ng et al. ",
              "index": 13
            },
            {
              "text": "[51]",
              "target": "#b50",
              "type": "bibr",
              "context": "ni et al. ",
              "index": 39
            },
            {
              "text": "[292]",
              "target": "#b294",
              "type": "bibr",
              "context": " MT-Bench ",
              "index": 54
            },
            {
              "text": "[184]",
              "target": "#b184",
              "type": "bibr",
              "context": "na et al. ",
              "index": 74
            },
            {
              "text": "[196]",
              "target": "#b198",
              "type": "bibr",
              "context": "hi et al. ",
              "index": 92
            },
            {
              "text": "[287]",
              "target": "#b289",
              "type": "bibr",
              "context": "ao et al. ",
              "index": 166
            },
            {
              "text": "[152]",
              "target": "#b152",
              "type": "bibr",
              "context": "uo et al. ",
              "index": 184
            },
            {
              "text": "[69]",
              "target": "#b68",
              "type": "bibr",
              "context": "ao et al. ",
              "index": 202
            },
            {
              "text": "[125]",
              "target": "#b124",
              "type": "bibr",
              "context": "is et al. ",
              "index": 221
            },
            {
              "text": "[244]",
              "target": "#b246",
              "type": "bibr",
              "context": "Wu et al. ",
              "index": 238
            },
            {
              "text": "[48]",
              "target": "#b47",
              "type": "bibr",
              "context": "kx et al. ",
              "index": 261
            },
            {
              "text": "[48]",
              "target": "#b47",
              "type": "bibr",
              "context": "kx et al. ",
              "index": 306
            },
            {
              "text": "[97]",
              "target": "#b96",
              "type": "bibr",
              "context": "Ji et al. ",
              "index": 322
            },
            {
              "text": "[216]",
              "target": "#b218",
              "type": "bibr",
              "context": "oy et al. ",
              "index": 342
            }
          ]
        },
        {
          "text": "Feng et al. [63], Pan et al. [171], Gao et al. [69], Szymanski et al. [212], Dorner et al. [55] FUTURE WORK ( §8)",
          "quote": [
            {
              "text": "[63]",
              "target": "#b62",
              "type": "bibr",
              "context": "ng et al. ",
              "index": 12
            },
            {
              "text": "[171]",
              "target": "#b171",
              "type": "bibr",
              "context": "an et al. ",
              "index": 29
            },
            {
              "text": "[69]",
              "target": "#b68",
              "type": "bibr",
              "context": "ao et al. ",
              "index": 47
            },
            {
              "text": "[212]",
              "target": "#b214",
              "type": "bibr",
              "context": "ki et al. ",
              "index": 70
            },
            {
              "text": "[55]",
              "target": "#b54",
              "type": "bibr",
              "context": "er et al. ",
              "index": 91
            }
          ]
        },
        {
          "text": "More Efficient ( §8.1) Automated Construction of Evaluation Criteria and Tasks [10, 233, 273, 280, 286], Scalable Evaluation Systems [257], Accelerating Evaluation Processes [31, 123, 149] More Effective ( §8.2) Integration of Reasoning and Judge Capabilities [207, 271, 304], Establishing a Collective Judgment Mechanism [22, 39], Enhancing Domain Knowledge [185],",
          "quote": [
            {
              "text": "[10,",
              "target": "#b9",
              "type": "bibr",
              "context": "and Tasks ",
              "index": 79
            },
            {
              "text": "233,",
              "target": "#b235",
              "type": "bibr",
              "context": "asks [10, ",
              "index": 84
            },
            {
              "text": "273,",
              "target": "#b275",
              "type": "bibr",
              "context": "[10, 233, ",
              "index": 89
            },
            {
              "text": "280,",
              "target": "#b282",
              "type": "bibr",
              "context": "233, 273, ",
              "index": 94
            },
            {
              "text": "286]",
              "target": "#b288",
              "type": "bibr",
              "context": "273, 280, ",
              "index": 99
            },
            {
              "text": "[257]",
              "target": "#b259",
              "type": "bibr",
              "context": "n Systems ",
              "index": 133
            },
            {
              "text": "[31,",
              "target": "#b30",
              "type": "bibr",
              "context": "Processes ",
              "index": 174
            },
            {
              "text": "123,",
              "target": "#b122",
              "type": "bibr",
              "context": "sses [31, ",
              "index": 179
            },
            {
              "text": "149]",
              "target": "#b149",
              "type": "bibr",
              "context": "[31, 123, ",
              "index": 184
            },
            {
              "text": "[207,",
              "target": "#b209",
              "type": "bibr",
              "context": "abilities ",
              "index": 260
            },
            {
              "text": "271,",
              "target": "#b273",
              "type": "bibr",
              "context": "ies [207, ",
              "index": 266
            },
            {
              "text": "304]",
              "target": "#b306",
              "type": "bibr",
              "context": "207, 271, ",
              "index": 271
            },
            {
              "text": "[22,",
              "target": "#b21",
              "type": "bibr",
              "context": "Mechanism ",
              "index": 322
            },
            {
              "text": "39]",
              "target": "#b38",
              "type": "bibr",
              "context": "nism [22, ",
              "index": 327
            },
            {
              "text": "[185]",
              "target": "#b185",
              "type": "bibr",
              "context": "Knowledge ",
              "index": 359
            }
          ]
        },
        {
          "text": "Cross-Domain and Cross-Language Transferability [77, 202, 241], Multimodal Integration Evaluation [24] More Reliable ( §8.3) Enhancing Interpretability and Transparency [147], Mitigating Bias and Ensuring Fairness [127], Enhancing Robustness [58, 196] Fig. . Taxonomy of LLMs-as-judges in limitation and future work.",
          "quote": [
            {
              "text": "[77,",
              "target": "#b76",
              "type": "bibr",
              "context": "erability ",
              "index": 48
            },
            {
              "text": "202,",
              "target": "#b204",
              "type": "bibr",
              "context": "lity [77, ",
              "index": 53
            },
            {
              "text": "241]",
              "target": "#b243",
              "type": "bibr",
              "context": "[77, 202, ",
              "index": 58
            },
            {
              "text": "[24]",
              "target": "#b23",
              "type": "bibr",
              "context": "valuation ",
              "index": 98
            },
            {
              "text": "[147]",
              "target": "#b147",
              "type": "bibr",
              "context": "nsparency ",
              "index": 169
            },
            {
              "text": "[127]",
              "target": "#b126",
              "type": "bibr",
              "context": " Fairness ",
              "index": 214
            },
            {
              "text": "[58,",
              "target": "#b57",
              "type": "bibr",
              "context": "obustness ",
              "index": 242
            },
            {
              "text": "196]",
              "target": "#b198",
              "type": "bibr",
              "context": "ness [58, ",
              "index": 247
            }
          ]
        },
        {
          "text": "evaluation process can be defined as follows:",
          "quote": []
        },
        {
          "text": "where 𝐸 is the evaluation function, taking the evaluation type T , evaluation criteria C, evaluation item X and optional references R as input. Based on these inputs, the LLM can produces three outputs: evaluation result Y, explanation E and feedback F . Different input-output configurations correspond to distinct methods and objectives. This unified formulation brings together diverse evaluation paradigms, offering a structured framework for categorizing and understanding various approaches within LLMs-as-judges.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "2.1",
        "name": "Evaluation Function 𝐸"
      },
      "p": [
        {
          "text": "The evaluation function 𝐸 in the context of LLMs-as-judges can be categorized into three primary configurations: Single-LLM systems, Multi-LLM systems, and Hybrid systems that combine LLMs with human evaluators. Each of these configurations serves distinct purposes, offers different advantages, and faces unique challenges.",
          "quote": []
        },
        {
          "text": "• Single-LLM Evaluation System [141, 145, 280]: A single LLM evaluation system relies on a single model to perform the evaluation tasks. It is simple to deploy and scale, making it efficient for tasks that don't require specialized evaluation. However, its flexibility is limited, as it may struggle with tasks that demand specialized knowledge or reasoning over complex inputs. Additionally, if not properly trained, a single model may introduce biases, leading to inaccurate evaluations. • Multi-LLM Evaluation Systems [22, 39, 132]: A Multi-LLM evaluation system combines multiple models that work together to perform evaluation tasks. These models may interact through various mechanisms, such as collaboration, or competition, to refine their outputs and achieve more accurate results. By leveraging the strengths of different models, a multi-model system can cover a broader range of evaluation criteria and provide a more comprehensive assessment. However, this comes at a higher computational cost and requires more resources, making deployment and maintenance more challenging, particularly for large-scale tasks. Moreover, while cooperation between models often enhances evaluation results, the methods through which these models achieve consensus or resolve differences remain key areas of ongoing exploration. • Human-AI Collaboration System [131, 192, 230]: In this system, LLMs work alongside human evaluators, combining the efficiency of automated evaluation with the nuanced judgment of human expertise. This configuration allows human evaluators to mitigate potential biases in the LLM's output and provide subjective insights into complex evaluation tasks. While this system offers greater reliability and depth, it comes with challenges in coordinating between the models and humans, ensuring consistent evaluation standards, and integrating feedback. Additionally, the inclusion of human evaluators increases both the cost and time required for the evaluation process, making it less scalable than purely model-based systems.",
          "quote": [
            {
              "text": "[141,",
              "target": "#b140",
              "type": "bibr",
              "context": "on System ",
              "index": 31
            },
            {
              "text": "145,",
              "target": "#b145",
              "type": "bibr",
              "context": "tem [141, ",
              "index": 37
            },
            {
              "text": "280]",
              "target": "#b282",
              "type": "bibr",
              "context": "141, 145, ",
              "index": 42
            },
            {
              "text": "[22,",
              "target": "#b21",
              "type": "bibr",
              "context": "n Systems ",
              "index": 521
            },
            {
              "text": "39,",
              "target": "#b38",
              "type": "bibr",
              "context": "tems [22, ",
              "index": 526
            },
            {
              "text": "132]",
              "target": "#b131",
              "type": "bibr",
              "context": " [22, 39, ",
              "index": 530
            },
            {
              "text": "[131,",
              "target": "#b130",
              "type": "bibr",
              "context": "on System ",
              "index": 1354
            },
            {
              "text": "192,",
              "target": "#b192",
              "type": "bibr",
              "context": "tem [131, ",
              "index": 1360
            },
            {
              "text": "230]",
              "target": "#b232",
              "type": "bibr",
              "context": "131, 192, ",
              "index": 1365
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "2.2",
        "name": "Evaluation Input"
      },
      "p": [
        {
          "text": "In the LLMs-as-judges paradigm, in addition to the evaluation item X, LLM judges typically receive three other types of inputs: Evaluation Type T , Evaluation Criteria C, and Evaluation References R. The following provides a detailed explanation:",
          "quote": []
        },
        {
          "text": "2.2.1 Evaluation Type T . The Evaluation Type T defines the specific evaluation mode, determining how the evaluation will be conducted. It typically includes three approaches: pointwise, pairwise, and listwise evaluation.",
          "quote": []
        },
        {
          "text": "• Pointwise Evaluation [109, 229, 266]: This method evaluates each candidate item individually based on the specified criteria. For example, in a text summarization task, the LLM might evaluate each generated summary separately, assigning a score based on factors like informativeness, coherence, and conciseness. Although pointwise evaluation is simple and easy to apply, it may fail to capture the relative quality differences between candidates and can be influenced by biases arising from evaluating items in isolation. • Pairwise Evaluation [20, 84, 89, 188]: This method involves directly comparing two candidate items to determine which one performs better according to the specified criteria. It is commonly used in preference-based tasks. For example, given two summaries of a news article, the LLM may be asked to decide which summary is more coherent or informative. Pairwise evaluation closely mirrors human decision-making processes by focusing on relative preferences rather than assigning absolute scores. This approach is especially effective when the differences between outputs are subtle and difficult to quantify. • Listwise Evaluation [87, 166, 263, 302]: This method is designed to collectively evaluate the entire list of candidate items, evaluating and ranking them based on the specific criteria. It is often applied in ranking tasks, such as document retrieval in search engines, where the objective is to determine the relevance of the documents in relation to a user query. Listwise evaluation takes into account the interactions between multiple candidates, making it well-suited for applications that require holistic analysis.",
          "quote": [
            {
              "text": "[109,",
              "target": "#b108",
              "type": "bibr",
              "context": "valuation ",
              "index": 23
            },
            {
              "text": "229,",
              "target": "#b231",
              "type": "bibr",
              "context": "ion [109, ",
              "index": 29
            },
            {
              "text": "266]",
              "target": "#b268",
              "type": "bibr",
              "context": "109, 229, ",
              "index": 34
            },
            {
              "text": "[20,",
              "target": "#b19",
              "type": "bibr",
              "context": "valuation ",
              "index": 546
            },
            {
              "text": "84,",
              "target": "#b83",
              "type": "bibr",
              "context": "tion [20, ",
              "index": 551
            },
            {
              "text": "89,",
              "target": "#b88",
              "type": "bibr",
              "context": " [20, 84, ",
              "index": 555
            },
            {
              "text": "188]",
              "target": "#b188",
              "type": "bibr",
              "context": ", 84, 89, ",
              "index": 559
            },
            {
              "text": "[87,",
              "target": "#b86",
              "type": "bibr",
              "context": "valuation ",
              "index": 1156
            },
            {
              "text": "166,",
              "target": "#b166",
              "type": "bibr",
              "context": "tion [87, ",
              "index": 1161
            },
            {
              "text": "263,",
              "target": "#b265",
              "type": "bibr",
              "context": "[87, 166, ",
              "index": 1166
            },
            {
              "text": "302]",
              "target": "#b304",
              "type": "bibr",
              "context": "166, 263, ",
              "index": 1171
            }
          ]
        },
        {
          "text": "In general, these three evaluation modes are not entirely independent. pointwise scores can be aggregated to create pairwise comparisons or used to construct a ranked list. Similarly, pairwise preferences can be organized into a complete ranking list for listwise analysis. However, these transformations are not always reliable within the LLMs-as-judges framework [149]. For example, in pointwise evaluation, output 𝐴 may receive a score of 5, while output 𝐵 receives a score of 4, yet, a direct pairwise comparison might not consistently yield 𝐴 > 𝐵 due to potential bias. Additionally, LLM judges do not always satisfy transitivity in their judgments. For instance, given pairwise preferences where 𝑧 𝑖 > 𝑧 𝑗 and 𝑧 𝑗 > 𝑧 𝑘 , the LLM may not necessarily yield 𝑧 𝑖 > 𝑧 𝑘 . These inconsistencies contribute to concerns about the reliability and trustworthiness of the LLM-as-Judge framework, which we will discuss in detail in Section ( §7). of quality attributes and can be tailored based on the nature of the task. Typically, the criteria encompass the following aspects:",
          "quote": [
            {
              "text": "[149]",
              "target": "#b149",
              "type": "bibr",
              "context": "framework ",
              "index": 365
            }
          ]
        },
        {
          "text": "• Linguistic Quality [34, 59, 283]: This category evaluates the language-related features of the output, such as fluency, grammatical accuracy, coherence, and Conciseness. Linguistic quality is crucial in tasks like text generation, machine translation, and summarization, where clarity and readability are essential. • Content Accuracy [30, 101, 213]: This dimension focuses on the correctness and relevance of the content. It includes evaluating aspects such as factual accuracy, ensuring that the output does not contain misleading or incorrect information. Content accuracy is particularly crucial in tasks such as code generation and fact-checking. • Task-Specific Metrics [96, 138, 213]: In addition to general quality metrics, many tasks require evaluation based on standards specific to their respective domains. These standards may include metrics such as informativeness (assessing whether the output provides comprehensive and valuable information) or completeness (ensuring all key aspects of the input are covered). Other criteria may include diversity, well-structured content, and logical clarity.",
          "quote": [
            {
              "text": "[34,",
              "target": "#b33",
              "type": "bibr",
              "context": "c Quality ",
              "index": 21
            },
            {
              "text": "59,",
              "target": "#b58",
              "type": "bibr",
              "context": "lity [34, ",
              "index": 26
            },
            {
              "text": "283]",
              "target": "#b285",
              "type": "bibr",
              "context": " [34, 59, ",
              "index": 30
            },
            {
              "text": "[30,",
              "target": "#b29",
              "type": "bibr",
              "context": " Accuracy ",
              "index": 337
            },
            {
              "text": "101,",
              "target": "#b100",
              "type": "bibr",
              "context": "racy [30, ",
              "index": 342
            },
            {
              "text": "213]",
              "target": "#b215",
              "type": "bibr",
              "context": "[30, 101, ",
              "index": 347
            },
            {
              "text": "[96,",
              "target": "#b95",
              "type": "bibr",
              "context": "c Metrics ",
              "index": 678
            },
            {
              "text": "138,",
              "target": "#b137",
              "type": "bibr",
              "context": "rics [96, ",
              "index": 683
            },
            {
              "text": "213]",
              "target": "#b215",
              "type": "bibr",
              "context": "[96, 138, ",
              "index": 688
            }
          ]
        },
        {
          "text": "In addition to providing clear evaluation criteria, offering several examples can also be beneficial for the assessment. By incorporating well-structured examples, LLMs can better align its output with user expectations, especially when handling complex tasks or ambiguous queries.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "2.2.3",
        "name": "Evaluation"
      },
      "p": [
        {
          "text": "References R. Evaluation References R are optional. Depending on the availability of evaluation reference, the evaluation process can be broadly divided into reference-based and reference-free scenarios.",
          "quote": []
        },
        {
          "text": "• Reference-Based Evaluation [66, 104]: The reference-based evaluation leverages reference data to determine whether the performance meets the expected standards. It is commonly applied in tasks where the quality of the output can be objectively judged by its similarity to established reference. In Natural Language Generation (NLG) tasks, this method is widely used to evaluate the resemblance between generated content and reference content. For example, in machine translation or text summarization, an LLM can compare the generated translations or summaries against high-quality references. The key strength of this approach is its well-defined benchmarking process; however, its effectiveness may be constrained by the quality and variety of the reference data. • Reference-Free Evaluation [82, 194, 292]: The reference-free evaluation does not rely on a specific reference R, instead, it evaluates X based on intrinsic quality standards or its alignment with the source context. For example, when assessing language fluency or content coherence, an LLM can autonomously generate evaluation results using internal grammatical and semantic rules. This method is widely used in fields like sentiment analysis and dialogue generation.",
          "quote": [
            {
              "text": "[66,",
              "target": "#b65",
              "type": "bibr",
              "context": "valuation ",
              "index": 29
            },
            {
              "text": "104]",
              "target": "#b103",
              "type": "bibr",
              "context": "tion [66, ",
              "index": 34
            },
            {
              "text": "[82,",
              "target": "#b81",
              "type": "bibr",
              "context": "valuation ",
              "index": 796
            },
            {
              "text": "194,",
              "target": "#b196",
              "type": "bibr",
              "context": "tion [82, ",
              "index": 801
            },
            {
              "text": "292]",
              "target": "#b294",
              "type": "bibr",
              "context": "[82, 194, ",
              "index": 806
            }
          ]
        },
        {
          "text": "The main advantage of this approach is its independence from specific references, providing",
          "quote": []
        },
        {
          "text": "greater flexibility for open-ended tasks. However, its drawback lies in the difficulty of obtaining satisfactory evaluations in domains where the LLM lacks relevant knowledge.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "2.3",
        "name": "Evaluation Output"
      },
      "p": [
        {
          "text": "In the LLMs-as-judges paradigm, the LLM typically generates three types of outputs: the evaluation result Y, the explanation E, and the feedback F . Below are detailed descriptions.",
          "quote": []
        },
        {
          "text": "• Evaluation Result Y [188, 286]: The evaluation result Y is the primary output, which can take the form of a numerical score, a ranking, a categorical label, or a qualitative assessment. It reflects the quality, relevance, or performance of the candidate items according to the specified evaluation criteria. For example, in a machine translation task, Y could be a score indicating translation quality, while in a dialogue generation task, it might be a rating of coherence and appropriateness on a scale from 1 to 5. The evaluation result Y provides a clear measure of performance, enabling researchers to effectively compare different models or outputs.",
          "quote": [
            {
              "text": "[188,",
              "target": "#b188",
              "type": "bibr",
              "context": " Result Y ",
              "index": 22
            },
            {
              "text": "286]",
              "target": "#b288",
              "type": "bibr",
              "context": "t Y [188, ",
              "index": 28
            }
          ]
        },
        {
          "text": "• Explanation E [252, 270]: The explanation E provides detailed reasoning and justifications for the evaluation result. It offers insights into why certain result received higher or lower scores, highlighting specific features of the candidate item that influenced the evaluation. For example, in a summarization task, the LLM judges might explain that the score was lowered due to missing critical information or the presence of redundant content. The explanation component enhances transparency, allowing users to understand the decision-making process of the LLM and gain deeper insights into the strengths and weaknesses of the evaluated content. • Feedback F [32, 155]: The feedback F consists of actionable suggestions or recommendations aimed at improving the evaluated output. Unlike the evaluation result, which merely indicates performance, the feedback component is designed to guide the refinement of the content. For instance, in a creative writing task, feedback might include recommendations for enhancing the narrative flow or improving clarity. This component is especially valuable for the iterative development of the evaluated item, as it provides concrete pointers that help both the LLM and content creators enhance the quality of the generated outputs.",
          "quote": [
            {
              "text": "[252,",
              "target": "#b254",
              "type": "bibr",
              "context": "anation E ",
              "index": 16
            },
            {
              "text": "270]",
              "target": "#b272",
              "type": "bibr",
              "context": "n E [252, ",
              "index": 22
            },
            {
              "text": "[32,",
              "target": "#b31",
              "type": "bibr",
              "context": "eedback F ",
              "index": 664
            },
            {
              "text": "155]",
              "target": "#b155",
              "type": "bibr",
              "context": "ck F [32, ",
              "index": 669
            }
          ]
        },
        {
          "text": "Depending on the intended purpose and specific requirements of the evaluation, the LLM judges can generate various combinations of the three outputs ( Y, E, F ) for a given task. In most cases,",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": -1,
        "name": "Performance Evaluation Data Construction"
      },
      "p": [
        {
          "text": "unlabeled data",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": -1,
        "name": "Model Enhancement"
      },
      "p": [
        {
          "text": "Responses Evaluation  providing explanation E not only helps users better understand and trust the evaluation results but also leads to more human-aligned and accurate evaluation result Y. Moreover, generating feedback F generally demands a higher level of model capability, as it requires not only assessing the quality of the input but also providing concrete, actionable recommendations for improvement.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "3",
        "name": "FUNCTIONALITY"
      },
      "p": [
        {
          "text": "As an emerging evaluation paradigm, LLMs-as-judges play a significant role across various scenarios.",
          "quote": []
        },
        {
          "text": "Based on their functionality, we categorize the application of LLM evaluators into three main directions: Performance Evaluation ( §3.1), Model Enhancement ( §3.2), and Data Construction ( §3.3). In this section, we will delve into these functionalities, explore their potential, and discuss specific implementation methods.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "3.1",
        "name": "Performance Evaluation"
      },
      "p": [
        {
          "text": "Performance evaluation represents the most fundamental application objective of an LLM judges, serving as the cornerstone for understanding and optimizing their other function. It can be broadly divided into two components: Responses Evaluation ( §3.1.1) and Model Evaluation ( §3.1.2).",
          "quote": []
        },
        {
          "text": "Response Evaluation focuses on aspects such as the quality, relevance, and coherence, and fluency of the responses for a given task. In contrast, model evaluation takes a holistic approach, assessing the overall capabilities of LLMs. Although these two aspects are interconnected, they focus on different levels of analysis, providing multidimensional insights into performance.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "3.1.1",
        "name": "Responses Evaluation."
      },
      "p": [
        {
          "text": "The purpose of evaluating responses is to identify better answers within the context of a specific question or task, which can enhance overall decision-making. These responses can originate from either AI models or humans. Evaluation criteria typically consider general attributes such as accuracy, relevance, coherence, and fluency. However, in practical applications, the evaluation of responses often requires customized metrics tailored to specific tasks. For instance, in the education domain, the focus may be more on the inspirational and educational value of the answers.",
          "quote": []
        },
        {
          "text": "LLM judges have also been widely applied in the assessment of text response [141, 228, 297]. Lin et al. [141] propose LLM-Eval, a unified framework employing a single-prompt strategy to evaluate the performance of open-domain dialogue systems across multiple dimensions, including content, grammar, relevance, and appropriateness. Wang et al. [228] proposed an article scoring and feedback system tailored to different genres, such as essays, narratives, and question-answering articles. Using BERT and ChatGPT models, they enabled automated scoring and detailed feedback, showcasing the potential of LLMs in article evaluation. Moreover, Zhou et al. [297] conduct a detailed evaluation of whether LLMs can serve as reliable tools for automated paper review. Their findings indicate that current LLMs are still not sufficiently reliable for such tasks, particularly in scenarios requiring logical reasoning or a deep knowledge base.",
          "quote": [
            {
              "text": "[141,",
              "target": "#b140",
              "type": "bibr",
              "context": " response ",
              "index": 76
            },
            {
              "text": "228,",
              "target": "#b230",
              "type": "bibr",
              "context": "nse [141, ",
              "index": 82
            },
            {
              "text": "297]",
              "target": "#b299",
              "type": "bibr",
              "context": "141, 228, ",
              "index": 87
            },
            {
              "text": "[141]",
              "target": "#b140",
              "type": "bibr",
              "context": "in et al. ",
              "index": 104
            },
            {
              "text": "[228]",
              "target": "#b230",
              "type": "bibr",
              "context": "ng et al. ",
              "index": 343
            },
            {
              "text": "[297]",
              "target": "#b299",
              "type": "bibr",
              "context": "ou et al. ",
              "index": 651
            }
          ]
        },
        {
          "text": "Furthermore, the evaluation of a single response is not limited to assessing the quality of the final answer but can also extend to analyzing the response process [3, 124, 188]. For instance, this can include evaluating whether retrieval is necessary at a given step, the relevance of the retrieved documents, and the interpretability of the response. For example, ARES [188] uses LLM judges to evaluate RAG systems across three dimensions: Contextual Relevance, Answer Faithfulness, and Answer Relevance. Similarly, Asai et al. [3] proposed SELF-RAG, which employs reflective token to determine whether retrieval is required and to self-assess the quality of generated outputs. Lei et al. [124] introduced LLMs to evaluate the quality of generated explanations, demonstrating the effectiveness of LLMs in understanding and generating explanations for recommendation tasks.",
          "quote": [
            {
              "text": "[3,",
              "target": "#b2",
              "type": "bibr",
              "context": "e process ",
              "index": 163
            },
            {
              "text": "124,",
              "target": "#b123",
              "type": "bibr",
              "context": "ocess [3, ",
              "index": 167
            },
            {
              "text": "188]",
              "target": "#b188",
              "type": "bibr",
              "context": " [3, 124, ",
              "index": 172
            },
            {
              "text": "[188]",
              "target": "#b188",
              "type": "bibr",
              "context": "ple, ARES ",
              "index": 370
            },
            {
              "text": "[3]",
              "target": "#b2",
              "type": "bibr",
              "context": "ai et al. ",
              "index": 529
            },
            {
              "text": "[124]",
              "target": "#b123",
              "type": "bibr",
              "context": "ei et al. ",
              "index": 690
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "3.1.2",
        "name": "Model Evaluation."
      },
      "p": [
        {
          "text": "Model evaluation typically begins with assessing individual responses and then extends to analyzing overall capabilities. This wider perspective aims to analyze the model's performance across various tasks or domains, such as coding ability, instruction-following proficiency, reasoning, and other specialized skills relevant to its intended applications.",
          "quote": []
        },
        {
          "text": "A common and straightforward approach is to represent model performance using average performance on static benchmarks [138, 213, 292]. LLM judges assess the model's performance using a set of carefully designed metrics, which results in a performance ranking. This method is widely adopted due to its simplicity and comparability. For example, task sets can be designed to evaluate the model's knowledge coverage, reasoning depth, and language generation quality [116, 148, 202], or real-world scenarios can be simulated to assess the model's ability to handle complex situations [144, 219].",
          "quote": [
            {
              "text": "[138,",
              "target": "#b137",
              "type": "bibr",
              "context": "enchmarks ",
              "index": 119
            },
            {
              "text": "213,",
              "target": "#b215",
              "type": "bibr",
              "context": "rks [138, ",
              "index": 125
            },
            {
              "text": "292]",
              "target": "#b294",
              "type": "bibr",
              "context": "138, 213, ",
              "index": 130
            },
            {
              "text": "[116,",
              "target": "#b115",
              "type": "bibr",
              "context": "n quality ",
              "index": 464
            },
            {
              "text": "148,",
              "target": "#b148",
              "type": "bibr",
              "context": "ity [116, ",
              "index": 470
            },
            {
              "text": "202]",
              "target": "#b204",
              "type": "bibr",
              "context": "116, 148, ",
              "index": 475
            },
            {
              "text": "[144,",
              "target": "#b144",
              "type": "bibr",
              "context": "ituations ",
              "index": 581
            },
            {
              "text": "219]",
              "target": "#b221",
              "type": "bibr",
              "context": "ons [144, ",
              "index": 587
            }
          ]
        },
        {
          "text": "As the demand for evaluation increases, the evaluation process has gradually shifted from traditional static testing to more dynamic, interactive assessments [10, 273, 286]. LLMs-as-judges has pioneered this approach, similar to Chatbot Arena [292], a crowdsourced platform that collects anonymous votes on LLM performance and ranks them using Elo scores. Auto-Arena [153, 286] and LMExam [10] assess model capabilities by using LLMs as both question setters and evaluators. These frameworks innovatively combine diverse question generation, multi-turn question-answering evaluation, and a decentralized model-to-model evaluation mechanism, providing more detailed and granular performance assessments. Additionally, KIEval [273] introduces an LLM-driven \"interactor\" role, which evaluates the knowledge mastery and generation abilities of LLMs through dynamic multi-turn conversations. These dynamic evaluation methods effectively address data leakage and evaluation bias issues common in traditional benchmark tests.",
          "quote": [
            {
              "text": "[10,",
              "target": "#b9",
              "type": "bibr",
              "context": "sessments ",
              "index": 158
            },
            {
              "text": "273,",
              "target": "#b275",
              "type": "bibr",
              "context": "ents [10, ",
              "index": 163
            },
            {
              "text": "286]",
              "target": "#b288",
              "type": "bibr",
              "context": "[10, 273, ",
              "index": 168
            },
            {
              "text": "[292]",
              "target": "#b294",
              "type": "bibr",
              "context": "bot Arena ",
              "index": 243
            },
            {
              "text": "[153,",
              "target": "#b153",
              "type": "bibr",
              "context": "uto-Arena ",
              "index": 367
            },
            {
              "text": "286]",
              "target": "#b288",
              "type": "bibr",
              "context": "ena [153, ",
              "index": 373
            },
            {
              "text": "[10]",
              "target": "#b9",
              "type": "bibr",
              "context": "nd LMExam ",
              "index": 389
            },
            {
              "text": "[273]",
              "target": "#b275",
              "type": "bibr",
              "context": "y, KIEval ",
              "index": 724
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "3.2",
        "name": "Model Enhancement"
      },
      "p": [
        {
          "text": "In addition to Performance Evaluation, LLMs-as-judges is also widely used for Model Enhancement. From training to inference, LLMs-as-judges plays a key role in improving model performance. Its application in model enhancement offers a novel optimization pathway for artificial intelligence, fostering the refinement and personalization of intelligent systems across a broader spectrum of real-world applications.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "3.2.1",
        "name": "Reward Modeling During"
      },
      "p": [
        {
          "text": "Training. A primary application of LLMs-as-judges is in reward modeling during training, particularly in reinforcement learning with feedback [21, 74, 239, 257, 274]. LLM judges assign scores to model outputs by evaluating them against human-defined criteria, guiding optimization toward desired behaviors. This ensures alignment with human values, improving the quality and relevance of the generated outputs and improving the effectiveness of LLMs in real-world tasks.",
          "quote": [
            {
              "text": "[21,",
              "target": "#b20",
              "type": "bibr",
              "context": " feedback ",
              "index": 142
            },
            {
              "text": "74,",
              "target": "#b73",
              "type": "bibr",
              "context": "back [21, ",
              "index": 147
            },
            {
              "text": "239,",
              "target": "#b241",
              "type": "bibr",
              "context": " [21, 74, ",
              "index": 151
            },
            {
              "text": "257,",
              "target": "#b259",
              "type": "bibr",
              "context": " 74, 239, ",
              "index": 156
            },
            {
              "text": "274]",
              "target": "#b276",
              "type": "bibr",
              "context": "239, 257, ",
              "index": 161
            }
          ]
        },
        {
          "text": "A series of works, such as SRLMs [274], OAIF [74], and RLAIF [121], have enabled LLMs to become their own reward models. This overcomes the traditional RLHF dependency on fixed reward models, allowing the model to iteratively reward and self-optimize, fostering self-evolution through continuous self-assessment. RELC [21] tackles the challenge of sparse rewards in traditional RL by introducing a Critic Language Model (Critic LM) to evaluate intermediate generation steps. This dense feedback at each step helps mitigate reward sparsity, offering more detailed guidance to the model during training.",
          "quote": [
            {
              "text": "[274]",
              "target": "#b276",
              "type": "bibr",
              "context": " as SRLMs ",
              "index": 33
            },
            {
              "text": "[74]",
              "target": "#b73",
              "type": "bibr",
              "context": "74], OAIF ",
              "index": 45
            },
            {
              "text": "[121]",
              "target": "#b120",
              "type": "bibr",
              "context": "and RLAIF ",
              "index": 61
            },
            {
              "text": "[21]",
              "target": "#b20",
              "type": "bibr",
              "context": "ent. RELC ",
              "index": 318
            }
          ]
        },
        {
          "text": "However, using the same LLM for both policy generation and reward modeling can pose challenges in ensuring the accuracy of the rewards. This dual role setup may lead to accumulated biases and preference data noise, which can undermine the training effectiveness. To address this issue, CREAM [239] introduces cross-iteration consistency constraints to regulate the training process and prevent the model from learning unreliable preference data. This significantly enhances reward consistency and alignment performance. In addition, CGPO [257] groups tasks by category (such as dialogue, mathematical reasoning, safety, etc.) and uses \"Mixed Judges\" to assign a specific reward model to each task group. This ensures that the reward signals are closely aligned with the task objectives, thereby preventing conflicts between different goals.",
          "quote": [
            {
              "text": "[239]",
              "target": "#b241",
              "type": "bibr",
              "context": "ue, CREAM ",
              "index": 292
            },
            {
              "text": "[257]",
              "target": "#b259",
              "type": "bibr",
              "context": "ion, CGPO ",
              "index": 538
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "3.2.2",
        "name": "Acting as Verifier During"
      },
      "p": [
        {
          "text": "Inference. During inference, LLM judges serve as verifier, responsible for selecting the optimal response from multiple candidates [15, 15, 137, 160, 265]. By comparing the outputs based on various metrics, such as factual accuracy and reasoning consistency, they are able to identify the best fit for the given task or context, thereby optimizing the inference process or improving the quality of the generated results.",
          "quote": [
            {
              "text": "[15,",
              "target": "#b14",
              "type": "bibr",
              "context": "andidates ",
              "index": 131
            },
            {
              "text": "15,",
              "target": "#b14",
              "type": "bibr",
              "context": "ates [15, ",
              "index": 136
            },
            {
              "text": "137,",
              "target": "#b136",
              "type": "bibr",
              "context": " [15, 15, ",
              "index": 140
            },
            {
              "text": "160,",
              "target": "#b160",
              "type": "bibr",
              "context": " 15, 137, ",
              "index": 145
            },
            {
              "text": "265]",
              "target": "#b267",
              "type": "bibr",
              "context": "137, 160, ",
              "index": 150
            }
          ]
        },
        {
          "text": "One of the simplest applications is Best-of-N sampling [102, 209], where the model is sampled N times, and the best result is selected to improve model performance. Similarly, Wang et al. [235] introduced a promising sampling method called self-consistency, where n samples are drawn from the judge model, and the average score is output. These sampling methods enhance inference stability by selecting the best result from multiple evaluations. Further optimization strategies include the Tree of Thoughts (ToT) [265] method, which models the problem-solving process as a tree structure. This allows the model to explore multiple solution paths and optimize path selection through self-assessment mechanisms. The Graph of Thoughts (GoT) [15] method extends this concept by introducing directed graphs, where the non-linear interactions between nodes improve the efficiency and precision of multi-step reasoning. In both methods, LLM judges play a crucial role in guiding the model to select the most promising paths, thereby enhancing the quality and accuracy of reasoning.",
          "quote": [
            {
              "text": "[102,",
              "target": "#b101",
              "type": "bibr",
              "context": " sampling ",
              "index": 55
            },
            {
              "text": "209]",
              "target": "#b211",
              "type": "bibr",
              "context": "ing [102, ",
              "index": 61
            },
            {
              "text": "[235]",
              "target": "#b237",
              "type": "bibr",
              "context": "ng et al. ",
              "index": 188
            },
            {
              "text": "[265]",
              "target": "#b267",
              "type": "bibr",
              "context": "hts (ToT) ",
              "index": 513
            },
            {
              "text": "[15]",
              "target": "#b14",
              "type": "bibr",
              "context": "hts (GoT) ",
              "index": 738
            }
          ]
        },
        {
          "text": "Similarly, Lightman et al. [137] discuss how step-by-step validation can enhance the performance of LLMs in multi-step reasoning tasks, particularly in the domain of mathematics. SE-GBS [250] integrates self-assessment into the multi-step reasoning decoding process, generating scores that reflect logical correctness and further ensuring the accuracy and consistency of the reasoning chain. The REPS [105] improves the accuracy and reliability of reasoning validation models by comparing reasoning paths pairwise, verifying their logical consistency and factual basis. Also, Musolesi et al. [160] proposed Creative Beam Search, with the LLM acting as a judge to simulate the human creative selection process, thereby enhancing the diversity and creativity of the generated results.",
          "quote": [
            {
              "text": "[137]",
              "target": "#b136",
              "type": "bibr",
              "context": "an et al. ",
              "index": 27
            },
            {
              "text": "[250]",
              "target": "#b252",
              "type": "bibr",
              "context": "s. SE-GBS ",
              "index": 186
            },
            {
              "text": "[105]",
              "target": "#b104",
              "type": "bibr",
              "context": " The REPS ",
              "index": 401
            },
            {
              "text": "[160]",
              "target": "#b160",
              "type": "bibr",
              "context": "si et al. ",
              "index": 592
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "3.2.3",
        "name": "Feedback for Refinement."
      },
      "p": [
        {
          "text": "After receiving the initial response, LLM judges provide actionable feedback to iteratively improve output quality. By analyzing the response based on specific task criteria, such as accuracy, coherence, or creativity, the LLM can identify weaknesses in the output and offer suggestions for improvement. This iterative refinement process plays a crucial role in applications that require adaptability [32, 93, 155, 176, 261]. SELF-REFINE [155] enables LLMs to iteratively improve output quality through feedback generated by the model itself, without requiring additional training or supervision data. On the other hand, SELF-DEBUGGING [32] demonstrates a practical application of self-correction in code generation by identifying and rectifying errors through self-explanation and feedback. This approach has significantly enhanced the performance of LLMs across various code generation tasks.",
          "quote": [
            {
              "text": "[32,",
              "target": "#b31",
              "type": "bibr",
              "context": "ptability ",
              "index": 401
            },
            {
              "text": "93,",
              "target": "#b92",
              "type": "bibr",
              "context": "lity [32, ",
              "index": 406
            },
            {
              "text": "155,",
              "target": "#b155",
              "type": "bibr",
              "context": " [32, 93, ",
              "index": 410
            },
            {
              "text": "176,",
              "target": "#b176",
              "type": "bibr",
              "context": " 93, 155, ",
              "index": 415
            },
            {
              "text": "261]",
              "target": "#b263",
              "type": "bibr",
              "context": "155, 176, ",
              "index": 420
            },
            {
              "text": "[155]",
              "target": "#b155",
              "type": "bibr",
              "context": "LF-REFINE ",
              "index": 438
            },
            {
              "text": "[32]",
              "target": "#b31",
              "type": "bibr",
              "context": "DEBUGGING ",
              "index": 636
            }
          ]
        },
        {
          "text": "In addition to refining response quality, LLMs judges are also widely used to enhance reasoning abilities. For example, REFINER [176] optimizes the reasoning performance of LLMs through interactions between a generator model and a critic model. In this framework, the generator model is responsible for producing intermediate reasoning steps, while the critic model analyzes these steps and provides detailed feedback, such as identifying calculation errors or logical inconsistencies. Xu et al. [261] propose a multi-agent collaboration strategy to enhance the reasoning abilities of LLMs by simulating the academic peer review process. The framework is divided into three stages: generation, review, and revision. Agents provide feedback and attach confidence scores to refine the initial answers, with the final result determined through majority voting.",
          "quote": [
            {
              "text": "[176]",
              "target": "#b176",
              "type": "bibr",
              "context": ", REFINER ",
              "index": 128
            },
            {
              "text": "[261]",
              "target": "#b263",
              "type": "bibr",
              "context": "Xu et al. ",
              "index": 496
            }
          ]
        },
        {
          "text": "While the feedback and correction mechanisms of LLMs judges are continually evolving, the limitations of self-feedback in improving quality should not be overlooked. Research on Self-Correct [93, 223] shows that, the intrinsic self-correction capabilities of LLMs often fall short of effectively improving reasoning quality. Valmeekam et al. [224] also raise concerns about the effectiveness of LLMs as self-validation tools in the absence of reliable external validators. Future research can focus on improving the accuracy of feedback provided by these LLM judges and incorporating external validation mechanisms to optimize their performance in complex reasoning tasks.",
          "quote": [
            {
              "text": "[93,",
              "target": "#b92",
              "type": "bibr",
              "context": "f-Correct ",
              "index": 191
            },
            {
              "text": "223]",
              "target": "#b225",
              "type": "bibr",
              "context": "rect [93, ",
              "index": 196
            },
            {
              "text": "[224]",
              "target": "#b226",
              "type": "bibr",
              "context": "am et al. ",
              "index": 342
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "3.3",
        "name": "Data Construction"
      },
      "p": [
        {
          "text": "Data collection is a crucial stage in the development of machine learning systems, especially those driven by the rapid advancements in deep learning. The quality of the data directly determines the performance of the trained models. The LLMs-as-judges has significantly transformed the landscape of data collection, substantially reducing reliance on human effort. In this section, we will explore the pivotal role of LLMs-as-judges in data collection from two key perspectives: Data Annotation ( §3.3.1) and Data Synthesize ( §3.3.2).",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "3.3.1",
        "name": "Data"
      },
      "p": [
        {
          "text": "Annotation. Data Annotation involves leveraging LLM judges to label large, unlabeled datasets efficiently [71, 79, 85, 217]. By utilizing the advanced natural language understanding and reasoning capabilities of LLMs, the annotation process can be automated to a significant extent, enabling the generation of high-quality labels with reduced human intervention.",
          "quote": [
            {
              "text": "[71,",
              "target": "#b70",
              "type": "bibr",
              "context": "ficiently ",
              "index": 106
            },
            {
              "text": "79,",
              "target": "#b78",
              "type": "bibr",
              "context": "ntly [71, ",
              "index": 111
            },
            {
              "text": "85,",
              "target": "#b84",
              "type": "bibr",
              "context": " [71, 79, ",
              "index": 115
            },
            {
              "text": "217]",
              "target": "#b219",
              "type": "bibr",
              "context": ", 79, 85, ",
              "index": 119
            }
          ]
        },
        {
          "text": "LLMs have demonstrated remarkable potential in text annotation tasks, consistently outperforming traditional methods and human annotators in various settings. He et al. [85] evaluated the performance of GPT-4 in crowdsourced data annotation workflows, particularly in text annotation tasks. Their comparative study revealed that, even with best practices, the highest accuracy achievable by MTurk workers was 81.5%, whereas GPT-4 achieved an accuracy of 83.6%. Similarly, Gilardi et al. [71] analyzed 6,183 tweets and news articles, demonstrating that ChatGPT outperformed crowdsourced workers in tasks such as stance detection, topic detection, and framing. Törnberg et al. [217] further investigated the classification of Twitter users' political leanings based on their tweet content. Their findings revealed that ChatGPT-4 not only surpassed human classifiers in accuracy and reliability but also exhibited bias levels that were comparable to or lower than those of human classifiers.",
          "quote": [
            {
              "text": "[85]",
              "target": "#b84",
              "type": "bibr",
              "context": "He et al. ",
              "index": 169
            },
            {
              "text": "[71]",
              "target": "#b70",
              "type": "bibr",
              "context": "di et al. ",
              "index": 487
            },
            {
              "text": "[217]",
              "target": "#b219",
              "type": "bibr",
              "context": "rg et al. ",
              "index": 675
            }
          ]
        },
        {
          "text": "As technology advances, more and more research is exploring their application in multimodal data annotation. For example, the FullAnno [79] uses the GPT-4V model to generate image annotations, significantly improving the quality of image descriptions through a multi-stage annotation process. Furthermore, Latif et al. [117] explored the application of LLMs in speech emotion annotation, demonstrating that, with data augmentation, LLM-annotated samples can significantly enhance the performance of speech emotion recognition models. By integrating text, audio features, and gender information, the effectiveness of LLM-based annotations was further improved, highlighting their potential in advancing multimodal annotation tasks.",
          "quote": [
            {
              "text": "[79]",
              "target": "#b78",
              "type": "bibr",
              "context": " FullAnno ",
              "index": 135
            },
            {
              "text": "[117]",
              "target": "#b116",
              "type": "bibr",
              "context": "if et al. ",
              "index": 319
            }
          ]
        },
        {
          "text": "As LLMs perform excellently in annotation tasks, researchers are actively exploring methods to further improve annotation quality and address potential challenges. For example, AnnoLLM [83] introducedthe \"explain-then-annotate\" method, which enhances both the accuracy and transparency of annotations by prompting the LLM to justify its label assignments. Additionally, the LLMAAA [282] framework incorporates an active learning strategy to efficiently select highinformation samples for annotation, thereby mitigating the effects of noisy labels and reducing the reliance on costly human annotation. These approach not only enhance the performance of task-specific models but also offer new perspectives on the efficient application of LLMs in annotation workflows.",
          "quote": [
            {
              "text": "[83]",
              "target": "#b82",
              "type": "bibr",
              "context": ", AnnoLLM ",
              "index": 185
            },
            {
              "text": "[282]",
              "target": "#b284",
              "type": "bibr",
              "context": "he LLMAAA ",
              "index": 381
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "3.3.2",
        "name": "Data"
      },
      "p": [
        {
          "text": "Synthesize. The goal of Data Synthesis is to create entirely new data, either from scratch or based on seed data, while ensuring it is similar in distribution to real data. Data Synthesis enables the generation of diverse data samples, enhancing a model's generalization ability to unseen examples while reducing reliance on sensitive real-world data [2, 52, 108, 157, 235, 268].",
          "quote": [
            {
              "text": "[2,",
              "target": "#b1",
              "type": "bibr",
              "context": "orld data ",
              "index": 351
            },
            {
              "text": "52,",
              "target": "#b51",
              "type": "bibr",
              "context": " data [2, ",
              "index": 355
            },
            {
              "text": "108,",
              "target": "#b107",
              "type": "bibr",
              "context": "a [2, 52, ",
              "index": 359
            },
            {
              "text": "157,",
              "target": "#b157",
              "type": "bibr",
              "context": " 52, 108, ",
              "index": 364
            },
            {
              "text": "235,",
              "target": "#b237",
              "type": "bibr",
              "context": "108, 157, ",
              "index": 369
            },
            {
              "text": "268]",
              "target": "#b270",
              "type": "bibr",
              "context": "157, 235, ",
              "index": 374
            }
          ]
        },
        {
          "text": "In recent years, advancements in LLMs have led to significant improvements in both the quality and efficiency of data synthesis methods. In this domain, methods like SELFEE [268] and SynPO [52] have effectively enhanced the alignment capabilities of LLMs by leveraging small amounts of labeled data and iteratively generating preference-aligned data. Arif et al. [2] also introduce a multi-agent workflow for generating optimized preference datasets. SELF-INSTRUCT [235] and Evol-Instruct [254, 278] represent innovative approaches to improving model alignment and performance through self-generated instruction data. SELF-INSTRUCT [235] requires minimal human annotation, instead relying on self-generated instruction data to align pre-trained models. Evol-Instruct [254, 278] further enhances LLM performance by automatically generating instruction data, significantly boosting model capabilities.",
          "quote": [
            {
              "text": "[268]",
              "target": "#b270",
              "type": "bibr",
              "context": "ke SELFEE ",
              "index": 173
            },
            {
              "text": "[52]",
              "target": "#b51",
              "type": "bibr",
              "context": "and SynPO ",
              "index": 189
            },
            {
              "text": "[2]",
              "target": "#b1",
              "type": "bibr",
              "context": "if et al. ",
              "index": 363
            },
            {
              "text": "[235]",
              "target": "#b237",
              "type": "bibr",
              "context": "-INSTRUCT ",
              "index": 465
            },
            {
              "text": "[254,",
              "target": "#b256",
              "type": "bibr",
              "context": "-Instruct ",
              "index": 489
            },
            {
              "text": "278]",
              "target": "#b280",
              "type": "bibr",
              "context": "uct [254, ",
              "index": 495
            },
            {
              "text": "[235]",
              "target": "#b237",
              "type": "bibr",
              "context": "-INSTRUCT ",
              "index": 632
            },
            {
              "text": "[254,",
              "target": "#b256",
              "type": "bibr",
              "context": "-Instruct ",
              "index": 767
            },
            {
              "text": "278]",
              "target": "#b280",
              "type": "bibr",
              "context": "uct [254, ",
              "index": 773
            }
          ]
        },
        {
          "text": "STaR [277] and ReSTEM [199] are research efforts aimed at enhancing reasoning capabilities through synthetic data. STaR [277] employs a self-guided iterative process to improve model performance on complex reasoning tasks, offering an effective solution for tackling increasingly sophisticated reasoning challenges in the future. ReSTEM [199], on the other hand, utilizes a self-training approach based on the expectation-maximization framework to enhance the problemsolving capabilities of large language models, particularly in areas such as solving mathematical problems and generating code.",
          "quote": [
            {
              "text": "[277]",
              "target": "#b279",
              "type": "bibr",
              "context": "STaR ",
              "index": 5
            },
            {
              "text": "[199]",
              "target": "#b201",
              "type": "bibr",
              "context": "nd ReSTEM ",
              "index": 22
            },
            {
              "text": "[277]",
              "target": "#b279",
              "type": "bibr",
              "context": "ata. STaR ",
              "index": 120
            },
            {
              "text": "[199]",
              "target": "#b201",
              "type": "bibr",
              "context": "e. ReSTEM ",
              "index": 337
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "4",
        "name": "METHODOLOGY"
      },
      "p": [
        {
          "text": "The use of LLM judges requires careful methodological considerations to ensure the accuracy and consistency of judgments. Researchers have developed various approaches according to the complexity and specific requirements of different judgment tasks, each offering unique advantages. In this section, we categorize these methodologies into three broad approaches: Single-LLM System ( §4.1): evaluation by a single-LLM, Multi-LLM System ( §4.2): evaluation by cooperation among multi-LLMs, and Human-AI Collaboration ( §4.3): evaluation by cooperation of LLMs and Human. Figure  presents an overview of methodology.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "4.1",
        "name": "Single-LLM System"
      },
      "p": [
        {
          "text": "Single-LLM System relies on a single model to perform judgment tasks, with its effectiveness largely determined by the LLM's capabilities and the strategies used to process input data. This approach can generally be divided into three fundamental components: Prompt Engineering ( §4.1.1), Tuning ( §4.1.2), and Post-processing ( §4.1.3) of model outputs.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "4.1.1",
        "name": "Prompt-based."
      },
      "p": [
        {
          "text": "Prompt engineering [189] involves crafting clear and structured input prompts tailored to elicit accurate and contextually appropriate responses from LLM judges. This approach is crucial for ensuring that LLMs grasp the complexities of specific tasks and provide relevant, consistent, and goal-aligned judgments. In many cases, well-designed prompts significantly reduce the need for extensive model training.",
          "quote": [
            {
              "text": "[189]",
              "target": "#b189",
              "type": "bibr",
              "context": "gineering ",
              "index": 19
            }
          ]
        },
        {
          "text": "In-Context Learning. In-Context Learning (ICL) is a distinctive capability of LLMs that allows them to dynamically adapt to evaluation tasks using carefully curated examples or explanations within the prompt [53]. Several recent methods have demonstrated the power of ICL in LLM-asjudges, showcasing how it enhances the flexibility and performance of LLMs in diverse settings. For example, GPTScore [68] leverages the few-shot learning capability of generative pre-trained models to evaluate generated text. By using relevant examples to customize prompts, it provides a flexible, training-free approach to assess multiple aspects of text quality. Similarly, LLM-EVAL [141] incorporates carefully crafted examples into prompts, proposing a unified, multi-dimensional automatic evaluation method for open-domain dialogue. Another notable example is TALEC [280], a model-based evaluation method that leverages in-context learning to enable users to set custom evaluation criteria for LLMs in specific domains. Through careful prompt engineering, users can iteratively adjust the examples to refine the evaluation process as needed. In addition, Jain et al. [94] proposed the In-Context Learning-based Evaluator (ICE) for multi-dimensional text evaluation. ICE leverages LLMs and a small number of in-context examples to evaluate generated text summaries, achieving competitive results.",
          "quote": [
            {
              "text": "[53]",
              "target": "#b52",
              "type": "bibr",
              "context": "he prompt ",
              "index": 208
            },
            {
              "text": "[68]",
              "target": "#b67",
              "type": "bibr",
              "context": " GPTScore ",
              "index": 399
            },
            {
              "text": "[141]",
              "target": "#b140",
              "type": "bibr",
              "context": " LLM-EVAL ",
              "index": 668
            },
            {
              "text": "[280]",
              "target": "#b282",
              "type": "bibr",
              "context": " is TALEC ",
              "index": 854
            },
            {
              "text": "[94]",
              "target": "#b93",
              "type": "bibr",
              "context": "in et al. ",
              "index": 1155
            }
          ]
        },
        {
          "text": "While ICL can enable effective evaluation, it is not without challenges. One major issue is that the model's responses may be influenced by the selection of prompt examples, potentially leading to bias [62, 78, 290, 296]. To address this issue, Hasanbeig et al. proposed ALLURE [81], a comprehensive protocol designed to mitigate bias in ICL for LLMs during text evaluation. ALLURE [81] improves evaluator accuracy by iteratively incorporating discrepancies between its assessments and annotated data into the learning context. Moreover, after uncovering the existence of symbol bias within LLM evaluators when using ICL, Song et al. [204] proposed two effective mitigation strategy prompt templates, Many-Shot with Reference (MSwR) and Many-Shot without Reference (MSoR), to bolster the reliability and precision of LLM-based assessments.",
          "quote": [
            {
              "text": "[62,",
              "target": "#b61",
              "type": "bibr",
              "context": "g to bias ",
              "index": 202
            },
            {
              "text": "78,",
              "target": "#b77",
              "type": "bibr",
              "context": "bias [62, ",
              "index": 207
            },
            {
              "text": "290,",
              "target": "#b292",
              "type": "bibr",
              "context": " [62, 78, ",
              "index": 211
            },
            {
              "text": "296]",
              "target": "#b298",
              "type": "bibr",
              "context": " 78, 290, ",
              "index": 216
            },
            {
              "text": "[81]",
              "target": "#b80",
              "type": "bibr",
              "context": "ed ALLURE ",
              "index": 278
            },
            {
              "text": "[81]",
              "target": "#b80",
              "type": "bibr",
              "context": "n. ALLURE ",
              "index": 382
            },
            {
              "text": "[204]",
              "target": "#b206",
              "type": "bibr",
              "context": "ng et al. ",
              "index": 634
            }
          ]
        },
        {
          "text": "Step-by-step.",
          "quote": []
        },
        {
          "text": "Step-by-step involves breaking down complex evaluation tasks into fine-grained components, leveraging the reasoning capabilities of LLMs to simplify the evaluation process. The most straightforward example of which is perhaps Chain-of-Thought (CoT) [113, 242]. Building on that, frameworks like G-EVAL [145] have been proposed to assess the quality of NLG outputs. G-EVAL [145] combines CoT with a form-filling paradigm, allowing the LLM to assess outputs in a structured manner. Similarly, ICE-Score [304] introduces a step-by-step framework for evaluating code, in which the LLM is instructed with task definitions, evaluation criteria, and detailed evaluation steps. By breaking the task down into clear steps, ICE-Score [304] improves the quality and consistency of code evaluation. Also, ProtocoLLM [271] employs a similar step-by-step approach to evaluate the specialized capabilities of LLMs in generating scientific protocols. Portia [134] achieves better evaluation results in a lightweight yet effective manner. It divides the answer into multiple parts, aligns similar content between candidate answers, and then merges them back into a single prompt for evaluation by the LLM.",
          "quote": [
            {
              "text": "[113,",
              "target": "#b112",
              "type": "bibr",
              "context": "ght (CoT) ",
              "index": 249
            },
            {
              "text": "242]",
              "target": "#b244",
              "type": "bibr",
              "context": "oT) [113, ",
              "index": 255
            },
            {
              "text": "[145]",
              "target": "#b145",
              "type": "bibr",
              "context": "ke G-EVAL ",
              "index": 302
            },
            {
              "text": "[145]",
              "target": "#b145",
              "type": "bibr",
              "context": "s. G-EVAL ",
              "index": 372
            },
            {
              "text": "[304]",
              "target": "#b306",
              "type": "bibr",
              "context": "ICE-Score ",
              "index": 501
            },
            {
              "text": "[304]",
              "target": "#b306",
              "type": "bibr",
              "context": "ICE-Score ",
              "index": 724
            },
            {
              "text": "[271]",
              "target": "#b273",
              "type": "bibr",
              "context": "rotocoLLM ",
              "index": 804
            },
            {
              "text": "[134]",
              "target": "#b133",
              "type": "bibr",
              "context": "s. Portia ",
              "index": 942
            }
          ]
        },
        {
          "text": "Some studies break down evaluations into two steps: \"explanation-rating. \" This approach suggests that providing an explanation enhances the reliability of the rating. Chiang et al. [36] offer empirical guidelines to improve the quality of LLM evaluations, demonstrating that combining rating with explanation (rate-explain) or explanation with rating (explain-rate) leads to higher correlations with human ratings. Another effective strategy is to decompose complex evaluation standards into specific, discrete criteria, allowing the LLM to assess each aspect independently. FineSurE [203] is an advanced example of this method, offering a framework for the fine-grained evaluation of text summarization quality. It breaks down the evaluation into multiple dimensions, such as faithfulness, completeness, and conciseness. Through detailed analysis, including fact-checking and key fact alignment, FineSurE [203] outperforms traditional methods in terms of evaluation accuracy.",
          "quote": [
            {
              "text": "[36]",
              "target": "#b35",
              "type": "bibr",
              "context": "ng et al. ",
              "index": 182
            },
            {
              "text": "[203]",
              "target": "#b205",
              "type": "bibr",
              "context": " FineSurE ",
              "index": 585
            },
            {
              "text": "[203]",
              "target": "#b205",
              "type": "bibr",
              "context": " FineSurE ",
              "index": 907
            }
          ]
        },
        {
          "text": "Definition Augmentation. The Enhanced Definition approach involves refining prompts to inject improved evaluation criteria, establish assessment principles, or incorporate external knowledge into the LLM judge's decision-making process. Some studies focus on enriching and clarifying the prompts to ensure that the evaluation criteria are both comprehensive and welldefined.",
          "quote": []
        },
        {
          "text": "For example, Liu et al. propose AUTOCALIBRATE [146], a multi-stage, gradient-free approach. This method involves the drafting, revision, and application of calibrated criteria, and it automatically calibrates and aligns an LLM-based evaluator to match human preferences for NLG quality assessment. Furthermore, SALC [76] enables LLMs to autonomously generate context-aware evaluation criteria for self-assessment, overcoming the limitations of static, human-defined metrics. On the other hand, the LLM-as-a-Personalized-Judge approach [54] introduces a novel perspective by incorporating diverse evaluative roles and principles. This allows LLMs to adapt to complex, varied evaluation scenarios, resulting in more nuanced and context-sensitive assessments.",
          "quote": [
            {
              "text": "[146]",
              "target": "#b146",
              "type": "bibr",
              "context": "CALIBRATE ",
              "index": 46
            },
            {
              "text": "[76]",
              "target": "#b75",
              "type": "bibr",
              "context": "ore, SALC ",
              "index": 316
            },
            {
              "text": "[54]",
              "target": "#b53",
              "type": "bibr",
              "context": " approach ",
              "index": 535
            }
          ]
        },
        {
          "text": "Another key aspect of Definition Augmentation is the retrieval of external knowledge, which helps reduce hallucinations and provides more factual support. For instance, BiasAlert [61], a tool designed to detect social bias in LLM-generated open-text outputs. It integrates external human knowledge with the LLM judge's inherent reasoning capabilities to reliably identify and mitigate bias, outperforming GPT4-as-A-Judge across various scenarios. Moreover, Chen et al. [33] found that within retrieval-augmented generation (RAG) frameworks, LLM judges do not exhibit a significant self-preference effect during evaluation.",
          "quote": [
            {
              "text": "[61]",
              "target": "#b60",
              "type": "bibr",
              "context": "BiasAlert ",
              "index": 179
            },
            {
              "text": "[33]",
              "target": "#b32",
              "type": "bibr",
              "context": "en et al. ",
              "index": 469
            }
          ]
        },
        {
          "text": "Multi-turn Optimization. Multi-turn optimization involves iterative interactions between the evaluator and the evaluated entity, refining evaluation results through diverse forms of feedback, thus fostering deeper analysis and a progressive improvement in evaluation quality [295]. Unlike traditional methods that rely on predefined criteria, Xu et al. proposed ACTIVE-CRITIC [256], enabling LLMs to infer evaluation criteria from data and dynamically optimize prompts through multiple rounds of interaction. Moreover, Some studies [10, 153, 273, 286] leverage LLMs as question designers to engage in dynamic interactions with the evaluated entities, adjusting the questions and task design in real time. This allows for flexible modification of the evaluation content based on the performance of the evaluated entity, thereby enabling more comprehensive assessments.",
          "quote": [
            {
              "text": "[295]",
              "target": "#b297",
              "type": "bibr",
              "context": "n quality ",
              "index": 275
            },
            {
              "text": "[256]",
              "target": "#b258",
              "type": "bibr",
              "context": "VE-CRITIC ",
              "index": 376
            },
            {
              "text": "[10,",
              "target": "#b9",
              "type": "bibr",
              "context": "e studies ",
              "index": 532
            },
            {
              "text": "153,",
              "target": "#b153",
              "type": "bibr",
              "context": "dies [10, ",
              "index": 537
            },
            {
              "text": "273,",
              "target": "#b275",
              "type": "bibr",
              "context": "[10, 153, ",
              "index": 542
            },
            {
              "text": "286]",
              "target": "#b288",
              "type": "bibr",
              "context": "153, 273, ",
              "index": 547
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "4.1.2",
        "name": "Tuning-based."
      },
      "p": [
        {
          "text": "Tuning involves training a pre-existing LLM on a specialized dataset to adapt it to specific judgment tasks. It's especially useful when the judgment domain involves highly specialized knowledge or nuanced decision-making [92].",
          "quote": [
            {
              "text": "[92]",
              "target": "#b91",
              "type": "bibr",
              "context": "on-making ",
              "index": 222
            }
          ]
        },
        {
          "text": "Score-based Tuning. Score-based tuning involves using data with scores to train models and enhance their ability to predict judgment scores based on specific evaluation criteria [28, 47, 229].",
          "quote": [
            {
              "text": "[28,",
              "target": "#b27",
              "type": "bibr",
              "context": " criteria ",
              "index": 178
            },
            {
              "text": "47,",
              "target": "#b46",
              "type": "bibr",
              "context": "eria [28, ",
              "index": 183
            },
            {
              "text": "229]",
              "target": "#b231",
              "type": "bibr",
              "context": " [28, 47, ",
              "index": 187
            }
          ]
        },
        {
          "text": "Many studies have explored the enhancement of LLM-as-judges by fine-tuning them on humanlabeled datasets. For instance, PHUDGE [47], fine-tuned from the Phi-3 model, achieves stateof-the-art performance in terms of latency and throughput when automatically evaluating the quality of outputs from LLMs. This fine-tuning process equips the model with the necessary judgment skills, enabling it to assess various types of content in a structured and accurate manner. Additionally, ECT [229] introduces a novel method for transferring scoring capabilities from LLMs to lighter models. This allows the lighter models to function as effective reward models for sequence generation tasks, enhancing sequence generation models through reinforcement learning and reranking approaches. AttrScore [276] is another framework for evaluating attribution and identifying specific types of attribution errors, using a curated test set from a generative search engine and simulated examples from existing benchmarks. The above research highlights that LLMs can better align their decision-making process with humans through fine-tuning with human-constructed datasets.",
          "quote": [
            {
              "text": "[47]",
              "target": "#b46",
              "type": "bibr",
              "context": "e, PHUDGE ",
              "index": 127
            },
            {
              "text": "[229]",
              "target": "#b231",
              "type": "bibr",
              "context": "ally, ECT ",
              "index": 482
            },
            {
              "text": "[276]",
              "target": "#b278",
              "type": "bibr",
              "context": "AttrScore ",
              "index": 786
            }
          ]
        },
        {
          "text": "In addition to human-labeled data, some studies have also attempted to fine-tune models using synthetic datasets like SorryBench [249] generated for evaluation tasks. These datasets are often created through rule-based methods or by generating artificial evaluation examples, which also give rise to some metrics like TIGERScore [99]. SELF-J [266] is a self-training framework for developing judge models to evaluate LLMs' adherence to human instructions without human-annotated quality scores. SELF-J [266] proposes selective instruction following, allowing systems to decline lowquality instructions. FENCE [252] is another factuality evaluator designed to provide claim-level feedback to language model generators. It details a data augmentation approach that enriches public datasets with textual critiques and diverse source documents from various tools, thereby enhancing factuality without introducing lesser-known facts. Utilizing synthetic training data to fine-tune lightweight language model judges and employing prediction-powered inference (PPI) for statistical confidence to mitigate potential prediction errors, ARES [188] can automatically assess RAG systems.",
          "quote": [
            {
              "text": "[249]",
              "target": "#b251",
              "type": "bibr",
              "context": "orryBench ",
              "index": 129
            },
            {
              "text": "[99]",
              "target": "#b98",
              "type": "bibr",
              "context": "IGERScore ",
              "index": 329
            },
            {
              "text": "[266]",
              "target": "#b268",
              "type": "bibr",
              "context": "]. SELF-J ",
              "index": 342
            },
            {
              "text": "[266]",
              "target": "#b268",
              "type": "bibr",
              "context": "s. SELF-J ",
              "index": 502
            },
            {
              "text": "[252]",
              "target": "#b254",
              "type": "bibr",
              "context": "ns. FENCE ",
              "index": 609
            },
            {
              "text": "[188]",
              "target": "#b188",
              "type": "bibr",
              "context": "ors, ARES ",
              "index": 1132
            }
          ]
        },
        {
          "text": "Preference-based Learning. Preference-based learning focuses on training LLMs to make inferences and learn based on preferences, enabling the development of more adaptive and customizable evaluation capabilities.",
          "quote": []
        },
        {
          "text": "Initially, researchers leverage these data in conjunction with advanced techniques like Direct Preference Optimization (DPO) [181] to train LLMs for more nuanced evaluative capabilities. In this method, the model is trained to predict which of two outputs is preferred according to humanlike values, rather than learning a scalar reward signal. Such self-improving approach is well reflected in Meta-Rewarding [245]. Con-J [270] trains a generative judge by using the DPO loss on contrastive judgments and the SFT loss on positive judgments to align LLMs with human values. In terms of evaluating other LLMs effectively in open-ended scenarios, JudgeLM [301] addresses key biases in the fine-tuning process with a high-quality preference dataset. Another typical method is PandaLM [236], which is trained on a reliable human-annotated preference dataset, focusing extends beyond just the objective correctness of responses, and addresses vital subjective factors. Moreover, Self-Taught [231] is another approach to train LLMs as effective evaluators without relying on human-annotated preference judgments, using synthetic training data only. Through an iterative self-improvement scheme, LLM judges are able to produce reasoning traces and final judgments. Not quite the same, FedEval-LLM [84] fine-tunes many personalized LLMs without relying on labeled datasets to provide domain-specific evaluation, mitigating biases associated with single referees. It is designed to assess the performance of LLMs on downstream tasks, at the same time, ensuring privacy preservation.",
          "quote": [
            {
              "text": "[181]",
              "target": "#b181",
              "type": "bibr",
              "context": "ion (DPO) ",
              "index": 125
            },
            {
              "text": "[245]",
              "target": "#b247",
              "type": "bibr",
              "context": "Rewarding ",
              "index": 410
            },
            {
              "text": "[270]",
              "target": "#b272",
              "type": "bibr",
              "context": "5]. Con-J ",
              "index": 423
            },
            {
              "text": "[301]",
              "target": "#b303",
              "type": "bibr",
              "context": ", JudgeLM ",
              "index": 653
            },
            {
              "text": "[236]",
              "target": "#b238",
              "type": "bibr",
              "context": "s PandaLM ",
              "index": 781
            },
            {
              "text": "[231]",
              "target": "#b233",
              "type": "bibr",
              "context": "lf-Taught ",
              "index": 986
            },
            {
              "text": "[84]",
              "target": "#b83",
              "type": "bibr",
              "context": "dEval-LLM ",
              "index": 1290
            }
          ]
        },
        {
          "text": "As research has progressed, newer methods have emerged that combine both score-based and preference-based data to refine model evaluation capabilities, not to mention some novel metrics like INSTRUCTSCORE [258]. FLAMe [226] is an example of such an approach. It's a family of Foundational Large Autorater Models which significantly improves generalization to a wide variety of held-out tasks using both pointwise and pairwise methods during training. As generative judge model, AUTO-J [130] addresses challenges in generality, flexibility, and interpretability by training on a diverse dataset containing scoring and preference. To critique and refine the outputs of large language models, Shepherd [232] leverages a high-quality feedback dataset to identify errors and suggest improvements across various domains. In the domain of NLG, X-EVAL [142] consists of a vanilla instruction tuning stage and an enhanced instruction tuning stage that exploits connections between fine-grained evaluation aspects. Notably, Themis [88] also achieved outstanding results acting as a reference-free NLG evaluation language model designed for flexibility and interpretability. Similarly, CritiqueLLM [106] provides effective and explainable evaluations of LLM outputs, and uses a dialogue-based prompting method to generate high-quality referenced and reference-free evaluation data. Self-Rationalization [220] enhances LLM performance by iteratively fine-tuning the judge via DPO, which allows LLMs to learn from their own reasoning. Based on pointwise and pairwise dataset, CompassJudger-1 [20] acts as an open-source, versatile LLM for efficient and accurate evaluation of other LLMs. Likewise, Zhou et al. [294] introduces a systematic framework for bias reduction, employing calibration for closed-source models and contrastive training for open-source models. Apart from that, HALU-J [227] is designed to enhance hallucination detection in LLMs by selecting pertinent evidence and providing detailed critiques. PROMETHEUS [109] and PROMETHEUS 2 [110] are open-source LLMs specialized for fine-grained evaluation that can generalize to diverse, real-world scoring rubrics beyond a single-dimensional preference, supporting both direct assessment and pairwise ranking, and can evaluate based on custom criteria. What's more, the following PROMETHEUS-VISION [122] fills the gap in the visual field. As for  [174] Human & LLM Bias Detection 268K Pairwise RLHF Llama3-8B-Instruct PandaLM [236] Human Various 300K Pairwise SFT Llama-7B PHUDGE [47] Human",
          "quote": [
            {
              "text": "[258]",
              "target": "#b260",
              "type": "bibr",
              "context": "RUCTSCORE ",
              "index": 205
            },
            {
              "text": "[226]",
              "target": "#b228",
              "type": "bibr",
              "context": "8]. FLAMe ",
              "index": 218
            },
            {
              "text": "[130]",
              "target": "#b129",
              "type": "bibr",
              "context": "l, AUTO-J ",
              "index": 485
            },
            {
              "text": "[232]",
              "target": "#b234",
              "type": "bibr",
              "context": " Shepherd ",
              "index": 699
            },
            {
              "text": "[142]",
              "target": "#b141",
              "type": "bibr",
              "context": "G, X-EVAL ",
              "index": 844
            },
            {
              "text": "[88]",
              "target": "#b87",
              "type": "bibr",
              "context": "y, Themis ",
              "index": 1021
            },
            {
              "text": "[106]",
              "target": "#b105",
              "type": "bibr",
              "context": "itiqueLLM ",
              "index": 1187
            },
            {
              "text": "[220]",
              "target": "#b222",
              "type": "bibr",
              "context": "alization ",
              "index": 1392
            },
            {
              "text": "[20]",
              "target": "#b19",
              "type": "bibr",
              "context": "sJudger-1 ",
              "index": 1579
            },
            {
              "text": "[294]",
              "target": "#b296",
              "type": "bibr",
              "context": "ou et al. ",
              "index": 1697
            },
            {
              "text": "[227]",
              "target": "#b229",
              "type": "bibr",
              "context": "t, HALU-J ",
              "index": 1877
            },
            {
              "text": "[109]",
              "target": "#b108",
              "type": "bibr",
              "context": "ROMETHEUS ",
              "index": 2015
            },
            {
              "text": "[110]",
              "target": "#b109",
              "type": "bibr",
              "context": "METHEUS 2 ",
              "index": 2038
            },
            {
              "text": "[122]",
              "target": "#b121",
              "type": "bibr",
              "context": "US-VISION ",
              "index": 2348
            },
            {
              "text": "[174]",
              "target": "#b174",
              "type": "bibr",
              "context": ". As for  ",
              "index": 2397
            },
            {
              "text": "[236]",
              "target": "#b238",
              "type": "bibr",
              "context": "t PandaLM ",
              "index": 2476
            },
            {
              "text": "[47]",
              "target": "#b46",
              "type": "bibr",
              "context": "7B PHUDGE ",
              "index": 2530
            }
          ]
        },
        {
          "text": "Human Various -Pointwise & Pairwise SFT Llama-7B SorryBench [249] Human & GPT-4 Unsafe Topics 2.7K Pointwise SFT Multiple LLMs Themis [88] Human & GPT-4 NLG 67K",
          "quote": [
            {
              "text": "[249]",
              "target": "#b251",
              "type": "bibr",
              "context": "orryBench ",
              "index": 60
            },
            {
              "text": "[88]",
              "target": "#b87",
              "type": "bibr",
              "context": "Ms Themis ",
              "index": 134
            }
          ]
        },
        {
          "text": "Pointwise & Pairwise SFT & DPO Llama3-8B TIGERScore [99] Human & GPT-4 Text Generation 42K Pointwise SFT Llama2-7B & 13B X-EVAL [142] Human NLG 55,602",
          "quote": [
            {
              "text": "[99]",
              "target": "#b98",
              "type": "bibr",
              "context": "IGERScore ",
              "index": 52
            },
            {
              "text": "[142]",
              "target": "#b141",
              "type": "bibr",
              "context": "3B X-EVAL ",
              "index": 128
            }
          ]
        },
        {
          "text": "Pointwise & Pairwise SFT Flan-T5 various multimodal tasks, LLaVA-Critic [253] demonstrates its effectiveness in providing reliable evaluation scores and generating reward signals for preference learning, highlighting the potential of open-source LMMs in self-critique and evaluation.",
          "quote": [
            {
              "text": "[253]",
              "target": "#b255",
              "type": "bibr",
              "context": "VA-Critic ",
              "index": 72
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "4.1.3",
        "name": "Post-processing."
      },
      "p": [
        {
          "text": "Post-processing involves further refining evaluation results to extract more precise and reliable outcomes. This step typically includes analyzing the initial outputs to identify patterns, inconsistencies, or areas requiring improvement, followed by targeted adjustments and in-depth analysis. By addressing these issues, post-processing ensures that the evaluation results are not only accurate but also aligned with the specific objectives and standards of the task. Probability Calibration. During the post-hoc process of the model output, some studies use rigorous mathematical derivations to quantify the differences, thereby optimizing them. For instance, Daynauth et al. [45] investigates the discrepancy between human preferences and automated evaluations in language model assessments, particularly employs Bayesian statistics and a t-test to quantify bias towards higher token counts, and develops a recalibration procedure to adjust the GPTScorers. Apart from that, ProbDiff [247] is another novel self-evaluation method for LLMs that assesses model efficacy by computing the probability discrepancy between initial responses and their revised versions. Moreover, Liusie et al. [150] introduces a Product of Experts (PoE) framework for efficient comparative assessment using LLMs, which yield an expression that can be maximized with respect to the underlying set of candidates. This paper proposes two experts, a soft Bradley-Terry expert and a Gaussian expert that has closed-form solutions. Unlike from frameworks above, CRISPR [264] is a novel bias mitigation method for LLMs executing instruction-based tasks, which identifies and prunes bias neurons with probability calibration, reducing bad performance without compromising pre-existing knowledge.",
          "quote": [
            {
              "text": "[45]",
              "target": "#b44",
              "type": "bibr",
              "context": "th et al. ",
              "index": 678
            },
            {
              "text": "[247]",
              "target": "#b249",
              "type": "bibr",
              "context": " ProbDiff ",
              "index": 986
            },
            {
              "text": "[150]",
              "target": "#b150",
              "type": "bibr",
              "context": "ie et al. ",
              "index": 1189
            },
            {
              "text": "[264]",
              "target": "#b266",
              "type": "bibr",
              "context": "e, CRISPR ",
              "index": 1542
            }
          ]
        },
        {
          "text": "Text Reprocessing. In LLMs-as-judges, text reprocessing methods are essential for enhancing the accuracy and reliability of evaluation outcomes. Specifically, text processing can improve the evaluation process by integrating multiple evaluation results or outcomes from several rounds of assessment. For example, Sottana et al. [206] employs a multi-round evaluation process. Each round involves scoring model outputs based on specific criteria, with the human and GPT-4 evaluations ranking model performances from best to worst and averaging these rankings to mitigate subjectivity. For the single-response evaluation, AUTO-J [130] employs a \"divide-andconquer\" strategy. Critiques that either adhere to or deviate from the scenario-specific criteria are consolidated to form a comprehensive evaluation judgment and then generate the final assessment. Consistent with former aforementioned studies, Yan et al. [262] introduces a post-processing method to consolidate the relevance labels generated by LLMs. It demonstrates that this approach effectively combines both the ranking and labeling abilities of LLMs through post-processing. Furthermore, REVISEVAL [281] is a novel evaluation paradigm that enhances the reliability of LLM Judges by generating response-adapted references through text revision capabilities of LLMs. Apart from that, Tessler et al. [214] explores the use of AI as a mediator in democratic deliberation, aiming to help diverse groups find common ground on complex social and political issues. With the goal of maximizing group approval, the researchers developed the \"Habermas Machine\", which iteratively generate group statements based on individual opinions.",
          "quote": [
            {
              "text": "[206]",
              "target": "#b208",
              "type": "bibr",
              "context": "na et al. ",
              "index": 328
            },
            {
              "text": "[130]",
              "target": "#b129",
              "type": "bibr",
              "context": "n, AUTO-J ",
              "index": 627
            },
            {
              "text": "[262]",
              "target": "#b264",
              "type": "bibr",
              "context": "an et al. ",
              "index": 911
            },
            {
              "text": "[281]",
              "target": "#b283",
              "type": "bibr",
              "context": "REVISEVAL ",
              "index": 1160
            },
            {
              "text": "[214]",
              "target": "#b216",
              "type": "bibr",
              "context": "er et al. ",
              "index": 1359
            }
          ]
        },
        {
          "text": "Another category of text reprocessing methods involves task transformation, primarily focusing on the conversion between open-ended and multiple-choice question (MCQ) formats. Ren et al. [186] explores the use of self-evaluation to enhance the selective generation capabilities of LLMs. Specifically, the authors reformulate open-ended generation tasks into token-level prediction tasks, reduce sequence-level scores to token-level scores to improve quality calibration. Conversely, Myrzakhan et al. [161] introduces the Open-LLM-Leaderboard, a new benchmark for evaluating LLMs using open-style questions, which eliminates selection bias and random guessing issues associated with multiple-choice questions. It presents a method to identify suitable open-style questions and validate the correctness of LLM open-style responses against human-annotated ground-truths.",
          "quote": [
            {
              "text": "[186]",
              "target": "#b186",
              "type": "bibr",
              "context": "en et al. ",
              "index": 187
            },
            {
              "text": "[161]",
              "target": "#b161",
              "type": "bibr",
              "context": "an et al. ",
              "index": 500
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "4.2",
        "name": "Multi-LLM System"
      },
      "p": [
        {
          "text": "Multi-LLM Evaluation harnesses the collective intelligence of multiple LLMs to bolster the robustness and reliability of evaluations. By either facilitating inter-model communication or independently aggregating their outputs, these systems can effectively mitigate biases, leverage complementary strengths across different models, refine decision-making precision, and foster a more nuanced understanding of complex judgments.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "4.2.1",
        "name": "Communication."
      },
      "p": [
        {
          "text": "Communication means the dynamic flow of information between LLMs, which is pivotal for sparking insights and sharing rationales during the judgment process. Recent research has shown that communication among LLMs can enable emergent abilities through their interactions [261], leading to a cohesive decision-making process and better judgment performance. The Multi-LLM system can benefit from LLM interactions in two ways: cooperation and competition.",
          "quote": [
            {
              "text": "[261]",
              "target": "#b263",
              "type": "bibr",
              "context": "eractions ",
              "index": 270
            }
          ]
        },
        {
          "text": "Cooperation. Multi-LLMs can work together to achieve a common goal with information and rationales sharing through interactions to enhance the overall evaluation process. For example, Zhang et al. [285] proposed an architecture named WideDeep to aggregate information at the LLM's neuro-level. In addition, Xu et al. [261] introduced a multi-agent collaboration strategy that mimics the academic peer review process to enhance complex reasoning in LLMs. The approach involves agents creating solutions, reviewing each other's work, and revising their initial submissions based on feedback. Similarly, ABSEval [136] utilizes four agents for answer synthesize, critique, execution, and commonsense, to build the overall workflow. Although the cooperation can complement each other's strengths between LLMs to a certain degree, this method still includes the risk of groupthink, where similar models reinforce each other's biases rather than providing diverse insights.",
          "quote": [
            {
              "text": "[285]",
              "target": "#b287",
              "type": "bibr",
              "context": "ng et al. ",
              "index": 197
            },
            {
              "text": "[261]",
              "target": "#b263",
              "type": "bibr",
              "context": "Xu et al. ",
              "index": 317
            },
            {
              "text": "[136]",
              "target": "#b135",
              "type": "bibr",
              "context": ", ABSEval ",
              "index": 609
            }
          ]
        },
        {
          "text": "Competition. Multi-LLMs systems can also benefit from competitive or adversarial communication, i.e., LLMs argue or debate to evaluate each other's outputs [22, 132, 158, 286]. Such multi-LLMs systems could be categorized into centralized and decentralized structures [168].",
          "quote": [
            {
              "text": "[22,",
              "target": "#b21",
              "type": "bibr",
              "context": "s outputs ",
              "index": 156
            },
            {
              "text": "132,",
              "target": "#b131",
              "type": "bibr",
              "context": "puts [22, ",
              "index": 161
            },
            {
              "text": "158,",
              "target": "#b158",
              "type": "bibr",
              "context": "[22, 132, ",
              "index": 166
            },
            {
              "text": "286]",
              "target": "#b288",
              "type": "bibr",
              "context": "132, 158, ",
              "index": 171
            },
            {
              "text": "[168]",
              "target": "#b168",
              "type": "bibr",
              "context": "tructures ",
              "index": 268
            }
          ]
        },
        {
          "text": "In the centralized structure, a single central LLM acts as the orchestrator of the conversation, highlighting the efficiency of a unified decision-making process. Auto-Arena [286] is such a novel framework that automates the evaluation of LLMs through agent peer battles and committee discussions, aiming to provide timely and reliable assessments. In detail, the framework conducts multi-round debates between LLM candidates, and uses an LLM judge committee to decide the winner. Inspired by courtroom dynamics, Bandi and Harrasse [12] propose two architectures, MORE and SAMRE, which utilize multiple advocates and iterative debates to dynamically assess LLM outputs.",
          "quote": [
            {
              "text": "[286]",
              "target": "#b288",
              "type": "bibr",
              "context": "uto-Arena ",
              "index": 174
            },
            {
              "text": "[12]",
              "target": "#b11",
              "type": "bibr",
              "context": " Harrasse ",
              "index": 532
            }
          ]
        },
        {
          "text": "In contrast, the decentralized structure emphasizes a collective intelligence where all models engage in direct communication, promoting a resilient and distributed decision-making structure. In the domain of LLM debates, Moniri et al. [158] introduced a unique automated benchmarking framework, employing another LLM as the judge to assess not only the models' domain knowledge but also their abilities in problem definition and inconsistency recognition. ChatEval [22] is another multi-agent debate framework that utilizes multiple LLMs with diverse role prompts and communication strategies on open-ended questions and traditional NLG tasks, significantly improves evaluation performance compared to single-agent methods. Moreover, PRD [132] applied peer rank and discussion to address issues like self-enhancement and positional bias in current LLM evaluation methods, leading to better alignment with human judgments and a path for fair model capability ranking.",
          "quote": [
            {
              "text": "[158]",
              "target": "#b158",
              "type": "bibr",
              "context": "ri et al. ",
              "index": 236
            },
            {
              "text": "[22]",
              "target": "#b21",
              "type": "bibr",
              "context": " ChatEval ",
              "index": 466
            },
            {
              "text": "[132]",
              "target": "#b131",
              "type": "bibr",
              "context": "over, PRD ",
              "index": 739
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "4.2.2",
        "name": "Aggregation."
      },
      "p": [
        {
          "text": "Alternatively, in multi-LLM systems without communication, judgments are independently generated by multiple models, which are subsequently synthesized into a final decision through various aggregation strategies. Techniques such as majority vote, weighted averages, and prioritizing the highest confidence predictions, each play a crucial role. These methods allow each model to assess without interference, and eventually extract and combine the most effective elements from each model's response.",
          "quote": []
        },
        {
          "text": "Simple voting methods, such as majority voting, by selecting the most frequent answers, offers a straightforward approach to synthesize evaluations. For example, Badshah et al. [8] introduced a reference-guided verdict method for evaluating free-form text using multiple LLMs as judges. Combining these LLMs through majority vote significantly improves the reliability and accuracy of evaluations, particularly for complex tasks. Furthermore, PoLL [225] demonstrates that using a diverse panel of smaller models as judges through max voting and average pooling is not only an effective method for evaluating LLM performance, but also reduces intra-model bias of a single large model. Language-Model-as-an-Examiner [10] is another benchmarking framework to evaluate the performance of foundation models on open-ended question answering through voting. In the peer-examination mechanism, the LM serves as a knowledgeable examiner that formulates questions and evaluates responses in a reference-free manner. What's more, multi-LLM evaluation could also be used in improving dataset quality. Choi et al. [38] provided an enhanced dataset, MULTI-NEWS+, which is the result of a cleansing strategy leveraging CoT and majority voting to identify and exclude irrelevant documents through LLM-based data annotation.",
          "quote": [
            {
              "text": "[8]",
              "target": "#b7",
              "type": "bibr",
              "context": "ah et al. ",
              "index": 177
            },
            {
              "text": "[225]",
              "target": "#b227",
              "type": "bibr",
              "context": "ore, PoLL ",
              "index": 448
            },
            {
              "text": "[10]",
              "target": "#b9",
              "type": "bibr",
              "context": "-Examiner ",
              "index": 714
            },
            {
              "text": "[38]",
              "target": "#b37",
              "type": "bibr",
              "context": "oi et al. ",
              "index": 1101
            }
          ]
        },
        {
          "text": "Weighted scoring aggregation involves assigning different importance to different model outputs, either by aggregating multiple overall scores for the same response or by combining assessments of different aspects of the response to form a comprehensive evaluation. On the one hand, through a peer-review mechanism, PiCO [165] allows LLMs to answer unlabeled questions and evaluate each other without human annotations. It formalizes the evaluation as a constrained optimization problem, maximizing the consistency between LLMs' capabilities and corresponding weights. Likewise, PRE [27, 39] can automatically evaluate LLMs through a peer-review process. It selects qualified LLMs as reviewers through a qualification exam and aggregates their ratings using weights which is proportional to their agreement of humans, demonstrating effectiveness and robustness in evaluating text summarization tasks. In the field of recommendation explanations, Zhang et al. [284] suggests that ensembles like averaging ratings of multiple LLMs can enhance evaluation accuracy and stability. On the other hand, for example, AIME [175] is an evaluation protocol that utilizes multiple LLMs that each with a specific role independently generate an evaluation on separate criteria and then combine them via concatenation. Similarly, a paper introduces HD-EVAL [147], which iteratively aligns LLM-based evaluators with human preference via Hierarchical Criteria Decomposition. By decomposing a given evaluation task into finer-grained criteria, aggregating them according to estimated human preferences, pruning insignificant criteria with attribution, and further decomposing significant criteria, HD-EVAL demonstrates its superiority.",
          "quote": [
            {
              "text": "[165]",
              "target": "#b165",
              "type": "bibr",
              "context": "ism, PiCO ",
              "index": 321
            },
            {
              "text": "[27,",
              "target": "#b26",
              "type": "bibr",
              "context": "wise, PRE ",
              "index": 583
            },
            {
              "text": "39]",
              "target": "#b38",
              "type": "bibr",
              "context": " PRE [27, ",
              "index": 588
            },
            {
              "text": "[284]",
              "target": "#b286",
              "type": "bibr",
              "context": "ng et al. ",
              "index": 959
            },
            {
              "text": "[175]",
              "target": "#b175",
              "type": "bibr",
              "context": "ple, AIME ",
              "index": 1113
            },
            {
              "text": "[147]",
              "target": "#b147",
              "type": "bibr",
              "context": "s HD-EVAL ",
              "index": 1341
            }
          ]
        },
        {
          "text": "Apart from weighting methods, there are some other advance mathematical aggregation techniques, such as Bayesian methods and graph-based approaches, offering more robust ways to handle uncertainties and inconsistencies across multiple evaluators. Notably, a paper introduces two calibration methods, Bayesian Win Rate Sampling (BWRS) and Bayesian Dawid-Skene [70], to address the win rate estimation bias when using many LLMs as evaluators for text generation quality. In addition to that, GED [90] addresses inconsistencies in LLM preference evaluations by leveraging multiple weak evaluators to construct preference graphs, and then utilize DAG structure to ensemble and denoise these graphs for better, non-contradictory evaluation results.",
          "quote": [
            {
              "text": "[70]",
              "target": "#b69",
              "type": "bibr",
              "context": "wid-Skene ",
              "index": 359
            },
            {
              "text": "[90]",
              "target": "#b89",
              "type": "bibr",
              "context": "that, GED ",
              "index": 494
            }
          ]
        },
        {
          "text": "LLM-based aggregation is a grand-new perspective like Fusion-Eval [198]. It's a novel framework that integrates various assistant evaluators using LLMs, each of which specializes in assessing distinct aspects of responses, to enhance the correlation of evaluation scores with human judgments for natural language systems.",
          "quote": [
            {
              "text": "[198]",
              "target": "#b200",
              "type": "bibr",
              "context": "sion-Eval ",
              "index": 66
            }
          ]
        },
        {
          "text": "In addition to the above direct use of multiple model evaluation, the cascade framework employs a tiered approach, where weaker models are used initially for evaluations, and stronger models are engaged only when higher confidence is required, optimizing resource use and enhancing evaluation precision. Jung et al. [103] proposes \"Cascaded Selective Evaluation\" to ensure high agreement with human judgments while using cheaper models. Similar to the work above, Huang et al. [92] proposes CascadedEval, a novel method integrating proprietary models, in order to compensate for the limitations of fine-tuned judge models.",
          "quote": [
            {
              "text": "[103]",
              "target": "#b102",
              "type": "bibr",
              "context": "ng et al. ",
              "index": 316
            },
            {
              "text": "[92]",
              "target": "#b91",
              "type": "bibr",
              "context": "ng et al. ",
              "index": 477
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "4.3",
        "name": "Human-AI Collaboration System"
      },
      "p": [
        {
          "text": "Human-AI Collaboration Systems bridge the gap between automated LLM judgments and the essential need for human oversight, particularly in high-stakes domains such as law, healthcare, and education. Human evaluators act either as the ultimate deciders, or as intermediaries who verify and refine model outputs. By incorporating human insights, Hybrid systems can ensure the final judgment is more reliable and aligned with ethical considerations, and empower continuous model improvement through feedback loops.",
          "quote": []
        },
        {
          "text": "In many Human-AI Collaboration systems, human evaluators play a vital role during the evaluation process itself, actively collaborating with the LLMs to review and refine the generated outputs. For example, COEVAL [131] introduces a collaborative evaluation pipeline where LLMs generate initial criteria and evaluations for open-ended natural language tasks. These machine-generated outputs are then reviewed and refined by human evaluators to guarantee reliability. To address a significant positional bias in LLMs when used as evaluators, Wang et al. [230] proposes a calibration framework with three strategies: Multiple Evidence Calibration, Balanced Position Calibration, and Human-in-the-Loop Calibration. Similarly, EvalGen [192] integrates human feedback iteratively to refine evaluation criteria, addressing challenges such as \"criteria drift\", where the standards of evaluation evolve as humans interact with the model. These systems allow human evaluators to provide real-time adjustments, enhancing the accuracy and trustworthiness of the evaluation process.",
          "quote": [
            {
              "text": "[131]",
              "target": "#b130",
              "type": "bibr",
              "context": "e, COEVAL ",
              "index": 214
            },
            {
              "text": "[230]",
              "target": "#b232",
              "type": "bibr",
              "context": "ng et al. ",
              "index": 553
            },
            {
              "text": "[192]",
              "target": "#b192",
              "type": "bibr",
              "context": ", EvalGen ",
              "index": 731
            }
          ]
        },
        {
          "text": "While in other systems, human involvement takes place after the LLM has completed its evaluations, providing a final layer of verification and adjustment. This method ensures that the LLM's judgments are thoroughly scrutinized and aligned with human values. EvaluLLM [170] allows humans to intervene and refine the evaluation results, thereby enhancing trust in the model's performance while also controlling for potential biases. Additionally, Chiang et al. [35] tried LLM TAs as an assignment evaluator in a large university course. After students submit assignments and receive LLM-generated feedback, the teaching team reviews and finalizes the evaluation results. This process illustrates how human oversight after the initial automated evaluation can guarantee fairness and consistentcy with academic standards.",
          "quote": [
            {
              "text": "[170]",
              "target": "#b170",
              "type": "bibr",
              "context": " EvaluLLM ",
              "index": 267
            },
            {
              "text": "[35]",
              "target": "#b34",
              "type": "bibr",
              "context": "ng et al. ",
              "index": 459
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "5",
        "name": "APPLICATION"
      },
      "p": [
        {
          "text": "Due to the convenience and effectiveness of LLM Judges, they have been widely applied as judges across various domains. These applications not only cover general domains but also specific domains such as multimodal, medical, legal, financial, education, information retrieval and others. In this section, we will provide a detailed introduction to these applications, demonstrating how LLMs achieve precise and efficient evaluations in different domains.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "5.1",
        "name": "General"
      },
      "p": [
        {
          "text": "In general domains, LLM Judges are applied to tasks requiring both understanding and generation, such as dialogue generation, open-ended question answering, summarization, and language translation. Each task follows its own set of evaluation criteria to meet its specific requirements. For instance, in dialogue generation [133], the criteria emphasize the natural flow, emotional resonance, and contextual relevance of the conversation. In summarization tasks [162], the evaluation focuses on the coherence, consistency, fluency, and relevance of the text. In translation tasks [64], the assessment prioritizes the quality, accuracy, fluency, and style.",
          "quote": [
            {
              "text": "[133]",
              "target": "#b132",
              "type": "bibr",
              "context": "eneration ",
              "index": 323
            },
            {
              "text": "[162]",
              "target": "#b162",
              "type": "bibr",
              "context": "ion tasks ",
              "index": 461
            },
            {
              "text": "[64]",
              "target": "#b63",
              "type": "bibr",
              "context": "ion tasks ",
              "index": 579
            }
          ]
        },
        {
          "text": "As these diverse sub-tasks require specialized evaluation criteria, LLM judges provides refined evaluation methods that go beyond traditional metrics, paving the way for more comprehensive and in-depth assessments. For instance, Shu et al. [198] introduced Fusion-Eval, an innovative approach that leverages LLMs to integrate insights from various assistant evaluators. Fusion-Eval evaluated summary quality across four dimensions-coherence, consistency, fluency, and relevance, achieving a system-level Kendall-Tau correlation of 0.962 with human judgments. For dialogue quality, it assessed six aspects: coherence, engagingness, naturalness, groundedness, understandability, and overall quality, attaining a turn-level Spearman correlation of 0.744. Furthermore, Xu et al. [256] proposed the ACTIVE-CRITIC framework, which enables LLMs to actively infer the target task and relevant evaluation criteria while dynamically optimizing prompts. In the story generation task, this framework achieved superior evaluation performance.",
          "quote": [
            {
              "text": "[198]",
              "target": "#b200",
              "type": "bibr",
              "context": "hu et al. ",
              "index": 240
            },
            {
              "text": "[256]",
              "target": "#b258",
              "type": "bibr",
              "context": "Xu et al. ",
              "index": 775
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "5.2",
        "name": "Multimodal"
      },
      "p": [
        {
          "text": "In the multimodal domain, the evaluation objects of LLMs are not limited to textual data but extend to various forms of information such as images, audio, and video. One of the primary challenges in evaluating multimodal tasks lies in the significant heterogeneity among these modalities, including substantial differences in data structures, representation methods, and feature distributions.",
          "quote": []
        },
        {
          "text": "To address this challenge, advanced techniques are often required to help LLMs integrate different forms of information, ensuring that they can provide accurate and meaningful evaluations. For example, Xiong et al. [253] trained an open-source multimodal LLM, LLaVA-Critic, specifically to evaluate model performance in multimodal scenarios. Similarly, Chen et al. [24] developed a Multimodal LLM-as-a-judge for 14 Vision-Language tasks, providing a unified evaluation framework. In addition, LLMs-as-judges can also be used in audio. For instance, Latif et al. [117] used LLMs for identifying and evaluating emotional cues in speech, achieving remarkable accuracy in the process. Beyond these efforts, some recent studies [46, 299] have also explored the potential of multimodal LLMs to self-evaluate and self-reward, enhancing their performance without the need for external evaluators or human annotations.",
          "quote": [
            {
              "text": "[253]",
              "target": "#b255",
              "type": "bibr",
              "context": "ng et al. ",
              "index": 215
            },
            {
              "text": "[24]",
              "target": "#b23",
              "type": "bibr",
              "context": "en et al. ",
              "index": 365
            },
            {
              "text": "[117]",
              "target": "#b116",
              "type": "bibr",
              "context": "if et al. ",
              "index": 562
            },
            {
              "text": "[46,",
              "target": "#b45",
              "type": "bibr",
              "context": "t studies ",
              "index": 723
            },
            {
              "text": "299]",
              "target": "#b301",
              "type": "bibr",
              "context": "dies [46, ",
              "index": 728
            }
          ]
        },
        {
          "text": "As the application of LLMs-as-judges continues to expand in multimodal domains, there is a growing interest in exploring their use in more specific real-world scenarios, such as autonomous driving. Chen et al. [29] proposed CODA-LM, a novel vision-language benchmark for self-driving, which provides automatic and systematic evaluation of Large Vision-Language Models (LVLMs) on road corner cases. Interestingly, they found that using the text-only LLM judges resulted in a closer alignment with human preferences than LVLMs.",
          "quote": [
            {
              "text": "[29]",
              "target": "#b28",
              "type": "bibr",
              "context": "en et al. ",
              "index": 210
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "5.3",
        "name": "Medical"
      },
      "p": [
        {
          "text": "In the medical field, LLMs-as-judges have demonstrated significant potential, particularly in areas such as diagnostic support, medical text analysis, clinical decision-making, and patient education. In this domain, high-quality evaluation requires LLM judges to possess precise interpretation capabilities for domain-specific terminology, the ability to comprehensively analyze diverse data types (such as clinical records and medical imaging), and strict compliance with high accuracy standards and ethical guidelines.",
          "quote": []
        },
        {
          "text": "In the realm of medical text generation, Xie et al. [251] used LLMs to evaluate the comp-duikeyi1leteness, conciseness, and attribution of medical texts at a fine-grained level. Similarly, Brake et al. [17] leveraged LLMs, such as Llama2, to assess clinical note consistency, with results indicating agreement levels comparable to human annotators. When it comes to medical question answering, Krolik et al. [114] explored the use of LLMs to automatically evaluate answer quality. Their focus was on evaluating adherence to medical knowledge and professional standards, completeness of information, accuracy of terminology, clarity of expression, and relevance to the question.",
          "quote": [
            {
              "text": "[251]",
              "target": "#b253",
              "type": "bibr",
              "context": "ie et al. ",
              "index": 52
            },
            {
              "text": "[17]",
              "target": "#b16",
              "type": "bibr",
              "context": "ke et al. ",
              "index": 202
            },
            {
              "text": "[114]",
              "target": "#b113",
              "type": "bibr",
              "context": "ik et al. ",
              "index": 408
            }
          ]
        },
        {
          "text": "In the area of mental health counseling, Li et al. [126] utilized LLMs to automate the evaluation of counseling effectiveness and quality. Key assessments included whether the counseling identified the client's emotional needs, provided appropriate responses, demonstrated empathy, managed negative emotions, and met the overall goals of mental health support. Beyond these above applications, LLMs' judging capabilities have also been applied to assist in improving performance in specialized medical reasoning tasks. For instance, many studies [95] employed LLMs to evaluate and filter medical information, thereby supporting enhanced medical reasoning.",
          "quote": [
            {
              "text": "[126]",
              "target": "#b125",
              "type": "bibr",
              "context": "Li et al. ",
              "index": 51
            },
            {
              "text": "[95]",
              "target": "#b94",
              "type": "bibr",
              "context": "y studies ",
              "index": 546
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "5.4",
        "name": "Legal"
      },
      "p": [
        {
          "text": "Due to the powerful evaluation capabilities of LLMs, LLMs-as-judges have been widely applied in the legal domain, covering multiple key scenarios, including evaluating the performance of law LLMs and relevance judgment in legal case retrieval. In the legal domain, the application of LLMs requires a deep understanding of the legal framework of specific jurisdictions, complex legal language, and rigorous logical reasoning abilities [128]. At the same time, interpretability and transparency of the evaluation results are essential core requirements, as legal practice highly depends on clear logic and verifiable conclusions. Furthermore, the bias and fairness of the model are of significant concern, as any bias in legal evaluations could have a profound impact on judicial fairness. These unique demands set higher standards for LLM judges.",
          "quote": [
            {
              "text": "[128]",
              "target": "#b127",
              "type": "bibr",
              "context": "abilities ",
              "index": 434
            }
          ]
        },
        {
          "text": "In response to these challenges, recent research has explored various ways in which LLMs can be effectively employed in legal evaluations. Some research used LLMs as judges to assist in evaluating the performance of Law LLMs. For example, Yue et al. [275] introduced DISC-LawLLM to provide a wide range of legal services and utilized GPT-3.5 as a referee to evaluate the model's performance. They assessed three key criteria-accuracy, completeness, and clarity-by assigning a rating score from 1 to 5. Similarly, Ryu et al. [187] applied retrieval-based evaluation to assess the performance of LLMs in Korean legal question-answering tasks, which applied RAG not for generation but for evaluation. What's more, LLMs have also been utilized to construct evaluation sets. Raju et al. [185] explored methods for constructing these domain-specific evaluation sets, which are essential for enabling LLMs-as-judges to perform effective evaluation in legal domain. Beyond performance evaluation, LLMs have also been utilized for relevance judgment in legal case retrieval. For instance, Ma et al. [154] used LLMs to automate the evaluation of large numbers of retrieved legal documents, improving both the scalability and accuracy of legal case retrieval systems. In conclusion, the application of LLMs-as-judges in law holds significant promise in future.",
          "quote": [
            {
              "text": "[275]",
              "target": "#b277",
              "type": "bibr",
              "context": "ue et al. ",
              "index": 250
            },
            {
              "text": "[187]",
              "target": "#b187",
              "type": "bibr",
              "context": "yu et al. ",
              "index": 524
            },
            {
              "text": "[185]",
              "target": "#b185",
              "type": "bibr",
              "context": "ju et al. ",
              "index": 782
            },
            {
              "text": "[154]",
              "target": "#b154",
              "type": "bibr",
              "context": "Ma et al. ",
              "index": 1090
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "5.5",
        "name": "Financial"
      },
      "p": [
        {
          "text": "In the financial domain, LLM judges have been extensively explored in scenarios such as investment risk assessment and credit scoring, which presenting unique challenges. For example, the complexity of risk assessment requires LLMs to accurately capture the influence of multifaceted factors, including market volatility, regulatory changes, and geopolitical events. Real-time processing demands further elevate the challenge, requiring LLMs not only to be computationally efficient but also to deliver rapid response times. Additionally, the dynamic nature of high-frequency trading demanding that LLMs swiftly adapt to fluctuating market conditions.",
          "quote": []
        },
        {
          "text": "In investment risk assessment, LLMs have proven effective due to their ability to process large amounts of data and make informed judgments. For instance, Xie et al. [248] developed a financial LLM, FinMA, fine-tuned on LLaMA to evaluate investment risks more effectively. Their model is designed to follow instructions for risk assessment and decision analysis, improving the accuracy and efficiency of financial evaluations.",
          "quote": [
            {
              "text": "[248]",
              "target": "#b250",
              "type": "bibr",
              "context": "ie et al. ",
              "index": 166
            }
          ]
        },
        {
          "text": "Another key application in the financial domain is credit scoring, which predicts the future repayment ability and default risk of individuals or businesses. By analyzing a vast array of data, including credit history, financial status, and other relevant factors, LLMs can help financial institutions make more accurate credit scoring assessments. For example, Babaei et al. [7] demonstrated how LLMs can process unstructured text data, such as customer histories, contract terms, and news reports, to enhance the precision of credit assessments.",
          "quote": [
            {
              "text": "[7]",
              "target": "#b6",
              "type": "bibr",
              "context": "ei et al. ",
              "index": 376
            }
          ]
        },
        {
          "text": "Furthermore, as the use of LLMs in finance continues to grow, there is a rising need to evaluate the performance of these financial LLMs. To address this, Son et al. [201] developed an automated financial evaluation benchmark that leverages LLMs to extract valuable insights from both unstructured and structured data. This framework helps optimize the construction, updating, and compliance checks of financial benchmarks, supporting more efficient and scalable evaluation processes. Based on this, they facilitated the continuous optimization of financial LLMs, driving further advancements in the financial domain.",
          "quote": [
            {
              "text": "[201]",
              "target": "#b203",
              "type": "bibr",
              "context": "on et al. ",
              "index": 166
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "5.6",
        "name": "Education"
      },
      "p": [
        {
          "text": "LLMs-as-judges have found extensive applications in the education domain, covering a wide range of tasks, such as grading student assignments, evaluating essays, assessing mathematical reasoning, and judging debate performance. These applications present several key challenges, including the diversity of student responses and individual differences, as well as the need for multidimensional evaluation. Effective evaluation in education requires LLMs to consider not only correctness but also creativity, clarity, and logical coherence. Additionally, the interpretability and fairness of the evaluation results are crucial, as educational assessments significantly impact students' development and future opportunities.",
          "quote": []
        },
        {
          "text": "In assignment grading, Chiang et al. [35] introduced the concept of an LLM Teaching Assistant (LLM TA) in university classrooms, utilizing GPT-4 to automate the grading of student assignments. By employing prompt engineering to define scoring criteria and task descriptions, LLM TA generates quantitative scores and detailed feedback. Their study emphasized the system's ability to maintain consistency, adhere to grading standards, and resist adversarial prompts, highlighting its robustness and practicality for classroom use.",
          "quote": [
            {
              "text": "[35]",
              "target": "#b34",
              "type": "bibr",
              "context": "ng et al. ",
              "index": 37
            }
          ]
        },
        {
          "text": "In addition to assignment grading, LLMs-as-judges are also being explored for automated essay scoring. Wang et al. [228] proposed an advanced intelligent essay scoring system, integrating LLMs such as BERT and ChatGPT to enable automated scoring and feedback generation for essays across various genres. Similarly, Song et al. [205] investigated a framework and methodology for automated essay scoring and revision based on open-source LLMs. Furthermore, Zhou et al. [297] explored the potential of LLMs in academic paper reviewing tasks, assessing their reliability, effectiveness, and possible biases as reviewer. They found that while LLMs show certain promise in the domain of automated reviewing, they are not yet sufficient to fully replace human reviewers, particularly in areas with high technical complexity or strong innovation.",
          "quote": [
            {
              "text": "[228]",
              "target": "#b230",
              "type": "bibr",
              "context": "ng et al. ",
              "index": 115
            },
            {
              "text": "[205]",
              "target": "#b207",
              "type": "bibr",
              "context": "ng et al. ",
              "index": 327
            },
            {
              "text": "[297]",
              "target": "#b299",
              "type": "bibr",
              "context": "ou et al. ",
              "index": 467
            }
          ]
        },
        {
          "text": "Another area where LLMs-as-judges are making an impact is in the evaluation of math reasoning. Unlike traditional mathematical task evaluation, which focuses solely on the correctness of the final results, Xia et al. [246] argued that additional aspects of the reasoning process should also be assessed, such as logical errors or unnecessary steps. In their work, the authors proposed ReasonEval, a new methodology for evaluating the quality of reasoning steps based on LLMs-as-judges.",
          "quote": [
            {
              "text": "[246]",
              "target": "#b248",
              "type": "bibr",
              "context": "ia et al. ",
              "index": 217
            }
          ]
        },
        {
          "text": "LLMs have also been employed in judging debate performance. Liang et al. [135] proposed Debatrix, a new method which leverages LLMs to evaluate and analyze debates. The main aspects assessed include the logical consistency of arguments, the effectiveness of rebuttals, the appropriateness of emotional expression, and the coherence of the debate.",
          "quote": [
            {
              "text": "[135]",
              "target": "#b134",
              "type": "bibr",
              "context": "ng et al. ",
              "index": 73
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "5.7",
        "name": "Information Retrieval"
      },
      "p": [
        {
          "text": "Information retrieval refers to the process of effectively retrieving, filtering, and ranking relevant information from a large collection of data, matching information resources to users' needs (queries). However, evaluating these systems presents several challenges, particularly due to the the complexity of real-world data, the diversity of user needs, and personalization. To solve these challenges, LLMs-as-judges have been used across various applications, including relevance judgment, text ranking, recommendation explanations evaluation, and assessing retrieval-augmented generation (RAG) systems.",
          "quote": []
        },
        {
          "text": "One key area in information retrieval is the evaluation of the relevance of retrieved results to user queries, a task that traditionally relies on manual annotations [183, 200]. Rahmani et al. [183] proposed a framework called LLMJudge which leveraged LLMs to assess the relevance of information retrieval system results to user queries, providing a more scalable and efficient evaluation approach.",
          "quote": [
            {
              "text": "[183,",
              "target": "#b183",
              "type": "bibr",
              "context": "notations ",
              "index": 166
            },
            {
              "text": "200]",
              "target": "#b202",
              "type": "bibr",
              "context": "ons [183, ",
              "index": 172
            },
            {
              "text": "[183]",
              "target": "#b183",
              "type": "bibr",
              "context": "ni et al. ",
              "index": 193
            }
          ]
        },
        {
          "text": "Another important aspect of information retrieval is the ranking of search results or recommendation lists. Traditional ranking models often rely on shallow features or direct matching scores, which may not yield optimal results. To address this, Qin et al. [180] examined the performance of LLMs in text ranking tasks and proposed a novel method based on pairwise ranking prompting, utilizing LLMs for text ranking. Additionally, Niu et al. [166] introduced a framework called JudgeRank, which leveraged LLMs to rerank results in reasoning-intensive tasks. By evaluating the logic, relevance, and quality of candidate results, this approach tried to enhance ranking performance.",
          "quote": [
            {
              "text": "[180]",
              "target": "#b180",
              "type": "bibr",
              "context": "in et al. ",
              "index": 258
            },
            {
              "text": "[166]",
              "target": "#b166",
              "type": "bibr",
              "context": "iu et al. ",
              "index": 442
            }
          ]
        },
        {
          "text": "In recommendation systems, explanation evaluation plays a crucial role in helping users understand why a specific product, movie, or piece of content is recommended. Zhang et al. [284] investigated the potential of LLMs as automated evaluators of recommendation explanations, assessing them across multiple dimensions such as quality, clarity, and relevance. This approach provides a more efficient way to evaluate the effectiveness of explanations, which is essential for improving user trust and satisfaction.",
          "quote": [
            {
              "text": "[284]",
              "target": "#b286",
              "type": "bibr",
              "context": "ng et al. ",
              "index": 179
            }
          ]
        },
        {
          "text": "Furthermore, with the growing use of retrieval-augmented generation (RAG) systems in tasks like question answering, fact-checking, and customer support, there is an increasing need to evaluate the quality of these systems. Traditional evaluation methods rely on large manually annotated datasets, which are time-consuming and costly. To address this, Saad et al. [188] proposed a new automated evaluation framework called ARES, which leveraged LLMs as the core evaluation tool to directly assess retrieval and generated content across multiple dimensions, including relevance, accuracy, coverage, fluency, and coherence.",
          "quote": [
            {
              "text": "[188]",
              "target": "#b188",
              "type": "bibr",
              "context": "ad et al. ",
              "index": 363
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "5.8",
        "name": "Others"
      },
      "p": []
    },
    {
      "section": {
        "index": "5.8.1",
        "name": "Soft"
      },
      "p": [
        {
          "text": "Engineering. The challenges that LLMs-as-judges need to overcome in the software engineering domain include complex code structures and the diversity of evaluation criteria. A numerous of articles [175, 243] used LLMs-as-judges to assess the quality of code generation. Moreover, Kumar et al. [115] employed LLMs to evaluate the quality of Bug Report Summarization.",
          "quote": [
            {
              "text": "[175,",
              "target": "#b175",
              "type": "bibr",
              "context": " articles ",
              "index": 197
            },
            {
              "text": "243]",
              "target": "#b245",
              "type": "bibr",
              "context": "les [175, ",
              "index": 203
            },
            {
              "text": "[115]",
              "target": "#b114",
              "type": "bibr",
              "context": "ar et al. ",
              "index": 293
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "5.8.2",
        "name": "Biology."
      },
      "p": [
        {
          "text": "The main evaluation challenges in biological field include the complexity and diversity of the data and the need for specialized biological knowledge [86, 167]. For example, Hijazi et al. [86] used LLMs to evaluate Query-focused summarization (QFS), which refers to generating concise and accurate summaries from a large set of biomedical literature based on a specified query. In this context, the LLMs are used to assess whether these summaries accurately answer the specified query and whether they cover the correct biological knowledge. 5.8.3 Social Science. LLMs-as-Judges have also found applications in social sciences. On one hand, they are used in real-world human social contexts. For example, Tessler et al. [214] used LLMs to participate in democratic discussions, assessing the quality of arguments, identifying fallacies, or providing a balanced view of an issue, thus helping people reach consensus on complex social and political matters. On the other hand, LLMs-as-judges are also used in social scenarios constructed by language agents. Zhou et al. [298] proposed an interactive evaluation framework called Sotopia, which used LLMs to assess the social intelligence of language agents from multiple dimensions, such as emotional understanding, response adaptability, and other social skills.",
          "quote": [
            {
              "text": "[86,",
              "target": "#b85",
              "type": "bibr",
              "context": "knowledge ",
              "index": 150
            },
            {
              "text": "167]",
              "target": "#b167",
              "type": "bibr",
              "context": "edge [86, ",
              "index": 155
            },
            {
              "text": "[86]",
              "target": "#b85",
              "type": "bibr",
              "context": "zi et al. ",
              "index": 188
            },
            {
              "text": "[214]",
              "target": "#b216",
              "type": "bibr",
              "context": "er et al. ",
              "index": 720
            },
            {
              "text": "[298]",
              "target": "#b300",
              "type": "bibr",
              "context": "ou et al. ",
              "index": 1068
            }
          ]
        },
        {
          "text": "In this section, we have outlined the specific applications of LLMs-as-judges across various domains. In these applications, LLMs leverage their powerful text understanding and generation capabilities to perform effective evaluations and judgments, providing accurate feedback and improvement suggestions. Although LLMs-as-judges have shown tremendous potential in these areas, especially in handling large-scale data and automating assessments, they still face challenges such as the depth of domain-specific knowledge, limitations in reasoning abilities, and the diversity of evaluation criteria. In the future, with continuous improvements in model performance and domain adaptation capabilities\" we believe the application of LLMs-as-judges will become more widespread and precise across various domains.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "6",
        "name": "META-EVALUATION"
      },
      "p": [
        {
          "text": "Meta-evaluation, the process of assessing the quality of the evaluator itself, is a crucial step in determining the reliability, consistency, and validity of LLM judges. Given the diverse applications of LLMs as evaluators, meta-evaluation methods have also been evolving. Researchers have proposed various datasets and metrics tailored to different tasks and evaluation objectives to assess the reliability and validity of LLM-based evaluations. This chapter will explore state-of-the-art Benchmarks ( §6.1) and evaluation Metrics ( §6.2), categorize existing approaches, and discuss their advantages and limitations.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "6.1",
        "name": "Benchmarks"
      },
      "p": [
        {
          "text": "To evaluate LLM-based judges, a common approach is to measure their alignment with human preferences, as human judgments are often considered the gold standard for quality and reliability. Given the diverse range of applications for LLM-based judges, different benchmarks have been created, each tailored to specific evaluation criteria and use cases. In this section, we present a comprehensive collection of 40 widely-used benchmarks, each designed to capture different aspects of evaluation, such as language understanding, factual accuracy, coherence, creativity, and fairness. To enhance clarity and facilitate comparison, we categorize these benchmarks by application domain.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "6.1.1",
        "name": "Code Generation."
      },
      "p": [
        {
          "text": "Code generation aims to produce executable program code from natural language input. This task typically involves translating user requirements or descriptions into precise code. The applications of code generation are vast, including automated script creation, bug fixing, and the generation of complex programming tasks.Evaluating code generation is highly challenging, and LLMs are increasingly being used as evaluators for assessing code quality. HumanEval [30] is a widely used benchmark dataset designed to evaluate programming capabilities. It consists of 164 coding tasks, each accompanied by a brief natural language description. The tasks primarily involve algorithmic problems and data structure exercises, with difficulty levels ranging from basic to intermediate. One notable feature of HumanEval [30] is the inclusion  [101] assesses the model's ability to handle comprehensive problem-solving and logical reasoning. However, the increased complexity also introduces challenges in establishing consistent evaluation criteria, particularly when it comes to subjective aspects like code style and efficiency. Moreover, DevAI [303] was introduced to address the limitations of existing benchmarks, which often fail to capture the iterative nature of software development and lack adequate signals for measuring long-term progress. The dataset includes 365 task requirements, focusing on more complex and challenging programming scenarios. CrossCodeEval [50] focuses on assessing crosslanguage programming models, containing over 1,000 tasks that involve translating code between different programming language pairs, such as Python to Java or JavaScript to C++. This dataset tests the model's ability to adapt and transform code across languages, highlighting the challenges of understanding varied syntax and semantics. CodeUltraFeedback [243] is designed to evaluate and enhance the alignment between LLMs and user-defined programming preferences. It includes 10,000 programming instructions, each paired with four responses from 14 different LLMs. These responses are scored by GPT-3.5 based on five distinct programming preferences, such as readability, efficiency, and adherence to user specifications. The dataset emphasizes fine-grained feedback and user-centered evaluation, making it a useful tool for analyzing preference alignment.",
          "quote": [
            {
              "text": "[30]",
              "target": "#b29",
              "type": "bibr",
              "context": "HumanEval ",
              "index": 461
            },
            {
              "text": "[30]",
              "target": "#b29",
              "type": "bibr",
              "context": "HumanEval ",
              "index": 810
            },
            {
              "text": "[101]",
              "target": "#b100",
              "type": "bibr",
              "context": "nclusion  ",
              "index": 833
            },
            {
              "text": "[303]",
              "target": "#b305",
              "type": "bibr",
              "context": "er, DevAI ",
              "index": 1137
            },
            {
              "text": "[50]",
              "target": "#b49",
              "type": "bibr",
              "context": "sCodeEval ",
              "index": 1464
            },
            {
              "text": "[243]",
              "target": "#b245",
              "type": "bibr",
              "context": "aFeedback ",
              "index": 1850
            }
          ]
        },
        {
          "text": "6.1.2 Machine Translation. Machine Translation (MT) refers to the process of automatically translating text from a source language to a target language. Over time, MT technology has progressed significantly, evolving from rule-based methods to Statistical Machine Translation (SMT), and more recently to Neural Machine Translation (NMT), which is the dominant approach. With the widespread adoption of NMT and the emergence of LLMs, evaluating translation quality has become a complex task, requiring robust evaluation frameworks that can assess accuracy, fluency, and contextual relevance across diverse language pairs. The Workshop on Machine Translation (WMT) [66] is a prominent annual evaluation event in the field of MT. It provides large-scale, human-annotated datasets for a variety of language pairs, including English-French, English-German, and English-Russian. Each year, WMT releases benchmark datasets that include source texts, model-generated translations, reference translations, and human evaluation scores. These datasets are widely used for assessing the performance of automated evaluation metrics by comparing their outputs against human judgments. WMT covers a broad range of tasks, from sentence-level translation to document-level and domainspecific challenges, making it a comprehensive resource for evaluating the correlation between automated evaluators and human assessments. However, WMT primarily focuses on high-resource languages, which may limit its applicability to low-resource or underrepresented languages. Literary Translation Comparisons [104] is designed to assess document-level translation quality, particularly in the context of literary works. It includes carefully selected paragraphs from various literary pieces, covering 18 language pairs such as Japanese-English, Polish-English, and French-English. Unlike sentence-level benchmarks, this dataset emphasizes the importance of evaluating translations in a broader context, as literary texts often require understanding of stylistic elements and cultural subtleties. This makes it particularly useful for evaluating the performance of LLMs, which may excel in capturing broader contextual information. The MQM [65] study is the largest evaluation effort to date focusing on machine translation quality. It involves professional translators annotating the outputs of top-performing systems from the WMT 2020 shared task, specifically targeting English-German and Chinese-English translations. MQM introduces a multidimensional quality assessment framework that goes beyond traditional metrics like BLEU or ROUGE. It evaluates translations across multiple dimensions, including accuracy, fluency, terminology, style, and locale, providing a more nuanced understanding of translation quality.",
          "quote": [
            {
              "text": "[66]",
              "target": "#b65",
              "type": "bibr",
              "context": "ion (WMT) ",
              "index": 663
            },
            {
              "text": "[104]",
              "target": "#b103",
              "type": "bibr",
              "context": "mparisons ",
              "index": 1578
            },
            {
              "text": "[65]",
              "target": "#b64",
              "type": "bibr",
              "context": ". The MQM ",
              "index": 2208
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "6.1.3",
        "name": "Text Summarization."
      },
      "p": [
        {
          "text": "Text Summarization (TS) is the task of generating a concise and coherent summary from a given piece of text while preserving its essential meaning. The main goal is to provide a quick, accurate overview of the source content, capturing key information and eliminating unnecessary details. As LLMs have shown impressive capabilities in generating summaries, the need for robust meta-evaluation benchmarks is critical to effectively assess their performance across various dimensions like coherence, relevance, consistency, and fluency.",
          "quote": []
        },
        {
          "text": "SummEval [59] is one of the most widely used benchmarks for evaluating summarization models. It includes summaries generated by 16 different models based on 100 news articles randomly sampled from the CNN/DailyMail test set. Each summary was annotated by five independent crowd-sourced workers and three expert evaluators, using a Likert scale from 1 to 5 across four key dimensions: coherence, consistency, fluency, and relevance. The dataset is valuable for analyzing the correlation between human judgments and automated evaluation metrics. The FRANK [169] dataset is dedicated to assessing the factual accuracy of summaries generated by automatic summarization systems. It provides detailed human annotations of factual errors, including semantic frame errors, discourse errors, and content verifiability issues. The dataset includes summaries from both the CNN/DailyMail and XSum datasets, making it a comprehensive resource for evaluating factual correctness. FRANK's detailed categorization of errors offers valuable insights into the types of factual inaccuracies common in generated summaries, highlighting areas where LLMs often struggle. However, focusing solely on factual errors may overlook other aspects of summary quality, such as coherence and fluency. OpinsummEval [194] is a meta-evaluation benchmark specifically designed for opinion summarization tasks, where the goal is to extract and summarize opinions from a large volume of user reviews. This dataset includes outputs from 14 different opinion summarization models and provides human annotations across four dimensions: aspect relevance, self-consistency, sentiment consistency, and readability.",
          "quote": [
            {
              "text": "[59]",
              "target": "#b58",
              "type": "bibr",
              "context": " ",
              "index": 9
            },
            {
              "text": "[169]",
              "target": "#b169",
              "type": "bibr",
              "context": "The FRANK ",
              "index": 554
            },
            {
              "text": "[194]",
              "target": "#b196",
              "type": "bibr",
              "context": "nsummEval ",
              "index": 1283
            }
          ]
        },
        {
          "text": "6.1.4 Dialogue Generation. Dialogue Generation is the task of automatically generating natural language conversations that are relevant to a given context. The primary goal is to develop dialogue systems that can understand context, generate fluent responses, and maintain logical consistency and contextual accuracy. Dialogue generation encompasses a wide range of applications, from chatbots and virtual assistants to social conversational agents. With the increasing capabilities of large language models (LLMs), evaluating dialogue generation has become more complex, requiring multi-faceted evaluation frameworks to assess various aspects of conversational quality.",
          "quote": []
        },
        {
          "text": "In the field of dialogue generation, the most commonly used datasets include Topical-Chat [72] and PERSONA-CHAT [283]. The Topical-Chat [72] dataset aims to advance research in open-domain conversational AI, covering eight major topics such as entertainment, health, and technology. The PERSONA-CHAT [283] dataset, on the other hand, focuses on enhancing dialogue systems by incorporating predefined personas to generate more personalized responses. Each dialogue participant is assigned a persona profile, consisting of several descriptive sentences about their personality or preferences. Mehri and Eskenazi [156] conducted a meta-evaluation study on these two widely-used open-domain dialogue corpora. They manually annotated 60 dialogue contexts from each dataset, with six responses per context for Topical-Chat and five for PERSONA-CHAT [283], including both model-generated and human responses. Each response was evaluated across six key dimensions: naturalness, coherence, engagement, groundedness, understandability, and overall quality. This study highlights the importance of multi-dimensional evaluation in dialogue generation, providing valuable insights into the strengths and weaknesses of different dialogue models. Additionally, the dataset from DSTC10 Track 5 [272, 279] focuses on evaluating open-domain dialogue systems and is designed for automatic evaluation and moderation of dialogue systems. The challenge aims to develop automatic evaluation mechanisms that accurately reflect human judgments while effectively handling harmful user inputs, maintaining conversational flow and engagement. The dataset includes annotations across four aspects: coherence, appropriateness, naturalness, and toxicity control.",
          "quote": [
            {
              "text": "[72]",
              "target": "#b71",
              "type": "bibr",
              "context": "ical-Chat ",
              "index": 90
            },
            {
              "text": "[283]",
              "target": "#b285",
              "type": "bibr",
              "context": "SONA-CHAT ",
              "index": 112
            },
            {
              "text": "[72]",
              "target": "#b71",
              "type": "bibr",
              "context": "ical-Chat ",
              "index": 136
            },
            {
              "text": "[283]",
              "target": "#b285",
              "type": "bibr",
              "context": "SONA-CHAT ",
              "index": 300
            },
            {
              "text": "[156]",
              "target": "#b156",
              "type": "bibr",
              "context": " Eskenazi ",
              "index": 610
            },
            {
              "text": "[283]",
              "target": "#b285",
              "type": "bibr",
              "context": "SONA-CHAT ",
              "index": 843
            },
            {
              "text": "[272,",
              "target": "#b274",
              "type": "bibr",
              "context": "0 Track 5 ",
              "index": 1278
            },
            {
              "text": "279]",
              "target": "#b281",
              "type": "bibr",
              "context": "k 5 [272, ",
              "index": 1284
            }
          ]
        },
        {
          "text": "6.1.5 Automatic Story Generation. Automatic Story Generation (ASG) is a challenging task that aims to enable models to create coherent, engaging narratives based on a given prompt or context. It emulates human storytelling abilities by generating stories that exhibit a logical structure, compelling characters, and interesting plot developments. Evaluating story generation systems is inherently complex, as it involves assessing not only linguistic quality but also narrative elements like coherence, engagement, and surprise.",
          "quote": []
        },
        {
          "text": "The HANNA [34] dataset is tailored for evaluating automatic story generation (ASG), featuring 1,056 stories generated by 10 different systems from 96 prompts. Each story is annotated by three human reviewers across six criteria: relevance, coherence, resonance, surprise, engagement, and complexity. This comprehensive annotation framework provides a detailed assessment of narrative quality, making HANNA a valuable benchmark for comparing ASG models. Another notable dataset is the MANS [73], which forms part of the OpenMEVA [73] framework. It compiles stories from various natural language generation models using well-known corpora like ROCStories [159] and WritingPrompts [60]. MANS [73] focuses on manual annotations of narrative elements, serving as a robust testbed for exploring diverse evaluation metrics. The StoryER [26] dataset offers a distinct approach to evaluating story generation by focusing on preference prediction and aspect-based rating. StoryER is divided into two primary components: the first is a 100k Story Ranking Data, which pairs stories from the WritingPrompts dataset. Each pair includes one story with high user engagement (upvotes ≥ 50) and another with low engagement (upvotes ≤ 0). This component leverages real-world user feedback to capture implicit preferences, providing a practical basis for training models to predict story quality. The second component, Aspect Rating and Reasoning Data, contains 46,000 entries where annotators provide detailed ratings (on a scale of 1-5) for various story aspects such as introduction, character development, and plot description, along with explanatory comments. This combination of quantitative rankings and qualitative reasoning enables a nuanced evaluation of stories, making StoryER particularly useful for both automated scoring and interpretability research. The PERSER [229] dataset takes a different approach by addressing the subjectivity inherent in open-domain text generation evaluations. PERSER restructures existing datasets and introduces personalized tags, resulting in two sub-datasets: Per-MPST and Per-DOC. Per-MPST is an adapted version of the Movie Plot Synopsis Dataset, while Per-DOC includes 7,000 instances of paired stories generated from the same premise. These stories are evaluated based on dimensions such as interestingness, adaptability, surprise, character development, and the quality of the ending.",
          "quote": [
            {
              "text": "[34]",
              "target": "#b33",
              "type": "bibr",
              "context": "The HANNA ",
              "index": 10
            },
            {
              "text": "[73]",
              "target": "#b72",
              "type": "bibr",
              "context": " the MANS ",
              "index": 489
            },
            {
              "text": "[73]",
              "target": "#b72",
              "type": "bibr",
              "context": " OpenMEVA ",
              "index": 528
            },
            {
              "text": "[159]",
              "target": "#b159",
              "type": "bibr",
              "context": "OCStories ",
              "index": 653
            },
            {
              "text": "[60]",
              "target": "#b59",
              "type": "bibr",
              "context": "ngPrompts ",
              "index": 678
            },
            {
              "text": "[73]",
              "target": "#b72",
              "type": "bibr",
              "context": "60]. MANS ",
              "index": 689
            },
            {
              "text": "[26]",
              "target": "#b25",
              "type": "bibr",
              "context": "e StoryER ",
              "index": 829
            },
            {
              "text": "[229]",
              "target": "#b231",
              "type": "bibr",
              "context": "he PERSER ",
              "index": 1858
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "6.1.6",
        "name": "Values Alignment."
      },
      "p": [
        {
          "text": "Values alignment is a critical task in the development of AI systems, focused on ensuring that their behavior and decisions consistently reflect core human values and ethical standards. In the context of LLM-as-Judge, the alignment process is vital to verify that the model's outputs adhere to societal norms and ethical principles, minimizing risks related to harmful, biased, or unethical behavior. To support research and model development in values alignment, several datasets have been created, each with unique characteristics designed to evaluate or enhance the ethical behavior of LLMs.",
          "quote": []
        },
        {
          "text": "One notable dataset is PKU-SafeRLHF [96], which was specifically curated for studying safe alignment in large language models. The dataset comprises 83.4K preference entries, focusing on two primary dimensions: harmlessness and usefulness. In each sample, the dataset presents a pair of model responses to a given prompt, annotated with safety meta-labels and preferences based on the levels of safety and utility. Another influential dataset is the HHH [6] (Honesty, Helpfulness, and Harmlessness) dataset, designed to evaluate LLM performance across various human-model interaction scenarios. The dataset emphasizes three core human-centered values: honesty, helpfulness, and harmlessness. It includes a diverse collection of conversational examples where models are tested on their adherence to these values. By exposing models to a wide range of contexts, the HHH dataset serves as a comprehensive benchmark for assessing whether LLMs align with essential ethical standards and effectively mitigate risks of misinformation, harmful advice, or biased outputs. Moreover, the CVALUES [255] benchmark is a more recent contribution aimed at evaluating human values alignment specifically for Chinese LLMs. It represents the first comprehensive framework tailored to assess values alignment in the Chinese language context, focusing on two critical criteria: Safety and Responsibility.",
          "quote": [
            {
              "text": "[96]",
              "target": "#b95",
              "type": "bibr",
              "context": "-SafeRLHF ",
              "index": 36
            },
            {
              "text": "[6]",
              "target": "#b5",
              "type": "bibr",
              "context": "s the HHH ",
              "index": 454
            },
            {
              "text": "[255]",
              "target": "#b257",
              "type": "bibr",
              "context": "e CVALUES ",
              "index": 1085
            }
          ]
        },
        {
          "text": "6.1.7 Recommendation. Recommendation systems aim to provide personalized suggestions based on users' preferences and historical behavior. As the use of large language models (LLMs) expands, their role in evaluating the performance of recommendation systems has garnered increasing attention. LLMs can serve as versatile evaluators, offering insights into multiple aspects of recommendation systems beyond traditional metrics like accuracy. They can assess factors such as user engagement, satisfaction, and the quality of generated explanations.",
          "quote": []
        },
        {
          "text": "The MovieLens [80] dataset is a widely-used public dataset for movie recommendations, available in multiple versions with varying scales, ranging from thousands of users and ratings to millions. Zhang et al. [284] further annotated the MovieLens [80] data to create a sub-dataset featuring user self-explanation texts. In this sub-dataset, users write explanatory texts after being presented with a recommended movie. These explanations are then rated on a five-point Likert scale across four dimensions: Persuasiveness, Transparency, Accuracy, and Satisfaction. This annotated data provides valuable reference texts for LLMs in the context of explainability evaluation.",
          "quote": [
            {
              "text": "[80]",
              "target": "#b79",
              "type": "bibr",
              "context": "MovieLens ",
              "index": 14
            },
            {
              "text": "[284]",
              "target": "#b286",
              "type": "bibr",
              "context": "ng et al. ",
              "index": 208
            },
            {
              "text": "[80]",
              "target": "#b79",
              "type": "bibr",
              "context": "MovieLens ",
              "index": 246
            }
          ]
        },
        {
          "text": "Another commonly used dataset is the Yelp dataset [4], which contains detailed review data from 11 metropolitan areas, covering approximately 150,000 businesses, nearly 7 million user reviews, and over 200,000 images. User reviews include ratings for businesses, such as hotel ratings (1-5 stars), as well as additional feedback like \"cool\" and \"funny\" votes. Furthermore, the Yelp dataset provides extensive business attribute information (e.g., operating hours, parking availability, and delivery options), offering rich contextual information that can be leveraged for developing and evaluating recommendation systems.",
          "quote": [
            {
              "text": "[4]",
              "target": "#b3",
              "type": "bibr",
              "context": "p dataset ",
              "index": 50
            }
          ]
        },
        {
          "text": "6.1.8 Search. The search task is a fundamental component of information retrieval (IR), focusing on identifying the most relevant documents from extensive text collections based on user queries. Traditionally, relevance assessments in search tasks have been conducted by human annotators following established guidelines. However, recent advances in large language models (LLMs) have opened up new opportunities for utilizing these models as evaluators, offering an automated and scalable approach to relevance assessment. With the advent of retrieval-augmented generation (RAG) models, the role of LLMs as evaluators has expanded. There is now a growing need to assess various dimensions of retrieved contexts, including context relevance, answer faithfulness, and answer relevance. This shift highlights the potential of LLMs to provide nuanced judgments that go beyond simple topical relevance.",
          "quote": []
        },
        {
          "text": "A key resource for evaluating the performance of LLMs as relevance assessors is the series of datasets from the Text Retrieval Conference (TREC). TREC workshops aim to advance research in IR by offering large-scale test collections, standardized evaluation procedures, and a platform for benchmarking retrieval models. The datasets from the TREC Deep Learning Track [118], specifically from 2021 (DL21) [42] and 2022 (DL22) [43], are commonly used for this purpose. These datasets are derived from the expanded MS MARCO v2 collection [11], which contains approximately 138 million passages. Relevance judgments are provided by assessors from the National Institute of Standards and Technology (NIST) using a 4-point scale (0 to 3). This structured and fine-grained annotation scheme allows for a detailed comparison between LLM-generated relevance scores and human judgments. While general-purpose datasets offer valuable benchmarks, specialized retrieval tasks often require domain-specific datasets that reflect unique relevance criteria. One notable example is LeCaRDv2 [129], a large-scale dataset tailored for legal case retrieval. LeCaRDv2 enriches the concept of relevance by incorporating three distinct aspects: characterization, penalty, and procedure. These additional criteria provide a more comprehensive perspective on relevance.",
          "quote": [
            {
              "text": "[118]",
              "target": "#b117",
              "type": "bibr",
              "context": "ing Track ",
              "index": 366
            },
            {
              "text": "[42]",
              "target": "#b41",
              "type": "bibr",
              "context": "21 (DL21) ",
              "index": 403
            },
            {
              "text": "[43]",
              "target": "#b42",
              "type": "bibr",
              "context": "22 (DL22) ",
              "index": 424
            },
            {
              "text": "[11]",
              "target": "#b10",
              "type": "bibr",
              "context": "ollection ",
              "index": 534
            },
            {
              "text": "[129]",
              "target": "#b128",
              "type": "bibr",
              "context": " LeCaRDv2 ",
              "index": 1073
            }
          ]
        },
        {
          "text": "6.1.9 Comprehensive Data. To thoroughly assess the role of LLMs-as-Judges and better align them with human preferences, a diverse set of comprehensive datasets has been developed. These datasets provide large-scale, well-annotated data, allowing for the effective training and evaluation of LLMs in complex, real-world contexts. As a result, they contribute to improving the models' reliability and effectiveness in their role as evaluators.",
          "quote": []
        },
        {
          "text": "Datasets such as HelpSteer [238] and HelpSteer2 [237] are designed to improve the alignment and usefulness of LLMs. They provide multi-attribute data, enabling the training of models that can generate responses that are factually correct, coherent, and tailored to diverse user preferences. These open-source datasets support adjustments in response complexity and verbosity, catering to varying user needs. Additionally, UltraFeedback [44] offers a large-scale dataset with around 64,000 prompts from sources like UltraChat [49], ShareGPT [37], and TruthfulQA [140]. It includes multiple responses per prompt generated by different LLMs, with high-quality preference labels and textual feedback covering aspects like instruction-following, truthfulness, and helpfulness. UltraFeedback's fine-grained annotations and diverse prompts provide a robust resource for training reward and critic models, enhancing the evaluative capabilities of LLMs.",
          "quote": [
            {
              "text": "[238]",
              "target": "#b240",
              "type": "bibr",
              "context": "HelpSteer ",
              "index": 27
            },
            {
              "text": "[237]",
              "target": "#b239",
              "type": "bibr",
              "context": "elpSteer2 ",
              "index": 48
            },
            {
              "text": "[44]",
              "target": "#b43",
              "type": "bibr",
              "context": "aFeedback ",
              "index": 436
            },
            {
              "text": "[49]",
              "target": "#b48",
              "type": "bibr",
              "context": "UltraChat ",
              "index": 525
            },
            {
              "text": "[37]",
              "target": "#b36",
              "type": "bibr",
              "context": " ShareGPT ",
              "index": 540
            },
            {
              "text": "[140]",
              "target": "#b139",
              "type": "bibr",
              "context": "ruthfulQA ",
              "index": 561
            }
          ]
        },
        {
          "text": "In exploring instruction following and dialogue capabilities, specialized tools like AlpacaEval [56], alongside interactive platforms such as Chatbot Arena [292] and benchmarks like MT-Bench [292], provide critical insights. AlpacaEval is an automated evaluation tool using GPT-4 or Claude as evaluators. It assesses chat-based LLMs against the AlpacaFarm dataset, providing win-rate calculations across a variety of tasks, enabling rapid and cost-effective comparisons with baseline models like GPT-3.5 (Davinci-003). Chatbot Arena, on the other hand, offers a user-driven evaluation framework where participants interact with anonymous models and vote based on their preferences. The platform has collected over 1,000,000 user votes, using the Bradley-Terry model to rank LLMs and chatbots, providing valuable insights into user preferences and model performance in open-domain dialogue.",
          "quote": [
            {
              "text": "[56]",
              "target": "#b55",
              "type": "bibr",
              "context": "lpacaEval ",
              "index": 96
            },
            {
              "text": "[292]",
              "target": "#b294",
              "type": "bibr",
              "context": "bot Arena ",
              "index": 156
            },
            {
              "text": "[292]",
              "target": "#b294",
              "type": "bibr",
              "context": " MT-Bench ",
              "index": 191
            }
          ]
        },
        {
          "text": "Benchmarks like WildBench [138] and FLASK [269] aim to evaluate LLMs on tasks more reflective of real-world applications. WildBench [138] collects challenging examples from real users via the AI2 WildChat project, providing fine-grained annotations, task types, and checklists for response quality evaluation, and employs length-penalized Elo ratings to ensure unbiased assessments. FLASK [269] introduces a fine-grained evaluation protocol that decomposes overall scoring into skill set-level scoring for each instruction, enhancing interpretability and reliability in both human-based and model-based evaluations. Additionally, comprehensive evaluations covering multiple domains-including factual question answering, reading comprehension, summarization, mathematical problem-solving, reasoning, poetry generation, and programming-have been conducted. These evaluations involve assessing models across multiple criteria such as correctness, fluency, informativeness, logicality, and harmlessness.",
          "quote": [
            {
              "text": "[138]",
              "target": "#b137",
              "type": "bibr",
              "context": "WildBench ",
              "index": 26
            },
            {
              "text": "[269]",
              "target": "#b271",
              "type": "bibr",
              "context": "and FLASK ",
              "index": 42
            },
            {
              "text": "[138]",
              "target": "#b137",
              "type": "bibr",
              "context": "WildBench ",
              "index": 132
            },
            {
              "text": "[269]",
              "target": "#b271",
              "type": "bibr",
              "context": "ts. FLASK ",
              "index": 389
            }
          ]
        },
        {
          "text": "Reward models and LLM-based judges face the crucial task of ensuring alignment with human expectations, a challenge addressed by datasets like RewardBench [116], RM-Bench [148], and JudgerBench [20]. RewardBench [116] focuses on assessing models through complex prompt-choice trios, covering diverse areas like chat, reasoning, and safety, with a particular emphasis on out-ofdistribution scenarios. RM-Bench [148] introduces a new benchmark for evaluating reward models based on their sensitivity to subtle content differences and resistance to stylistic biases, emphasizing the need for refined assessments that correlate highly with aligned language models' performance. JudgerBench [20], with its dual components (JDB-A and JDB-B), offers a structured framework for evaluating alignment and critique abilities. By including data from human voting results and combining insights from varied sources, JudgerBench [20] provides a nuanced understanding of model performance across different languages and dialogue formats.",
          "quote": [
            {
              "text": "[116]",
              "target": "#b115",
              "type": "bibr",
              "context": "wardBench ",
              "index": 155
            },
            {
              "text": "[148]",
              "target": "#b148",
              "type": "bibr",
              "context": " RM-Bench ",
              "index": 171
            },
            {
              "text": "[20]",
              "target": "#b19",
              "type": "bibr",
              "context": "dgerBench ",
              "index": 194
            },
            {
              "text": "[116]",
              "target": "#b115",
              "type": "bibr",
              "context": "wardBench ",
              "index": 212
            },
            {
              "text": "[148]",
              "target": "#b148",
              "type": "bibr",
              "context": " RM-Bench ",
              "index": 409
            },
            {
              "text": "[20]",
              "target": "#b19",
              "type": "bibr",
              "context": "dgerBench ",
              "index": 686
            },
            {
              "text": "[20]",
              "target": "#b19",
              "type": "bibr",
              "context": "dgerBench ",
              "index": 915
            }
          ]
        },
        {
          "text": "With the growing complexity of tasks handled by LLMs, there is an increasing demand for more objective and reliable evaluation frameworks. JUDGEBENCH [213] proposes a novel approach to assessing LLM-based judges on challenging response pairs across domains like knowledge, reasoning, mathematics, and coding. It addresses the limitations of existing benchmarks by introducing preference labels that reflect objective correctness, providing a robust platform for evaluating the capabilities of advanced LLM-based judges.",
          "quote": [
            {
              "text": "[213]",
              "target": "#b215",
              "type": "bibr",
              "context": "UDGEBENCH ",
              "index": 150
            }
          ]
        },
        {
          "text": "As LLMs evolve beyond text-only tasks, evaluation frameworks have expanded to encompass multimodal and multilingual contexts. MLLM-as-a-Judge [24] serves as a benchmark for assessing Multimodal LLMs, covering tasks like image description, mathematical reasoning, and infographic interpretation. By integrating human annotations, it provides a comprehensive evaluation across visual and textual domains, reflecting the growing demand for models capable of processing diverse inputs. In a parallel effort, MM-Eval [202] addresses the multilingual aspect, offering extensive analysis across 18 languages. With core subsets like Chat, Reasoning, and Linguistics, alongside a broader Language Resource subset spanning 122 languages, MM-Eval [202] highlights performance discrepancies, especially in low-resource languages where models tend to default to neutral scores.",
          "quote": [
            {
              "text": "[24]",
              "target": "#b23",
              "type": "bibr",
              "context": "s-a-Judge ",
              "index": 142
            },
            {
              "text": "[202]",
              "target": "#b204",
              "type": "bibr",
              "context": ", MM-Eval ",
              "index": 512
            },
            {
              "text": "[202]",
              "target": "#b204",
              "type": "bibr",
              "context": ", MM-Eval ",
              "index": 736
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "6.2",
        "name": "Metric"
      },
      "p": [
        {
          "text": "The evaluation of LLMs-as-Judges models centers around assessing the extent to which the model's judgments align with human evaluations, which are typically considered the benchmark for quality. Given the complexity and subjectivity of many evaluation tasks, achieving high agreement with human ratings is a key indicator of the LLM's performance. To quantify this agreement, a range of statistical metrics is employed. Below, we outline these metrics and their applications in evaluating LLMs-as-Judges models.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "6.2.1",
        "name": "Accuracy."
      },
      "p": [
        {
          "text": "Accuracy is a fundamental metric used to assess the proportion of correct judgments made by the LLM compared to human evaluations. In classification tasks, it is defined as:",
          "quote": []
        },
        {
          "text": "where the number of correct predictions corresponds to instances where the LLM's judgment matches the human evaluator's judgment. While accuracy is simple to compute and intuitive, it may not fully capture the quality of the model, especially when dealing with tasks that involve nuanced or continuous evaluations. [41] measures the linear relationship between two continuous variables, in this case, the evaluation scores assigned by the LLM and those assigned by human evaluators. It is defined as:",
          "quote": [
            {
              "text": "[41]",
              "target": "#b40",
              "type": "bibr",
              "context": "luations. ",
              "index": 315
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "6.2.2",
        "name": "Pearson Correlation Coefficient. The Pearson Correlation Coefficient"
      },
      "p": [
        {
          "text": "where 𝑥 𝑖 and 𝑦 𝑖 are the scores from the LLM and the human, respectively, and x and ȳ are their means. Pearson correlation values range from −1 to 1.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "6.2.3",
        "name": "Spearman's Rank Correlation"
      },
      "p": [
        {
          "text": "Coefficient. Spearman's Rank Correlation Coefficient (𝜌) [190] assesses the monotonic relationship between two variables by comparing their ranked values rather than the raw scores. It is defined as:",
          "quote": [
            {
              "text": "[190]",
              "target": "#b190",
              "type": "bibr",
              "context": "ient (𝜌) ",
              "index": 58
            }
          ]
        },
        {
          "text": "where 𝑑 𝑖 is the difference between the ranks of corresponding scores from the LLM and the human evaluator, and 𝑛 is the number of paired scores. Spearman's 𝜌 is less sensitive to outliers and non-linear relationships compared to Pearson's correlation, making it a robust choice for evaluating tasks where the relative order of scores is more important than the exact values. It is commonly used in ranking-based evaluations such as preference judgments or ranking tasks.  [191] is another rank-based correlation metric that measures the ordinal association between two ranked lists. It is defined as:",
          "quote": [
            {
              "text": "[191]",
              "target": "#b191",
              "type": "bibr",
              "context": "g tasks.  ",
              "index": 477
            }
          ]
        },
        {
          "text": "where 𝐶 is the number of concordant pairs (where the rank order agrees between the LLM and human), and 𝐷 is the number of discordant pairs. Kendall's 𝜏 is particularly useful when evaluating the consistency of rankings produced by LLMs and human evaluators. It is often preferred when the dataset contains many ties, as it provides a more nuanced measure of agreement than Spearman's 𝜌.",
          "quote": []
        },
        {
          "text": "6.2.5 Cohen's Kappa. Cohen's Kappa (𝜅) [240] measures the level of agreement between two raters (in this case, the LLM and the human) beyond what would be expected by chance. It is defined as:",
          "quote": [
            {
              "text": "[240]",
              "target": "#b242",
              "type": "bibr",
              "context": "appa (𝜅) ",
              "index": 40
            }
          ]
        },
        {
          "text": "where 𝑝 𝑜 is the observed agreement and 𝑝 𝑒 is the expected agreement by chance. Cohen's Kappa is particularly effective in classification tasks where both the LLM and the human evaluators assign categorical labels. It accounts for the possibility of random agreement, making it a more robust metric than simple accuracy.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "6.2.6",
        "name": "Intraclass Correlation Coefficient (ICC)."
      },
      "p": [
        {
          "text": "The Intraclass Correlation Coefficient (ICC) [13] assesses the reliability of ratings when there are multiple evaluators. It evaluates the consistency or conformity of measurements made by different raters, including LLMs and human annotators. ICC is defined based on the variance components derived from a one-way or two-way ANOVA model. The ICC is particularly useful when comparing multiple LLMs or when evaluating the consistency of an LLM across different subsets of data, providing a broader view of its reliability as an evaluator.",
          "quote": [
            {
              "text": "[13]",
              "target": "#b12",
              "type": "bibr",
              "context": "ent (ICC) ",
              "index": 45
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "7",
        "name": "LIMITATION"
      },
      "p": [
        {
          "text": "Although the application of LLMs-as-judges holds great promise, there are still several significant limitations that can affect their effectiveness, reliability, and fairness [208, 215]. These limitations arise from the inherent characteristics of LLMs, including their reliance on large-scale data for training and token-based decoding mechanisms. In this section, we will primarily explore the limitations in the following three key aspets: Biases ( §7.1), Adversarial Attacks ( §7.2), and Inherent Weaknesses ( §7.3).",
          "quote": [
            {
              "text": "[208,",
              "target": "#b210",
              "type": "bibr",
              "context": " fairness ",
              "index": 175
            },
            {
              "text": "215]",
              "target": "#b217",
              "type": "bibr",
              "context": "ess [208, ",
              "index": 181
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "7.1",
        "name": "Biases"
      },
      "p": [
        {
          "text": "Essentially, LLMs are trained on vast amounts of data gathered from diverse sources. While this allows them to generate human-like responses, it also makes them inherit to the biases inherent Overconfidence Bias A tendency to exhibit inflated confidence in evaluation judgments, leading to overly assertive but potentially incorrect conclusions Self-Enhancement Bias A tendency to favor outputs generated by the same model acting as a judge, undermining objectivity.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": -1,
        "name": "Refinement-Aware Bias"
      },
      "p": [
        {
          "text": "A tendency for scoring variations influenced by whether an answer is original, refined, or accompanied by conversation history during evaluation.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": -1,
        "name": "Distraction Bias"
      },
      "p": [
        {
          "text": "A tendency to be influenced by irrelevant content, which can detract from the quality of judgments by diverting attention from critical elements.",
          "quote": []
        },
        {
          "text": "Fallacy-Oversight Bias A tendency to overlook logical fallacies, which can undermine the accuracy of judgments.",
          "quote": []
        },
        {
          "text": "in the training data. These biases are presented in various forms, which can significantly affect evaluation results, compromising the fairness and accuracy of decisions.",
          "quote": []
        },
        {
          "text": "To gain a deeper understanding of the impact of bias, we have provided a detailed classification of bias. As shown in Table 1.4), the biases exhibited by LLMs-as-judges can be systematically categorized into four groups based on their underlying causes and manifestations: Presentation-Related Biases ( §7.1.1), Social-Related Biases ( §7.1.2), Content-Related Biases ( §7.1.3), and Cognitive-Related Biases ( §7. . In this section, we provide a detailed overview of the definition, impact, and solutions to these biases.",
          "quote": [
            {
              "text": "1.4)",
              "target": "",
              "type": "bibr",
              "context": " in Table ",
              "index": 124
            }
          ]
        },
        {
          "text": "7.1.1 Presentation-Related Biases. Presentation-Related Biases refer to tendencies in LLMs where judgments are influenced more by the structure or presentation of information than by its substantive content. For example, models may prioritize certain formats, styles, or patterns of expression, which can affect the quality of the input. Next, we introduce two biases related to Presentation-Related Biases: position bias and verbosity bias. Position bias is a prevalent issue not only in the context of LLMs-as-judges but also in human decision-making and across various machine learning domains. Research has shown that humans are often influenced by the order of options presented to them, leading to biased decision that can impact fairness and objectivity [16, 182, 197, 288]. Similarly, in other ML applications, models trained on ordered data exhibit a positional preference, skewing outcomes based on the sequence of input [111, 234]. Position bias in LLMs-as-judges refers to the tendency of LLMs to favor certain answers based on their position in the response set. For example, when presented with multiple answer choices or compared pairwise, LLMs disproportionately select options that appear earlier in the list, leading to skewed judgment.",
          "quote": [
            {
              "text": "[16,",
              "target": "#b15",
              "type": "bibr",
              "context": "jectivity ",
              "index": 761
            },
            {
              "text": "182,",
              "target": "#b182",
              "type": "bibr",
              "context": "vity [16, ",
              "index": 766
            },
            {
              "text": "197,",
              "target": "#b199",
              "type": "bibr",
              "context": "[16, 182, ",
              "index": 771
            },
            {
              "text": "288]",
              "target": "#b290",
              "type": "bibr",
              "context": "182, 197, ",
              "index": 776
            },
            {
              "text": "[111,",
              "target": "#b110",
              "type": "bibr",
              "context": " of input ",
              "index": 931
            },
            {
              "text": "234]",
              "target": "#b236",
              "type": "bibr",
              "context": "put [111, ",
              "index": 937
            }
          ]
        },
        {
          "text": "Recent studies have further examined position bias in the LLMs-as-judges context. For instance, a framework [151] is proposed to investigate position bias in pairwise comparisons, introducing metrics such as repetition stability, position consistency, and preference fairness to better understand how positions affect LLM judgments. Another study [292] explores the limitations of LLMs-asjudges, including position biases, and verifies agreement between LLM judgments and human preferences across multiple benchmarks. These findings underscore the need for robust debiasing strategies to enhance the fairness and reliableness of LLMs-as-judges.",
          "quote": [
            {
              "text": "[151]",
              "target": "#b151",
              "type": "bibr",
              "context": "framework ",
              "index": 108
            },
            {
              "text": "[292]",
              "target": "#b294",
              "type": "bibr",
              "context": "her study ",
              "index": 347
            }
          ]
        },
        {
          "text": "Several methods are proposed to mitigate position bias. The naive approach involves excluding inconsistent judgments by swapping the positions of the candidate answers and verifying whether the LLM's judgment remains consistent. Inconsistent judgments are then filtered out [25, 127, 130, 230, 291, 292]. The swap-based debiasing method can be further divided into two categories: score-based and comparison-based. Both approaches start by swapping the positions of the candidate answers. The difference lies in how the final judgment is determined. In the score-based method, each candidate answer is scored, and the average score across multiple swaps is taken as the final score for that answer [87, 130, 184, 230, 292].In contrast, the comparison-based method considers the outcome a tie if the LLM's judgments are inconsistent after swapping. The conclusion of a tie is based on an analysis of the quality gap between answers. The larger the quality gap between candidate answers, the smaller the impact of position bias, resulting in higher consistency in predictions after swapping their positions, which is detailed in a recently study [151]. In addition to the aforementioned methods, PORTIA [134] employs an alignment-based approach that simulates human comparison strategies. It divides each answer into multiple segments, aligns similar content across candidate answers, and then merges these aligned segments into a single prompt for the LLM to evaluate. By presenting content in a balanced and aligned format, PORTIA enables the model to make more consistent and unbiased judgments, focusing on answer quality rather than order. This approach is effective across various LLMs, significantly improving evaluation consistency and reducing costs. Further efforts to enhance LLM-based evaluations have explored new techniques to address position bias and other judgment inconsistencies. Discussion-based methods [107, 132] incorporate peer ranking and discussion to improve evaluation accuracy. Instead of relying solely on a single LLM's judgment, these methods prompt multiple LLMs to compare answers and discuss preferences to reach a consensus, thereby reducing individual positional bias and enhancing alignment with human judgments. This collaborative evaluation approach represents a promising direction for mitigating biases inherent in LLM assessments. Verbosity bias [25, 107, 163, 292] refers to the tendency of a judge, whether human or model-based, to favor lengthier responses over shorter ones, irrespective of the actual content quality or relevance. This bias may cause LLMs prefer longer responses, even if the extended content does not contribute substantively to the correctness of the judgments.",
          "quote": [
            {
              "text": "[25,",
              "target": "#b24",
              "type": "bibr",
              "context": "tered out ",
              "index": 274
            },
            {
              "text": "127,",
              "target": "#b126",
              "type": "bibr",
              "context": " out [25, ",
              "index": 279
            },
            {
              "text": "130,",
              "target": "#b129",
              "type": "bibr",
              "context": "[25, 127, ",
              "index": 284
            },
            {
              "text": "230,",
              "target": "#b232",
              "type": "bibr",
              "context": "127, 130, ",
              "index": 289
            },
            {
              "text": "291,",
              "target": "#b293",
              "type": "bibr",
              "context": "130, 230, ",
              "index": 294
            },
            {
              "text": "292]",
              "target": "#b294",
              "type": "bibr",
              "context": "230, 291, ",
              "index": 299
            },
            {
              "text": "[87,",
              "target": "#b86",
              "type": "bibr",
              "context": "at answer ",
              "index": 698
            },
            {
              "text": "130,",
              "target": "#b129",
              "type": "bibr",
              "context": "swer [87, ",
              "index": 703
            },
            {
              "text": "184,",
              "target": "#b184",
              "type": "bibr",
              "context": "[87, 130, ",
              "index": 708
            },
            {
              "text": "230,",
              "target": "#b232",
              "type": "bibr",
              "context": "130, 184, ",
              "index": 713
            },
            {
              "text": "292]",
              "target": "#b294",
              "type": "bibr",
              "context": "184, 230, ",
              "index": 718
            },
            {
              "text": "[151]",
              "target": "#b151",
              "type": "bibr",
              "context": "tly study ",
              "index": 1144
            },
            {
              "text": "[134]",
              "target": "#b133",
              "type": "bibr",
              "context": "s, PORTIA ",
              "index": 1201
            },
            {
              "text": "[107,",
              "target": "#b106",
              "type": "bibr",
              "context": "d methods ",
              "index": 1922
            },
            {
              "text": "132]",
              "target": "#b131",
              "type": "bibr",
              "context": "ods [107, ",
              "index": 1928
            },
            {
              "text": "[25,",
              "target": "#b24",
              "type": "bibr",
              "context": "sity bias ",
              "index": 2387
            },
            {
              "text": "107,",
              "target": "#b106",
              "type": "bibr",
              "context": "bias [25, ",
              "index": 2392
            },
            {
              "text": "163,",
              "target": "#b163",
              "type": "bibr",
              "context": "[25, 107, ",
              "index": 2397
            },
            {
              "text": "292]",
              "target": "#b294",
              "type": "bibr",
              "context": "107, 163, ",
              "index": 2402
            }
          ]
        },
        {
          "text": "To mitigate verbosity bias in LLMs-as-judges, several approaches [107, 267, 270] have been proposed. One approach [107] employs persuasive debating techniques, structuring responses to prioritize substance. By training LLMs-as-judges to engage in a debate-like format, this method encourages clarity and relevance, reducing the tendency to favor verbose arguments that lack substantive content. Additionally, the CALM [267] framework introduces controlled modifications to systematically assess and quantify verbosity's impact on judgments, using automated perturbations to evaluate robustness against verbosity bias and refine LLMs-as-judges toward objective and concise assessments. Complementing these methods, the contrastive judgments (Con-J) [270] approach trains models with structured rationale pairs instead of scalar scores, encouraging LLMsas-judges to focus on well-reasoned content rather than associating verbosity with quality.",
          "quote": [
            {
              "text": "[107,",
              "target": "#b106",
              "type": "bibr",
              "context": "pproaches ",
              "index": 65
            },
            {
              "text": "267,",
              "target": "#b269",
              "type": "bibr",
              "context": "hes [107, ",
              "index": 71
            },
            {
              "text": "270]",
              "target": "#b272",
              "type": "bibr",
              "context": "107, 267, ",
              "index": 76
            },
            {
              "text": "[107]",
              "target": "#b106",
              "type": "bibr",
              "context": " approach ",
              "index": 114
            },
            {
              "text": "[267]",
              "target": "#b269",
              "type": "bibr",
              "context": " the CALM ",
              "index": 418
            },
            {
              "text": "[270]",
              "target": "#b272",
              "type": "bibr",
              "context": "s (Con-J) ",
              "index": 748
            }
          ]
        },
        {
          "text": "7.1.2 Social-Related Biases. Social-Related Biases refer to biases in language models that resemble social phenomena [289]. These biases may manifest when models are swayed by references to authoritative sources (Authority Bias), align with prevailing majority opinions without independent evaluation (Bandwagon-Effect Bias), or adjust their judgments based on anonymization strategies or identity markers such as gender and ethnicity (Compassion-Fade Bias and Diversity Bias). Next, we present the details of these biases. Authority bias [25, 267] in the context of LLMs-as-judges refers to the tendency of the model to attribute greater credibility to statements associated with authoritative references, regardless of the actual evidence supporting them. For instance, LLMs-as-judges may favor responses that include references to well-known sources or experts, even when the content is inaccurate or irrelevant. This bias highlights a critical vulnerability where the appearance of authority can unduly influence judgment outcomes.",
          "quote": [
            {
              "text": "[289]",
              "target": "#b291",
              "type": "bibr",
              "context": "phenomena ",
              "index": 117
            },
            {
              "text": "[25,",
              "target": "#b24",
              "type": "bibr",
              "context": "rity bias ",
              "index": 539
            },
            {
              "text": "267]",
              "target": "#b269",
              "type": "bibr",
              "context": "bias [25, ",
              "index": 544
            }
          ]
        },
        {
          "text": "While specific solutions to mitigate authority bias in LLMs-as-judges are still under active exploration, potential approaches include using retrieval-augmented generation (RAG) techniques to verify the validity of authoritative claims against external knowledge bases. This approach allows the model to cross-check referenced information and ensure its alignment with factual evidence. Another possible strategy is to design prompts that explicitly emphasize semantic accuracy and relevance over perceived authority. Further research is needed to validate these approaches and develop robust methods for addressing authority bias effectively in evaluative contexts. Bandwagon-effect bias [112, 267] in the context of LLMs-as-judges refers to the tendency of the model to align its judgments with the majority opinion or prevailing trends, regardless of the actual quality or correctness of the evaluated content. For instance, when multiple responses are presented with indications of popular support or consensus, LLMs-as-judges may disproportionately favor these responses over alternatives, even when the consensus is flawed or biased. This bias reflects a susceptibility to groupthink dynamics, undermining the objectivity and fairness of the judgment process.",
          "quote": [
            {
              "text": "[112,",
              "target": "#b111",
              "type": "bibr",
              "context": "fect bias ",
              "index": 689
            },
            {
              "text": "267]",
              "target": "#b269",
              "type": "bibr",
              "context": "ias [112, ",
              "index": 695
            }
          ]
        },
        {
          "text": "Solutions to address bandwagon-effect bias include designing evaluation prompts that anonymize information about majority opinions, ensuring that judgments are based solely on the intrinsic quality of the responses rather than external indicators of popularity. Further exploration of debiasing strategies tailored to specific evaluative contexts is necessary to mitigate the impact of bandwagon-effect bias effectively.",
          "quote": []
        },
        {
          "text": "Compassion-fade bias [112, 267] occurs when the anonymity of model names or the absence of identifiable contextual cues affects the judgments made by LLMs-as-judges. For example, anonymizing model names or using neutral identifiers may lead to shifts in evaluation outcomes. This bias highlights how the lack of personalized or contextual information can diminish the model's sensitivity to equitable considerations.",
          "quote": [
            {
              "text": "[112,",
              "target": "#b111",
              "type": "bibr",
              "context": "fade bias ",
              "index": 21
            },
            {
              "text": "267]",
              "target": "#b269",
              "type": "bibr",
              "context": "ias [112, ",
              "index": 27
            }
          ]
        },
        {
          "text": "To mitigate compassion-fade bias, it is important to design evaluation prompts that standardize judgment criteria, ensuring that assessments remain consistent regardless of whether identifying details are present. Additionally, fairness-driven frameworks that explicitly address anonymization effects can further enhance the reliability of LLMs-as-judges. Diversity bias [25, 267] in the context of LLMs-as-judges refers to the model's tendency to exhibit judgment shifts based on identity-related markers, such as gender, ethnicity, religion, or other social categorizations. For example, LLMs-as-judges might favor responses associated with certain demographic groups over others, leading to unfair or skewed judgments. This bias reflects the model's susceptibility to implicit stereotypes or unequal treatment of diverse identities present in the training data. Continued efforts to address this bias are crucial for ensuring fairness and inclusivity in the judgments conducted by LLMs-as-judges.",
          "quote": [
            {
              "text": "[25,",
              "target": "#b24",
              "type": "bibr",
              "context": "sity bias ",
              "index": 371
            },
            {
              "text": "267]",
              "target": "#b269",
              "type": "bibr",
              "context": "bias [25, ",
              "index": 376
            }
          ]
        },
        {
          "text": "7.1.3 Content-Related Biases. Content-Related Biases involve preferences or skewed judgments based on the content's characteristics. An LLM might favor responses with certain emotional tones (Sentiment Bias), prefer frequently occurring words from its training data (Token Bias), or be influenced by specific cultural or domain contexts leading to insensitive outcomes (Context Bias). We present the details of these biases in the following. Sentiment bias [267] in the context of LLMs-as-judges refers to the tendency of the model to favor responses that exhibit certain emotional tones, such as positive or neutral sentiments, over others, regardless of their actual content quality or relevance. For instance, LLMs-as-judges may disproportionately reward responses that are cheerful or optimistic while penalizing those that are negative or emotionally intense, even if the latter are more contextually appropriate or accurate.",
          "quote": [
            {
              "text": "[267]",
              "target": "#b269",
              "type": "bibr",
              "context": "ment bias ",
              "index": 457
            }
          ]
        },
        {
          "text": "To address sentiment bias, potential solution is the use of sentiment-neutralizing mechanisms, such as filtering or adjusting responses to remove sentiment-driven influences during evaluation. Token Bias [98, 127, 178, 184] refers to that LLMs favor certain tokens during the evaluation process. This bias often arises from the model's pre-training data, where more frequently occurring tokens are prioritized over less common ones, regardless of the contextual appropriateness or correctness in judgment. Contextual Bias refers to the tendency of LLMs to produce skewed or biased judgments based on the specific context in which they are applied. For instance, models used in healthcare may propagate biases found in medical datasets, potentially influencing diagnoses or treatment recommendations [179], while in finance, they might reflect biases in credit scoring or loan approval processes [300]. In addition, the selection of contextual examples may also introduce bias [62, 78, 290, 296].",
          "quote": [
            {
              "text": "[98,",
              "target": "#b97",
              "type": "bibr",
              "context": "oken Bias ",
              "index": 204
            },
            {
              "text": "127,",
              "target": "#b126",
              "type": "bibr",
              "context": "Bias [98, ",
              "index": 209
            },
            {
              "text": "178,",
              "target": "#b178",
              "type": "bibr",
              "context": "[98, 127, ",
              "index": 214
            },
            {
              "text": "184]",
              "target": "#b184",
              "type": "bibr",
              "context": "127, 178, ",
              "index": 219
            },
            {
              "text": "[179]",
              "target": "#b179",
              "type": "bibr",
              "context": "endations ",
              "index": 799
            },
            {
              "text": "[300]",
              "target": "#b302",
              "type": "bibr",
              "context": "processes ",
              "index": 895
            },
            {
              "text": "[62,",
              "target": "#b61",
              "type": "bibr",
              "context": "duce bias ",
              "index": 976
            },
            {
              "text": "78,",
              "target": "#b77",
              "type": "bibr",
              "context": "bias [62, ",
              "index": 981
            },
            {
              "text": "290,",
              "target": "#b292",
              "type": "bibr",
              "context": " [62, 78, ",
              "index": 985
            },
            {
              "text": "296]",
              "target": "#b298",
              "type": "bibr",
              "context": " 78, 290, ",
              "index": 990
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "7.1.4",
        "name": "Cognitive-Related Biases."
      },
      "p": [
        {
          "text": "Cognitive-Related Biases pertain to the inherent cognitive tendencies of LLMs in processing information. This includes exhibiting unwarranted confidence in judgments (Overconfidence Bias), favoring outputs generated by themselves (Self-Enhancement Bias), varying scores based on whether an answer is original or refined (Refinement-Aware Bias), being distracted by irrelevant information (Distraction Bias), or overlooking logical fallacies (Fallacy-Oversight Bias). The details of these biases are presented in the following. Overconfidence bias [103, 107] in the context of LLMs-as-judges refers to the tendency of models to exhibit an inflated level of confidence in their judgments, often resulting in overly assertive evaluations that may not accurately reflect the true reliability of the answer. This bias is particularly concerning in evaluative contexts, as it can lead LLMs-as-judges to overstate the correctness of certain outputs, compromising the objectivity and dependability of assessments.",
          "quote": [
            {
              "text": "[103,",
              "target": "#b102",
              "type": "bibr",
              "context": "ence bias ",
              "index": 547
            },
            {
              "text": "107]",
              "target": "#b106",
              "type": "bibr",
              "context": "ias [103, ",
              "index": 553
            }
          ]
        },
        {
          "text": "To address Overconfidence bias, researchers have proposed several methods. Cascaded Selective Evaluation [103] addresses overconfidence by using Simulated Annotators to estimate confidence. This involves simulating diverse annotator preferences through in-context learning, which provides a more realistic measure of the likelihood that a human would agree with the LLM's judgment. By analyzing multiple simulated responses, this method offers a confidence metric that reflects human-like disagreement, which helps to avoid overconfidence bias. Another method uses an adversarial debate mechanism, where two LLMs argue for different outcomes. Through structured debate rounds, each model is required to substantiate its position, which can reveal overconfidence by prompting self-reflection and critical analysis. This approach has been shown to improve truthfulness and reduces overconfidence by fostering a balanced evaluation, aligning LLMs' judgments more closely with accurate and reasoned conclusions. Self-enhancement bias is the tendency to favor their own outputs [132, 145, 145, 172, 292]. This concept of self-enhancement is drawn from social psychology, as discussed in Brown's work in social cognition literature [19]. In the context of LLMs-as-judges, this bias manifests when a LLM evaluates its own generated outputs more favorably than those of other LLMs. Such bias is particularly concerning in applications involving self-assessment or feedback generation, as it compromises the objectivity of the LLMs-as-judges.",
          "quote": [
            {
              "text": "[103]",
              "target": "#b102",
              "type": "bibr",
              "context": "valuation ",
              "index": 105
            },
            {
              "text": "[132,",
              "target": "#b131",
              "type": "bibr",
              "context": "n outputs ",
              "index": 1073
            },
            {
              "text": "145,",
              "target": "#b145",
              "type": "bibr",
              "context": "uts [132, ",
              "index": 1079
            },
            {
              "text": "145,",
              "target": "#b145",
              "type": "bibr",
              "context": "132, 145, ",
              "index": 1084
            },
            {
              "text": "172,",
              "target": "#b172",
              "type": "bibr",
              "context": "145, 145, ",
              "index": 1089
            },
            {
              "text": "292]",
              "target": "#b294",
              "type": "bibr",
              "context": "145, 172, ",
              "index": 1094
            },
            {
              "text": "[19]",
              "target": "#b18",
              "type": "bibr",
              "context": "iterature ",
              "index": 1226
            }
          ]
        },
        {
          "text": "To address self-enhancement bias, PRD [132] introduces Peer Rank (PR) and Peer Discussion (PD) mechanisms. PR mitigates bias by using multiple LLMs as reviewers, each assessing pairwise comparisons between responses from different LLMs. By aggregating evaluations from several peer LLMs and weighting their preferences based on consistency with human judgments, PR reduces the impact of any single LLM's self-enhancement bias, as more reliable reviewers have a greater influence. PD further alleviates self-enhancement bias by enabling two LLMs to engage in a dialogue to reach a mutual agreement on their preference between two answers. This multiturn discussion encourages models to re-evaluate their initial judgments and consider alternative perspectives, focusing on content quality rather than self-generated responses. By promoting collaborative assessment and accountability, PR and PD effectively mitigate self-enhancement bias, aligning evaluations more closely with human standards. Recently, an automated bias quantification framework named CALM [267] has been proposed to systematically evaluate biases in LLMs-asjudges. CALM's findings suggest that one effective way to reduce self-enhancement bias is to avoid using the same model to both generate and judge answers, thereby ensuring that evaluation remains more impartial. Moreover, the Reference-Guided Verdict method [8] further addresses self-enhancement bias by providing a definitive gold-standard answer as a reference for LLM judges. This reference anchor helps align judgments to objective criteria, even when an LLM evaluates its own output, thus reducing the tendency to favor self-generated answers. Through structured prompts, this method has been shown to enhance reliability and mitigate variability in judgments, especially when multiple LLMs are used collectively. The integration of multiple LLMs, trained on varied datasets or fine-tuned with different parameters, has proven instrumental in producing less biased, more balanced evaluations, highlighting the effectiveness of model diversity and reference-guided criteria in combating self-enhancement bias. Refinement-aware bias [267] in the context of LLMs-as-judges refers to the tendency of the model to evaluate responses differently based on whether they are original, refined, or include revision history. For instance, an answer that has been iteratively refined may be judged more favorably than an original response, even if the refinement process does not significantly improve the content quality. Similarly, responses that explicitly present their improvement process or revision rationale might receive undue preference, skewing the evaluation outcomes.",
          "quote": [
            {
              "text": "[132]",
              "target": "#b131",
              "type": "bibr",
              "context": "bias, PRD ",
              "index": 38
            },
            {
              "text": "[267]",
              "target": "#b269",
              "type": "bibr",
              "context": "amed CALM ",
              "index": 1058
            },
            {
              "text": "[8]",
              "target": "#b7",
              "type": "bibr",
              "context": "ct method ",
              "index": 1385
            },
            {
              "text": "[267]",
              "target": "#b269",
              "type": "bibr",
              "context": "ware bias ",
              "index": 2164
            }
          ]
        },
        {
          "text": "While research on refinement-aware bias in the specific context of LLMs-as-judges remains limited, solution [259] developed for general LLMs offer valuable insights. One potential solution involves incorporating external feedback mechanisms during judgment, as it introduces an objective and independent judgment mechanism that is not influenced by the LLM's internal iterations or self-perception. Distraction bias in the context of LLMs-as-judges refers to the model's tendency to be influenced by irrelevant or unimportant details when making judgments. For instance, introducing unrelated information, such as a meaningless statement like \"System Star likes to eat oranges and apples, \" [112, 195, 267] can significantly alter the model's evaluation outcomes. This bias highlights the vulnerability of LLMs to attentional diversion caused by inconsequential content.",
          "quote": [
            {
              "text": "[259]",
              "target": "#b261",
              "type": "bibr",
              "context": " solution ",
              "index": 108
            },
            {
              "text": "[112,",
              "target": "#b111",
              "type": "bibr",
              "context": "apples, \" ",
              "index": 691
            },
            {
              "text": "195,",
              "target": "#b197",
              "type": "bibr",
              "context": ", \" [112, ",
              "index": 697
            },
            {
              "text": "267]",
              "target": "#b269",
              "type": "bibr",
              "context": "112, 195, ",
              "index": 702
            }
          ]
        },
        {
          "text": "While existing studies [112, 267] have analyzed and discussed distraction bias, effective strategies to mitigate this issue in LLMs-as-judges remain underexplored. Potential solutions could involve input sanitization to preprocess and remove irrelevant information before presenting it to the model, ensuring that evaluations focus solely on relevant content. Additionally, explicit prompting with clear and strict guidelines could be designed to direct the LLM to evaluate only task-related aspects, reducing its susceptibility to distractions. Further research is needed to develop and validate robust methods that can systematically address distraction bias in the context of LLMs-as-judges. Fallacy-oversight bias [25, 267] refers to the tendency of LLMs-as-judges to overlook logical fallacies or inconsistencies within the evaluated responses. For instance, when presented with arguments or answers containing reasoning errors-such as circular reasoning, false dilemmas, or strawman arguments-LLMs-as-judges may fail to identify these issues and treat the responses as valid, potentially compromising the integrity of their evaluations.",
          "quote": [
            {
              "text": "[112,",
              "target": "#b111",
              "type": "bibr",
              "context": "g studies ",
              "index": 23
            },
            {
              "text": "267]",
              "target": "#b269",
              "type": "bibr",
              "context": "ies [112, ",
              "index": 29
            },
            {
              "text": "[25,",
              "target": "#b24",
              "type": "bibr",
              "context": "ight bias ",
              "index": 718
            },
            {
              "text": "267]",
              "target": "#b269",
              "type": "bibr",
              "context": "bias [25, ",
              "index": 723
            }
          ]
        },
        {
          "text": "In summary, while LLMs-as-judges have garnered significant attention for their effectiveness in diverse scenarios, the exploration of various biases that impact their performance remains relatively underdeveloped. These biases pose significant challenges to ensuring fair, objective, and reliable judgments across tasks, particularly in various applications where the implications of biased judgments can be severe. Future research must focus on systematically identifying, quantifying, and addressing these biases within the LLMs-as-judges framework. Drawing from methodologies developed for general LLMs, such as external feedback mechanisms, balanced datasets, and fairnessaware prompting, could offer initial insights. However, domain-specific challenges require tailored solutions that align with the unique demands of LLMs-as-judges.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "7.2",
        "name": "Adversarial Attacks"
      },
      "p": [
        {
          "text": "Adversarial attacks involve carefully crafted inputs designed to deceive the model into producing incorrect or unintended outputs. For LLM judges, attackers may subtly modify the input content, alter the wording of questions, or introduce misleading context to influence the model's evaluation results. Researchers have found that for LLMs, even small, seemingly insignificant changes to the input data, such as adding or removing words, changing word order, or introducing ambiguous phrasing, can significantly affect the model's response [100, 193, 305]. Such attacks can lead to inaccurate ratings or assessments, particularly when evaluating complex or high-risk tasks.",
          "quote": [
            {
              "text": "[100,",
              "target": "#b99",
              "type": "bibr",
              "context": " response ",
              "index": 540
            },
            {
              "text": "193,",
              "target": "#b195",
              "type": "bibr",
              "context": "nse [100, ",
              "index": 546
            },
            {
              "text": "305]",
              "target": "#b307",
              "type": "bibr",
              "context": "100, 193, ",
              "index": 551
            }
          ]
        },
        {
          "text": "In this section, we first review research on adversarial attacks on LLMs within general domains. Then, we specifically focus on adversarial attacks in the context of LLMs-as-judges.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "7.2.1",
        "name": "Adversarial Attacks on LLMs."
      },
      "p": [
        {
          "text": "Adversarial attacks on LLMs focus on exploiting vulnerabilities within the general framework of language model functionality. These attacks can be classified into three main categories based on the manipulation level: text-level manipulations, structural and semantic distortions, and optimization-based attacks.",
          "quote": []
        },
        {
          "text": "Text-Level Manipulations involve subtle changes to the input text to deceive the model. Character-level perturbations, such as introducing typos, swapping letters, or inserting unnecessary characters, can cause significant changes in predictions despite minimal visible alterations [57, 100]. Sentence-level modifications, such as rearranging phrases, adding irrelevant information, or paraphrasing inputs, further exploit the model's sensitivity to surface-level changes [18, 177].",
          "quote": [
            {
              "text": "[57,",
              "target": "#b56",
              "type": "bibr",
              "context": "terations ",
              "index": 282
            },
            {
              "text": "100]",
              "target": "#b99",
              "type": "bibr",
              "context": "ions [57, ",
              "index": 287
            },
            {
              "text": "[18,",
              "target": "#b17",
              "type": "bibr",
              "context": "l changes ",
              "index": 472
            },
            {
              "text": "177]",
              "target": "#b177",
              "type": "bibr",
              "context": "nges [18, ",
              "index": 477
            }
          ]
        },
        {
          "text": "Structural and Semantic Distortions focus on the syntactic and semantic properties of the input. Syntactic attacks rewrite sentence structures while preserving semantic meaning, targeting the model's reliance on specific linguistic patterns [260]. Semantic preservation with perturbations modifies critical tokens identified through saliency analysis, ensuring the attack minimally affects meaning but significantly alters predictions.",
          "quote": [
            {
              "text": "[260]",
              "target": "#b262",
              "type": "bibr",
              "context": " patterns ",
              "index": 241
            }
          ]
        },
        {
          "text": "Optimization-Based Attacks leverage algorithmic techniques to craft adversarial inputs. Gradient-based methods utilize the model's gradients to identify and manipulate influential input features, causing substantial shifts in predictions [210, 211]. Population-based optimization techniques iteratively generate adversarial examples in black-box settings, exploiting the model's outputs to refine attacks [120].",
          "quote": [
            {
              "text": "[210,",
              "target": "#b212",
              "type": "bibr",
              "context": "edictions ",
              "index": 238
            },
            {
              "text": "211]",
              "target": "#b213",
              "type": "bibr",
              "context": "ons [210, ",
              "index": 244
            },
            {
              "text": "[120]",
              "target": "#b119",
              "type": "bibr",
              "context": "e attacks ",
              "index": 405
            }
          ]
        },
        {
          "text": "These attacks highlight the vulnerabilities in LLMs, demonstrating their susceptibility to subtle manipulations. Studying these adversarial attacks is essential, as it provides insights that can guide the development of robust defense mechanisms, ensuring that LLMs maintain reliability against such manipulations.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "7.2.2",
        "name": "Adversarial"
      },
      "p": [
        {
          "text": "Attacks on LLMs-as-judges. Recent studies have unveiled significant vulnerabilities in LLMs-as-judges to adversarial attacks [51, 184, 196, 293]. Zheng et al. [293] and Doddapaneni et al. [51] demonstrated that automatic benchmarking systems like MT-Bench [292] can be easily deceived to yield artificially high scores. These findings highlight that malicious inputs can manipulate evaluation metrics, undermining the reliability of such benchmarks.",
          "quote": [
            {
              "text": "[51,",
              "target": "#b50",
              "type": "bibr",
              "context": "l attacks ",
              "index": 125
            },
            {
              "text": "184,",
              "target": "#b184",
              "type": "bibr",
              "context": "acks [51, ",
              "index": 130
            },
            {
              "text": "196,",
              "target": "#b198",
              "type": "bibr",
              "context": "[51, 184, ",
              "index": 135
            },
            {
              "text": "293]",
              "target": "#b295",
              "type": "bibr",
              "context": "184, 196, ",
              "index": 140
            },
            {
              "text": "[293]",
              "target": "#b295",
              "type": "bibr",
              "context": "ng et al. ",
              "index": 159
            },
            {
              "text": "[51]",
              "target": "#b50",
              "type": "bibr",
              "context": "ni et al. ",
              "index": 188
            },
            {
              "text": "[292]",
              "target": "#b294",
              "type": "bibr",
              "context": " MT-Bench ",
              "index": 256
            }
          ]
        },
        {
          "text": "Building on this, Raina et al. [184] investigated the robustness of LLMs-as-judges against universal adversarial attacks. Their work showed that appending short, carefully crafted phrases to evaluated texts can effortlessly manipulate LLM scores, inflating them to their maximum regardless of the actual quality. Remarkably, these universal attack phrases are transferable across models; phrases optimized on smaller surrogate models (e.g., FlanT5-xl [40]) can successfully deceive larger models like GPT-3.5 and Llama2 [218].",
          "quote": [
            {
              "text": "[184]",
              "target": "#b184",
              "type": "bibr",
              "context": "na et al. ",
              "index": 31
            },
            {
              "text": "[40]",
              "target": "#b39",
              "type": "bibr",
              "context": "FlanT5-xl ",
              "index": 451
            },
            {
              "text": "[218]",
              "target": "#b220",
              "type": "bibr",
              "context": "nd Llama2 ",
              "index": 520
            }
          ]
        },
        {
          "text": "Furthermore, Shi et al. [196] introduced JudgeDeceiver, an optimization-based prompt injection attack tailored for the LLMs-as-judges framework. Unlike handcrafted methods, JudgeDeceiver formulates a precise optimization objective to efficiently generate adversarial sequences. These sequences can mislead LLMs-as-judges into selecting biased or incorrect responses among candidate answers, thereby compromising the evaluation process.",
          "quote": [
            {
              "text": "[196]",
              "target": "#b198",
              "type": "bibr",
              "context": "hi et al. ",
              "index": 24
            }
          ]
        },
        {
          "text": "Although preliminary studies [184, 196] have highlighted the vulnerability of LLMs-as-judges to adversarial manipulations, this field remains largely underexplored. It is imperative to advance our understanding of these weaknesses and devise effective defense strategies. As the use of LLMs-asjudges grows across diverse applications, future research should focus on uncovering new attack methods and strengthening the models against such adversarial threats.",
          "quote": [
            {
              "text": "[184,",
              "target": "#b184",
              "type": "bibr",
              "context": "y studies ",
              "index": 29
            },
            {
              "text": "196]",
              "target": "#b198",
              "type": "bibr",
              "context": "ies [184, ",
              "index": 35
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "7.3",
        "name": "Inherent Weaknesses"
      },
      "p": [
        {
          "text": "Despite the remarkable capabilities of LLMs, they possess several inherent weaknesses that can compromise their reliability and robustness in LLMs-as-judges. This subsection discusses key limitations, including issues related to knowledge recency, hallucination, and other domain-specific knowledge gaps [287]. 7.3.1 Knowledge Recency. One significant limitation of LLMs is their inability to access or incorporate up-to-date information reliably. LLMs are generally trained on static datasets that may become outdated over time, limiting their ability to evaluate scenarios that require knowledge of recent events, legislation, or rapidly evolving fields. The most straightforward solution is to retrain the model on new data; however, this approach is resource-intensive and risks catastrophic forgetting [152], where previously learned knowledge is overwritten during training. This temporal disconnect can lead to judgments based on invalid data, or obsolete practices, compromising their reliability in real-world, time-sensitive applications. Consider a case where LLMs-as-judges are used to evaluate which of two responses from LLMs better answers a prompt about the COVID-19 pandemic. Suppose one response references the WHO guidelines updated timely, while the other relies on outdated 2020 guidelines. If the LLM-as-Judge has not been updated with the latest guidelines, it might erroneously prefer the outdated response, incorrectly deeming it more accurate. This failure to account for recent developments highlights the importance of addressing knowledge recency in LLMs-as-judges.",
          "quote": [
            {
              "text": "[287]",
              "target": "#b289",
              "type": "bibr",
              "context": "edge gaps ",
              "index": 304
            },
            {
              "text": "[152]",
              "target": "#b152",
              "type": "bibr",
              "context": "orgetting ",
              "index": 807
            }
          ]
        },
        {
          "text": "Addressing the issue of knowledge recency can involve integrating retrieval-augmented generation (RAG) methods [69, 125], which enable LLMs to query external, dynamically updated databases or knowledge sources during evaluation. Additionally, periodic fine-tuning with updated datasets or leveraging continual learning frameworks [244] can ensure that LLMs-as-judges remain aligned with the latest information. Combining these approaches with robust fact-checking mechanisms [48] can further enhance temporal reliability in judgment contexts.",
          "quote": [
            {
              "text": "[69,",
              "target": "#b68",
              "type": "bibr",
              "context": ") methods ",
              "index": 111
            },
            {
              "text": "125]",
              "target": "#b124",
              "type": "bibr",
              "context": "hods [69, ",
              "index": 116
            },
            {
              "text": "[244]",
              "target": "#b246",
              "type": "bibr",
              "context": "rameworks ",
              "index": 330
            },
            {
              "text": "[48]",
              "target": "#b47",
              "type": "bibr",
              "context": "echanisms ",
              "index": 475
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "7.3.2",
        "name": "Hallucination."
      },
      "p": [
        {
          "text": "Another critical issue in LLMs is the hallucination problem, where models generate incorrect or fabricated information with high confidence. In the context of LLMs-as-judges, hallucination can manifest as the invention of non-existent precedents, misinterpretation of facts, or fabrication of sources, which can severely undermine the reliability of their judgments. This issue is particularly concerning in various applications, where such errors can lead to unfair or harmful outcomes.",
          "quote": []
        },
        {
          "text": "Employing fact-checking mechanisms [48, 97, 216] during evaluation is crucial to mitigate hallucination. By cross-verifying the outputs of LLMs-as-judges with trusted databases and external knowledge sources, hallucinated information can be identified and corrected. 7.3.3 Domain-Specific Knowledge Gaps. While LLMs demonstrate broad generalization capabilities, they often lack the depth of understanding required for specialized domains [55, 63, 171, 212]. For instance, legal judgments demand intricate knowledge of statutes, precedents, and contextual nuances, which may not be adequately captured in the training data of general-purpose LLMs. This limitation can lead to shallow or incorrect judgments in domain-specific contexts.",
          "quote": [
            {
              "text": "[48,",
              "target": "#b47",
              "type": "bibr",
              "context": "echanisms ",
              "index": 35
            },
            {
              "text": "97,",
              "target": "#b96",
              "type": "bibr",
              "context": "isms [48, ",
              "index": 40
            },
            {
              "text": "216]",
              "target": "#b218",
              "type": "bibr",
              "context": " [48, 97, ",
              "index": 44
            },
            {
              "text": "[55,",
              "target": "#b54",
              "type": "bibr",
              "context": "d domains ",
              "index": 439
            },
            {
              "text": "63,",
              "target": "#b62",
              "type": "bibr",
              "context": "ains [55, ",
              "index": 444
            },
            {
              "text": "171,",
              "target": "#b171",
              "type": "bibr",
              "context": " [55, 63, ",
              "index": 448
            },
            {
              "text": "212]",
              "target": "#b214",
              "type": "bibr",
              "context": " 63, 171, ",
              "index": 453
            }
          ]
        },
        {
          "text": "Domain adaptation techniques, such as integrating LLMs with domain-specific knowledge graphs [63, 171] or leveraging RAG systems [69], can substantially improve their performance in specialized domains. Knowledge graphs provide structured, expert-curated information that enhances context-awareness, while RAG enables LLMs to dynamically retrieve relevant knowledge from specific domain.",
          "quote": [
            {
              "text": "[63,",
              "target": "#b62",
              "type": "bibr",
              "context": "ge graphs ",
              "index": 93
            },
            {
              "text": "171]",
              "target": "#b171",
              "type": "bibr",
              "context": "aphs [63, ",
              "index": 98
            },
            {
              "text": "[69]",
              "target": "#b68",
              "type": "bibr",
              "context": "G systems ",
              "index": 129
            }
          ]
        },
        {
          "text": "The inherent weaknesses of LLMs highlight the need for continued research and innovation. Addressing these limitations through RAG, enhanced training methods, and knowledge graph techniques is crucial for ensuring that LLMs-as-judges deliver reliable, accurate, and trustworthy evaluations in diverse applications.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "8",
        "name": "FUTURE WORK"
      },
      "p": [
        {
          "text": "In this section, we will explore the key directions for future work, focusing on how to build more efficient, more effective, and more reliable LLM judges. These directions aim to address the bottlenecks and challenges in current technologies and practices, while also promoting broader applications and deeper integration of LLM judges in diverse scenarios.",
          "quote": []
        },
        {
          "text": "8.1 More Efficient LLMs-as-Judges 8.1.1 Automated Construction of Evaluation Criteria and Tasks. Current LLM judges often rely on manually predefined evaluation criteria, lacking the ability to adapt dynamically during the assessment process. Designing prompts for these systems is not only tedious and time-consuming but also struggles to address the diverse requirements of various task scenarios [10, 233, 273, 280, 286]. To overcome these limitations, future LLM judges could incorporate enhanced adaptability by tailoring evaluation criteria based on task types, target audiences, and domain-specific knowledge. Such advancements would significantly streamline the configuration process of LLM judges, while also greatly improving their practicality and efficiency in real-world applications.",
          "quote": [
            {
              "text": "[10,",
              "target": "#b9",
              "type": "bibr",
              "context": "scenarios ",
              "index": 399
            },
            {
              "text": "233,",
              "target": "#b235",
              "type": "bibr",
              "context": "rios [10, ",
              "index": 404
            },
            {
              "text": "273,",
              "target": "#b275",
              "type": "bibr",
              "context": "[10, 233, ",
              "index": 409
            },
            {
              "text": "280,",
              "target": "#b282",
              "type": "bibr",
              "context": "233, 273, ",
              "index": 414
            },
            {
              "text": "286]",
              "target": "#b288",
              "type": "bibr",
              "context": "273, 280, ",
              "index": 419
            }
          ]
        },
        {
          "text": "Moreover, existing static evaluation datasets are prone to issues such as training data contamination, which can compromise their effectiveness in accurately assessing the evolving capabilities of LLMs. To address this, future LLM judges could focus on dynamically constructing more suitable evaluation tasks and continuously optimizing the evaluation process, thereby enhancing applicability and precision [10, 286].",
          "quote": [
            {
              "text": "[10,",
              "target": "#b9",
              "type": "bibr",
              "context": "precision ",
              "index": 407
            },
            {
              "text": "286]",
              "target": "#b288",
              "type": "bibr",
              "context": "sion [10, ",
              "index": 412
            }
          ]
        },
        {
          "text": "8.1.2 Scalable Evaluation Systems. Existing LLM judges often exhibit limited adaptability in practical applications. While these judges may perform effectively on specific downstream tasks, they frequently struggle in cross-domain or multi-task settings, thereby falling short of meeting the diverse and broader demands of real-world applications.",
          "quote": []
        },
        {
          "text": "To address these limitations, future research could focus on modular design principles to create scalable evaluation frameworks [257]. Such frameworks would allow users to flexibly add or customize evaluation modules to suit their specific needs. This modular approach not only enhances the usability and flexibility of the system but also significantly reduces the cost and complexity of transferring the framework across different domains.",
          "quote": [
            {
              "text": "[257]",
              "target": "#b259",
              "type": "bibr",
              "context": "rameworks ",
              "index": 128
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "8.1.3",
        "name": "Accelerating Evaluation Processes."
      },
      "p": [
        {
          "text": "Existing LLM systems often face significant computational costs when performing evaluation tasks. For example, pairwise comparison methods require multiple rounds of comparisons for each candidate, which becomes extremely time-consuming as the number of candidates grows. In resource-constrained environments, such high-cost evaluation methods are challenging to deploy effectively. To address this issue, future research could focus on developing more efficient candidate selection algorithms, thereby unlocking new opportunities for the use of LLMs in low-resource settings [123, 149].",
          "quote": [
            {
              "text": "[123,",
              "target": "#b122",
              "type": "bibr",
              "context": " settings ",
              "index": 576
            },
            {
              "text": "149]",
              "target": "#b149",
              "type": "bibr",
              "context": "ngs [123, ",
              "index": 582
            }
          ]
        },
        {
          "text": "Similarly, the multi-LLM evaluation paradigm, which relies on multiple rounds of interaction, further exacerbates computational demands. To mitigate these challenges, future efforts could explore streamlined communication frameworks that support high-quality evaluation tasks while minimizing resource requirements [31]. Advances in these areas could lead to the development of more efficient and scalable evaluation systems, making LLM-based evaluations more practical across diverse and resource-limited scenarios.",
          "quote": [
            {
              "text": "[31]",
              "target": "#b30",
              "type": "bibr",
              "context": "uirements ",
              "index": 315
            }
          ]
        },
        {
          "text": "8.2 More Effective LLMs-as-Judges 8.2.1 Integration of Reasoning and Judge Capabilities. Current LLMs-as-judges systems often treat reasoning and evaluation capabilities as distinct and independent modules, which can hinder effectiveness when addressing complex tasks. As the demand for evaluating increasingly complex systems grows, future LLM-as-Judge systems should prioritize the deep integration of reasoning and evaluation capabilities to achieve a seamless synergy [207, 271, 304]. For instance, in legal scenarios, the model could first infer the relevant legal provisions and then assess the case's relevance, making the evaluation process more effective. 8.2.2 Establishing a Collective Judgment Mechanism. Current LLMs-as-Judge systems typically rely on a single model for evaluation. While this approach is straightforward, it is prone to biases inherent in individual models, leading to reduced accuracy and stabilitys. Moreover, a single model often struggles to comprehensively address the diverse requirements of such tasks. Future research could investigate collaborative multi-agent mechanisms to enable \"collective judgment\" where multiple LLMs work together, leveraging their respective strengths in reasoning and knowledge [22, 39],. Additionally, ensemble techniques could be employed to dynamically balance the contributions of different models, leading to more stable and reliable judgment outcomes. 8.2.3 Enhancing Domain Knowledge. Current LLMs-as-Judge systems often fall short when handling tasks in specialized fields due to insufficient domain knowledge. Furthermore, as domain knowledge continues to evolve, these models struggle to keep up with the latest developments, further limiting their effectiveness and applicability in real-world scenarios.",
          "quote": [
            {
              "text": "[207,",
              "target": "#b209",
              "type": "bibr",
              "context": "s synergy ",
              "index": 472
            },
            {
              "text": "271,",
              "target": "#b273",
              "type": "bibr",
              "context": "rgy [207, ",
              "index": 478
            },
            {
              "text": "304]",
              "target": "#b306",
              "type": "bibr",
              "context": "207, 271, ",
              "index": 483
            },
            {
              "text": "[22,",
              "target": "#b21",
              "type": "bibr",
              "context": "knowledge ",
              "index": 1244
            },
            {
              "text": "39]",
              "target": "#b38",
              "type": "bibr",
              "context": "edge [22, ",
              "index": 1249
            }
          ]
        },
        {
          "text": "To address these challenges, future LLMs-as-judges systems should focus on integrating comprehensive domain knowledge to enhance their performance in specialized tasks [185]. This can be achieved by utilizing knowledge graphs, embedding domain-specific expertise, and fine-tuning models based on feedback from subject-matter experts. In addition, these systems should incorporate dynamic knowledge updating capabilities. For instance, in the legal domain, models could regularly acquire and integrate updates on new statutes, case law, and policy changes, ensuring that their judgments remain current and aligned with the latest legal standards.",
          "quote": [
            {
              "text": "[185]",
              "target": "#b185",
              "type": "bibr",
              "context": "zed tasks ",
              "index": 168
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": -1,
        "name": "8.2.4"
      },
      "p": [
        {
          "text": "Cross-Domain and Cross-Language Transferability. Current LLMs-as-Judge systems are often confined to specific domains or languages, making it challenging for them to transfer across different fields. For instance, an LLM proficient in processing legal texts may struggle to effectively handle evaluation tasks in the medical or financial domains. This limitation greatly restricts the applicability of such systems.",
          "quote": []
        },
        {
          "text": "Future research can focus on exploring cross-domain and cross-language transfer learning techniques to enhance the adaptability of LLMs in diverse fields. By leveraging shared general knowledge across fields, models can quickly adapt to new tasks with minimal additional training costs [77, 202, 241]. For example, evaluation capabilities developed in English could be transferred to contexts in German, thereby improving the evaluation performance in these new areas.",
          "quote": [
            {
              "text": "[77,",
              "target": "#b76",
              "type": "bibr",
              "context": "ing costs ",
              "index": 286
            },
            {
              "text": "202,",
              "target": "#b204",
              "type": "bibr",
              "context": "osts [77, ",
              "index": 291
            },
            {
              "text": "241]",
              "target": "#b243",
              "type": "bibr",
              "context": "[77, 202, ",
              "index": 296
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "8.2.5",
        "name": "Multimodal Integration Evaluation."
      },
      "p": [
        {
          "text": "Current LLM-as-Judge systems primarily focus on processing textual data, with limited attention to integrating other modalities like images, audio, and video. This single-modal approach falls short in complex scenarios requiring multimodal analysis, such as combining visual and textual information in medical assessments. Future systems should develop cross-modal integration capabilities to process and evaluate multimodal data simultaneously [24]. Leveraging cross-modal validation can enhance evaluation accuracy. Key research areas include efficient multimodal feature extraction, integration, and the design of unified frameworks for more comprehensive and precise evaluations.",
          "quote": [
            {
              "text": "[24]",
              "target": "#b23",
              "type": "bibr",
              "context": "taneously ",
              "index": 445
            }
          ]
        },
        {
          "text": "8.3 More Reliable LLMs-as-Judges 8.3.1 Enhancing Interpretability and Transparency. Current LLM-as-Judge systems often operate as black boxes, with their rulings lacking transparency and a clear reasoning process. This opacity is particularly concerning in high-stakes domains such as legal judgments, where users cannot fully understand the basis of the model's decisions or trust its outputs. Future research should focus on improving the interpretability of LLMs [147]. For example, the LLM judges should not only provide evaluation results but also present a clear explanation. Research could explore designing validation models based on logical frameworks to make the decision-making process more transparent.",
          "quote": [
            {
              "text": "[147]",
              "target": "#b147",
              "type": "bibr",
              "context": "y of LLMs ",
              "index": 466
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "8.3.2",
        "name": "Mitigating"
      },
      "p": [
        {
          "text": "Bias and Ensuring Fairness. LLMs may be influenced by biases present in their training data, leading to unfair judgments in different social, cultural, or legal contexts. These biases could be amplified by the model and compromise the fairness of its decisions. Future research could focus on ensuring fairness in model outputs through debiasing algorithms and fairness constraints [127]. Targeted approaches, such as adversarial debiasing training or bias detection tools, can dynamically identify and mitigate potential biases during the model's reasoning process.",
          "quote": [
            {
              "text": "[127]",
              "target": "#b126",
              "type": "bibr",
              "context": "nstraints ",
              "index": 382
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "8.3.3",
        "name": "Enhancing Robustness."
      },
      "p": [
        {
          "text": "LLMs are sensitive to noise, incompleteness, or ambiguity in input instructions, which may lead to errors or instability in evaluation results when handling complex or highly uncertain texts. This lack of robustness significantly limits their reliability in practical applications. Future research can adopt several methods to enable LLMs robust and reliable performance in real-world environments [58, 196]. For instance, introducing more advanced data augmentation techniques to generate diverse and uncertain simulated cases can help train models to adapt to various complex input conditions.",
          "quote": [
            {
              "text": "[58,",
              "target": "#b57",
              "type": "bibr",
              "context": "ironments ",
              "index": 398
            },
            {
              "text": "196]",
              "target": "#b198",
              "type": "bibr",
              "context": "ents [58, ",
              "index": 403
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "9",
        "name": "CONCLUSION"
      },
      "p": [
        {
          "text": "This survey systematically examined the LLMs-as-Judges framework across five dimensions: functionality, methodology, applications, meta-evaluation, and limitations, providing a comprehensive understanding of its advantages, limitations, practical implementations, applications, and methods for evaluating its effectiveness. To advance research in this field, we also outlined several promising directions for future exploration, including the development of more efficient, effective, and reliable LLM judges. We hope to promote the ongoing development of this field by providing foundational resources and will continue to update relevant content.",
          "quote": []
        }
      ]
    }
  ],
  "chart": [
    "Fig. 3. Overview of the LLMs-as-judges system.",
    "Fig. 4. Overview of the Functionality of LLMs-as-judges.",
    "Fig. 5. Overview of the Methodology of LLMs-as-judges.",
    "Fig. 6. LLMs-as-judges are widely applied across various domains.",
    "Table 1 .An Overview of Fine-Tuning Methods in Single-LLM Evaluation (Sorted in ascending alphabetical order).",
    "Table 2 .Statistical information of different benchmarks (Part 1).",
    "Table 3 .Statistical information of different benchmarks (Part 2).",
    "Table 4 .Summary of Common Metrics for Evaluating LLMs-as-Judges Models",
    "Table 5 .Overview of different biases."
  ],
  "reference": [
    {
      "index": "b0",
      "title": "Shyamal Anadkat, et al. 2023. Gpt-4 technical report",
      "author": [
        {
          "forename": "Josh",
          "surname": "Achiam",
          "name": "Josh Achiam",
          "email": ""
        },
        {
          "forename": "Steven",
          "surname": "Adler",
          "name": "Steven Adler",
          "email": ""
        },
        {
          "forename": "Sandhini",
          "surname": "Agarwal",
          "name": "Sandhini Agarwal",
          "email": ""
        },
        {
          "forename": "Lama",
          "surname": "Ahmad",
          "name": "Lama Ahmad",
          "email": ""
        },
        {
          "forename": "Ilge",
          "surname": "Akkaya",
          "name": "Ilge Akkaya",
          "email": ""
        },
        {
          "forename": "Florencia Leoni ",
          "surname": "Aleman",
          "name": "Florencia Leoni  Aleman",
          "email": ""
        },
        {
          "forename": "Diogo",
          "surname": "Almeida",
          "name": "Diogo Almeida",
          "email": ""
        },
        {
          "forename": "Janko",
          "surname": "Altenschmidt",
          "name": "Janko Altenschmidt",
          "email": ""
        },
        {
          "forename": "Sam",
          "surname": "Altman",
          "name": "Sam Altman",
          "email": ""
        }
      ],
      "doi": "arXiv:2303.08774",
      "venue": "Shyamal Anadkat, et al. 2023. Gpt-4 technical report",
      "date": "2023"
    },
    {
      "index": "b1",
      "title": "Awais Athar, and Agha Ali Raza. 2024. The fellowship of the llms: Multi-agent workflows for synthetic preference optimization dataset generation",
      "author": [
        {
          "forename": "Samee",
          "surname": "Arif",
          "name": "Samee Arif",
          "email": ""
        },
        {
          "forename": "Sualeha",
          "surname": "Farid",
          "name": "Sualeha Farid",
          "email": ""
        }
      ],
      "doi": "arXiv:2408.08688",
      "venue": "Awais Athar, and Agha Ali Raza. 2024. The fellowship of the llms: Multi-agent workflows for synthetic preference optimization dataset generation",
      "date": "2024"
    },
    {
      "index": "b2",
      "title": "Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection",
      "author": [
        {
          "forename": "Akari",
          "surname": "Asai",
          "name": "Akari Asai",
          "email": ""
        },
        {
          "forename": "Zeqiu",
          "surname": "Wu",
          "name": "Zeqiu Wu",
          "email": ""
        },
        {
          "forename": "Yizhong",
          "surname": "Wang",
          "name": "Yizhong Wang",
          "email": ""
        }
      ],
      "doi": "arXiv:2310.11511",
      "venue": "Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection",
      "date": "2023"
    },
    {
      "index": "b3",
      "title": "Yelp dataset challenge: Review rating prediction",
      "author": [
        {
          "forename": "Nabiha",
          "surname": "Asghar",
          "name": "Nabiha Asghar",
          "email": ""
        }
      ],
      "doi": "arXiv:1605.05362",
      "venue": "Yelp dataset challenge: Review rating prediction",
      "date": "2016"
    },
    {
      "index": "b4",
      "title": "Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences",
      "author": [
        {
          "forename": "Zahra",
          "surname": "Ashktorab",
          "name": "Zahra Ashktorab",
          "email": ""
        },
        {
          "forename": "Michael",
          "surname": "Desmond",
          "name": "Michael Desmond",
          "email": ""
        },
        {
          "forename": "Qian",
          "surname": "Pan",
          "name": "Qian Pan",
          "email": ""
        },
        {
          "forename": "M.",
          "surname": "James",
          "name": "M. James",
          "email": ""
        },
        {
          "forename": "Martin Santillan ",
          "surname": "Johnson",
          "name": "Martin Santillan  Johnson",
          "email": ""
        },
        {
          "forename": "Elizabeth M.",
          "surname": "Cooper",
          "name": "Elizabeth M. Cooper",
          "email": ""
        },
        {
          "forename": "Rahul",
          "surname": "Daly",
          "name": "Rahul Daly",
          "email": ""
        },
        {
          "forename": "Tejaswini",
          "surname": "Nair",
          "name": "Tejaswini Nair",
          "email": ""
        },
        {
          "forename": "Swapnaja",
          "surname": "Pedapati",
          "name": "Swapnaja Pedapati",
          "email": ""
        },
        {
          "forename": "Werner",
          "surname": "Achintalwar",
          "name": "Werner Achintalwar",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.00873",
      "venue": "Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences",
      "date": "2024"
    },
    {
      "index": "b5",
      "title": "A general language assistant as a laboratory for alignment. arXiv. Preprint posted online December",
      "author": [
        {
          "forename": "A.",
          "surname": "Askell",
          "name": "A. Askell",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "A general language assistant as a laboratory for alignment. arXiv. Preprint posted online December",
      "date": "2021"
    },
    {
      "index": "b6",
      "title": "GPT classifications, with application to credit lending",
      "author": [
        {
          "forename": "Golnoosh",
          "surname": "Babaei",
          "name": "Golnoosh Babaei",
          "email": ""
        },
        {
          "forename": "Paolo",
          "surname": "Giudici",
          "name": "Paolo Giudici",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Machine Learning with Applications",
      "date": "2024"
    },
    {
      "index": "b7",
      "title": "Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of Free-Form Text",
      "author": [
        {
          "forename": "Sher",
          "surname": "Badshah",
          "name": "Sher Badshah",
          "email": ""
        },
        {
          "forename": "Hassan",
          "surname": "Sajjad",
          "name": "Hassan Sajjad",
          "email": ""
        }
      ],
      "doi": "arXiv:2408.09235",
      "venue": "Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of Free-Form Text",
      "date": "2024"
    },
    {
      "index": "b9",
      "title": "Benchmarking foundation models with language-model-as-an-examiner",
      "author": [
        {
          "forename": "Yushi",
          "surname": "Bai",
          "name": "Yushi Bai",
          "email": ""
        },
        {
          "forename": "Jiahao",
          "surname": "Ying",
          "name": "Jiahao Ying",
          "email": ""
        },
        {
          "forename": "Yixin",
          "surname": "Cao",
          "name": "Yixin Cao",
          "email": ""
        },
        {
          "forename": "Xin",
          "surname": "Lv",
          "name": "Xin Lv",
          "email": ""
        },
        {
          "forename": "Yuze",
          "surname": "He",
          "name": "Yuze He",
          "email": ""
        },
        {
          "forename": "Xiaozhi",
          "surname": "Wang",
          "name": "Xiaozhi Wang",
          "email": ""
        },
        {
          "forename": "Jifan",
          "surname": "Yu",
          "name": "Jifan Yu",
          "email": ""
        },
        {
          "forename": "Kaisheng",
          "surname": "Zeng",
          "name": "Kaisheng Zeng",
          "email": ""
        },
        {
          "forename": "Yijia",
          "surname": "Xiao",
          "name": "Yijia Xiao",
          "email": ""
        },
        {
          "forename": "Haozhe",
          "surname": "Lyu",
          "name": "Haozhe Lyu",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Advances in Neural Information Processing Systems",
      "date": "2024"
    },
    {
      "index": "b10",
      "title": "Ms marco: A human generated machine reading comprehension dataset",
      "author": [
        {
          "forename": "Payal",
          "surname": "Bajaj",
          "name": "Payal Bajaj",
          "email": ""
        },
        {
          "forename": "Daniel",
          "surname": "Campos",
          "name": "Daniel Campos",
          "email": ""
        },
        {
          "forename": "Nick",
          "surname": "Craswell",
          "name": "Nick Craswell",
          "email": ""
        },
        {
          "forename": "Li",
          "surname": "Deng",
          "name": "Li Deng",
          "email": ""
        },
        {
          "forename": "Jianfeng",
          "surname": "Gao",
          "name": "Jianfeng Gao",
          "email": ""
        },
        {
          "forename": "Xiaodong",
          "surname": "Liu",
          "name": "Xiaodong Liu",
          "email": ""
        },
        {
          "forename": "Rangan",
          "surname": "Majumder",
          "name": "Rangan Majumder",
          "email": ""
        },
        {
          "forename": "Andrew",
          "surname": "Mcnamara",
          "name": "Andrew Mcnamara",
          "email": ""
        },
        {
          "forename": "Bhaskar",
          "surname": "Mitra",
          "name": "Bhaskar Mitra",
          "email": ""
        },
        {
          "forename": "Tri",
          "surname": "Nguyen",
          "name": "Tri Nguyen",
          "email": ""
        }
      ],
      "doi": "arXiv:1611.09268",
      "venue": "Ms marco: A human generated machine reading comprehension dataset",
      "date": "2016"
    },
    {
      "index": "b11",
      "title": "Adversarial Multi-Agent Evaluation of Large Language Models through Iterative Debates",
      "author": [
        {
          "forename": "Chaithanya",
          "surname": "Bandi",
          "name": "Chaithanya Bandi",
          "email": ""
        },
        {
          "forename": "Abir",
          "surname": "Harrasse",
          "name": "Abir Harrasse",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.04663",
      "venue": "Adversarial Multi-Agent Evaluation of Large Language Models through Iterative Debates",
      "date": "2024"
    },
    {
      "index": "b12",
      "title": "The intraclass correlation coefficient as a measure of reliability",
      "author": [
        {
          "forename": "J.",
          "surname": "John",
          "name": "J. John",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Psychological reports",
      "date": "1966"
    },
    {
      "index": "b13",
      "title": "Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks",
      "author": [
        {
          "forename": "Anna",
          "surname": "Bavaresco",
          "name": "Anna Bavaresco",
          "email": ""
        },
        {
          "forename": "Raffaella",
          "surname": "Bernardi",
          "name": "Raffaella Bernardi",
          "email": ""
        },
        {
          "forename": "Leonardo",
          "surname": "Bertolazzi",
          "name": "Leonardo Bertolazzi",
          "email": ""
        },
        {
          "forename": "Desmond",
          "surname": "Elliott",
          "name": "Desmond Elliott",
          "email": ""
        },
        {
          "forename": "Raquel",
          "surname": "Fernández",
          "name": "Raquel Fernández",
          "email": ""
        },
        {
          "forename": "Albert",
          "surname": "Gatt",
          "name": "Albert Gatt",
          "email": ""
        },
        {
          "forename": "Esam",
          "surname": "Ghaleb",
          "name": "Esam Ghaleb",
          "email": ""
        },
        {
          "forename": "Mario",
          "surname": "Giulianelli",
          "name": "Mario Giulianelli",
          "email": ""
        },
        {
          "forename": "Michael",
          "surname": "Hanna",
          "name": "Michael Hanna",
          "email": ""
        },
        {
          "forename": "Alexander",
          "surname": "Koller",
          "name": "Alexander Koller",
          "email": ""
        }
      ],
      "doi": "arXiv:2406.18403",
      "venue": "Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks",
      "date": "2024"
    },
    {
      "index": "b14",
      "title": "Graph of thoughts: Solving elaborate problems with large language models",
      "author": [
        {
          "forename": "Maciej",
          "surname": "Besta",
          "name": "Maciej Besta",
          "email": ""
        },
        {
          "forename": "Nils",
          "surname": "Blach",
          "name": "Nils Blach",
          "email": ""
        },
        {
          "forename": "Ales",
          "surname": "Kubicek",
          "name": "Ales Kubicek",
          "email": ""
        },
        {
          "forename": "Robert",
          "surname": "Gerstenberger",
          "name": "Robert Gerstenberger",
          "email": ""
        },
        {
          "forename": "Michal",
          "surname": "Podstawski",
          "name": "Michal Podstawski",
          "email": ""
        },
        {
          "forename": "Lukas",
          "surname": "Gianinazzi",
          "name": "Lukas Gianinazzi",
          "email": ""
        },
        {
          "forename": "Joanna",
          "surname": "Gajda",
          "name": "Joanna Gajda",
          "email": ""
        },
        {
          "forename": "Tomasz",
          "surname": "Lehmann",
          "name": "Tomasz Lehmann",
          "email": ""
        },
        {
          "forename": "Hubert",
          "surname": "Niewiadomski",
          "name": "Hubert Niewiadomski",
          "email": ""
        },
        {
          "forename": "Piotr",
          "surname": "Nyczyk",
          "name": "Piotr Nyczyk",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "date": "2024"
    },
    {
      "index": "b15",
      "title": "Position bias in multiple-choice questions",
      "author": [
        {
          "forename": "J.",
          "surname": "Niels",
          "name": "J. Niels",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Journal of Marketing Research",
      "date": "1984"
    },
    {
      "index": "b16",
      "title": "Comparing Two Model Designs for Clinical Note Generation",
      "author": [
        {
          "forename": "Nathan",
          "surname": "Brake",
          "name": "Nathan Brake",
          "email": ""
        },
        {
          "forename": "Thomas",
          "surname": "Schaaf",
          "name": "Thomas Schaaf",
          "email": ""
        }
      ],
      "doi": "arXiv:2404.06503",
      "venue": "Is an LLM a Useful Evaluator of Consistency? arXiv preprint",
      "date": "2024"
    },
    {
      "index": "b17",
      "title": "Evaluating the susceptibility of pre-trained language models via handcrafted adversarial examples",
      "author": [
        {
          "forename": "J.",
          "surname": "Hezekiah",
          "name": "J. Hezekiah",
          "email": ""
        },
        {
          "forename": "Jonathan Rodriguez ",
          "surname": "Branch",
          "name": "Jonathan Rodriguez  Branch",
          "email": ""
        },
        {
          "forename": "Jeremy",
          "surname": "Cefalu",
          "name": "Jeremy Cefalu",
          "email": ""
        },
        {
          "forename": "Leyla",
          "surname": "Mchugh",
          "name": "Leyla Mchugh",
          "email": ""
        },
        {
          "forename": "Aditya",
          "surname": "Hujer",
          "name": "Aditya Hujer",
          "email": ""
        },
        {
          "forename": "Daniel",
          "surname": "Bahl",
          "name": "Daniel Bahl",
          "email": ""
        },
        {
          "forename": "Ron",
          "surname": "Del Castillo Iglesias",
          "name": "Ron Del Castillo Iglesias",
          "email": ""
        },
        {
          "forename": "Ramesh",
          "surname": "Heichman",
          "name": "Ramesh Heichman",
          "email": ""
        }
      ],
      "doi": "arXiv:2209.02128",
      "venue": "Evaluating the susceptibility of pre-trained language models via handcrafted adversarial examples",
      "date": "2022"
    },
    {
      "index": "b18",
      "title": "Evaluations of self and others: Self-enhancement biases in social judgments",
      "author": [
        {
          "forename": "D.",
          "surname": "Jonathon",
          "name": "D. Jonathon",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Social cognition",
      "date": "1986"
    },
    {
      "index": "b19",
      "title": "CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution",
      "author": [
        {
          "forename": "Maosong",
          "surname": "Cao",
          "name": "Maosong Cao",
          "email": ""
        },
        {
          "forename": "Alexander",
          "surname": "Lam",
          "name": "Alexander Lam",
          "email": ""
        },
        {
          "forename": "Haodong",
          "surname": "Duan",
          "name": "Haodong Duan",
          "email": ""
        },
        {
          "forename": "Hongwei",
          "surname": "Liu",
          "name": "Hongwei Liu",
          "email": ""
        },
        {
          "forename": "Songyang",
          "surname": "Zhang",
          "name": "Songyang Zhang",
          "email": ""
        },
        {
          "forename": "Kai",
          "surname": "Chen",
          "name": "Kai Chen",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.16256",
      "venue": "CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution",
      "date": "2024"
    },
    {
      "index": "b20",
      "title": "Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic",
      "author": [
        {
          "forename": "Meng",
          "surname": "Cao",
          "name": "Meng Cao",
          "email": ""
        },
        {
          "forename": "Lei",
          "surname": "Shu",
          "name": "Lei Shu",
          "email": ""
        },
        {
          "forename": "Lei",
          "surname": "Yu",
          "name": "Lei Yu",
          "email": ""
        },
        {
          "forename": "Yun",
          "surname": "Zhu",
          "name": "Yun Zhu",
          "email": ""
        },
        {
          "forename": "Nevan",
          "surname": "Wichers",
          "name": "Nevan Wichers",
          "email": ""
        },
        {
          "forename": "Yinxiao",
          "surname": "Liu",
          "name": "Yinxiao Liu",
          "email": ""
        },
        {
          "forename": "Lei",
          "surname": "Meng",
          "name": "Lei Meng",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
      "date": "2024"
    },
    {
      "index": "b21",
      "title": "Chateval: Towards better llm-based evaluators through multi-agent debate",
      "author": [
        {
          "forename": "Chi-Min",
          "surname": "Chan",
          "name": "Chi-Min Chan",
          "email": ""
        },
        {
          "forename": "Weize",
          "surname": "Chen",
          "name": "Weize Chen",
          "email": ""
        },
        {
          "forename": "Yusheng",
          "surname": "Su",
          "name": "Yusheng Su",
          "email": ""
        },
        {
          "forename": "Jianxuan",
          "surname": "Yu",
          "name": "Jianxuan Yu",
          "email": ""
        },
        {
          "forename": "Wei",
          "surname": "Xue",
          "name": "Wei Xue",
          "email": ""
        },
        {
          "forename": "Shanghang",
          "surname": "Zhang",
          "name": "Shanghang Zhang",
          "email": ""
        },
        {
          "forename": "Jie",
          "surname": "Fu",
          "name": "Jie Fu",
          "email": ""
        },
        {
          "forename": "Zhiyuan",
          "surname": "Liu",
          "name": "Zhiyuan Liu",
          "email": ""
        }
      ],
      "doi": "arXiv:2308.07201",
      "venue": "Chateval: Towards better llm-based evaluators through multi-agent debate",
      "date": "2023"
    },
    {
      "index": "b22",
      "title": "A survey on evaluation of large language models",
      "author": [
        {
          "forename": "Yupeng",
          "surname": "Chang",
          "name": "Yupeng Chang",
          "email": ""
        },
        {
          "forename": "Xu",
          "surname": "Wang",
          "name": "Xu Wang",
          "email": ""
        },
        {
          "forename": "Jindong",
          "surname": "Wang",
          "name": "Jindong Wang",
          "email": ""
        },
        {
          "forename": "Yuan",
          "surname": "Wu",
          "name": "Yuan Wu",
          "email": ""
        },
        {
          "forename": "Linyi",
          "surname": "Yang",
          "name": "Linyi Yang",
          "email": ""
        },
        {
          "forename": "Kaijie",
          "surname": "Zhu",
          "name": "Kaijie Zhu",
          "email": ""
        },
        {
          "forename": "Hao",
          "surname": "Chen",
          "name": "Hao Chen",
          "email": ""
        },
        {
          "forename": "Xiaoyuan",
          "surname": "Yi",
          "name": "Xiaoyuan Yi",
          "email": ""
        },
        {
          "forename": "Cunxiang",
          "surname": "Wang",
          "name": "Cunxiang Wang",
          "email": ""
        },
        {
          "forename": "Yidong",
          "surname": "Wang",
          "name": "Yidong Wang",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "ACM Transactions on Intelligent Systems and Technology",
      "date": "2024"
    },
    {
      "index": "b23",
      "title": "Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark",
      "author": [
        {
          "forename": "Dongping",
          "surname": "Chen",
          "name": "Dongping Chen",
          "email": ""
        },
        {
          "forename": "Ruoxi",
          "surname": "Chen",
          "name": "Ruoxi Chen",
          "email": ""
        },
        {
          "forename": "Shilin",
          "surname": "Zhang",
          "name": "Shilin Zhang",
          "email": ""
        },
        {
          "forename": "Yinuo",
          "surname": "Liu",
          "name": "Yinuo Liu",
          "email": ""
        },
        {
          "forename": "Yaochen",
          "surname": "Wang",
          "name": "Yaochen Wang",
          "email": ""
        },
        {
          "forename": "Huichi",
          "surname": "Zhou",
          "name": "Huichi Zhou",
          "email": ""
        },
        {
          "forename": "Qihui",
          "surname": "Zhang",
          "name": "Qihui Zhang",
          "email": ""
        },
        {
          "forename": "Yao",
          "surname": "Wan",
          "name": "Yao Wan",
          "email": ""
        },
        {
          "forename": "Pan",
          "surname": "Zhou",
          "name": "Pan Zhou",
          "email": ""
        },
        {
          "forename": "Lichao",
          "surname": "Sun",
          "name": "Lichao Sun",
          "email": ""
        }
      ],
      "doi": "arXiv:2402.04788",
      "venue": "Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark",
      "date": "2024"
    },
    {
      "index": "b24",
      "title": "Humans or llms as the judge? a study on judgement biases",
      "author": [
        {
          "forename": "Guiming",
          "surname": "Hardy Chen",
          "name": "Guiming Hardy Chen",
          "email": ""
        },
        {
          "forename": "Shunian",
          "surname": "Chen",
          "name": "Shunian Chen",
          "email": ""
        },
        {
          "forename": "Ziche",
          "surname": "Liu",
          "name": "Ziche Liu",
          "email": ""
        },
        {
          "forename": "Feng",
          "surname": "Jiang",
          "name": "Feng Jiang",
          "email": ""
        },
        {
          "forename": "Benyou",
          "surname": "Wang",
          "name": "Benyou Wang",
          "email": ""
        }
      ],
      "doi": "arXiv:2402.10669",
      "venue": "Humans or llms as the judge? a study on judgement biases",
      "date": "2024"
    },
    {
      "index": "b25",
      "title": "StoryER: Automatic story evaluation via ranking, rating and reasoning",
      "author": [
        {
          "forename": "Hong",
          "surname": "Chen",
          "name": "Hong Chen",
          "email": ""
        },
        {
          "forename": "Minh",
          "surname": "Duc",
          "name": "Minh Duc",
          "email": ""
        },
        {
          "forename": "Hiroya",
          "surname": "Vo",
          "name": "Hiroya Vo",
          "email": ""
        },
        {
          "forename": "Yusuke",
          "surname": "Takamura",
          "name": "Yusuke Takamura",
          "email": ""
        },
        {
          "forename": "Hideki",
          "surname": "Miyao",
          "name": "Hideki Miyao",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Journal of Natural Language Processing",
      "date": "2023"
    },
    {
      "index": "b26",
      "title": "An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation",
      "author": [
        {
          "forename": "Junjie",
          "surname": "Chen",
          "name": "Junjie Chen",
          "email": ""
        },
        {
          "forename": "Weihang",
          "surname": "Su",
          "name": "Weihang Su",
          "email": ""
        },
        {
          "forename": "Zhumin",
          "surname": "Chu",
          "name": "Zhumin Chu",
          "email": ""
        },
        {
          "forename": "Haitao",
          "surname": "Li",
          "name": "Haitao Li",
          "email": ""
        },
        {
          "forename": "Qinyao",
          "surname": "Ai",
          "name": "Qinyao Ai",
          "email": ""
        },
        {
          "forename": "Yiqun",
          "surname": "Liu",
          "name": "Yiqun Liu",
          "email": ""
        },
        {
          "forename": "Min",
          "surname": "Zhang",
          "name": "Min Zhang",
          "email": ""
        },
        {
          "forename": "Shaoping",
          "surname": "Ma",
          "name": "Shaoping Ma",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.12265",
      "venue": "An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation",
      "date": "2024"
    },
    {
      "index": "b27",
      "title": "Adaptation with self-evaluation to improve selective prediction in llms",
      "author": [
        {
          "forename": "Jiefeng",
          "surname": "Chen",
          "name": "Jiefeng Chen",
          "email": ""
        },
        {
          "forename": "Jinsung",
          "surname": "Yoon",
          "name": "Jinsung Yoon",
          "email": ""
        },
        {
          "forename": "Sayna",
          "surname": "Ebrahimi",
          "name": "Sayna Ebrahimi",
          "email": ""
        },
        {
          "forename": "O.",
          "surname": "Sercan",
          "name": "O. Sercan",
          "email": ""
        },
        {
          "forename": "Tomas",
          "surname": "Arik",
          "name": "Tomas Arik",
          "email": ""
        },
        {
          "forename": "Somesh",
          "surname": "Pfister",
          "name": "Somesh Pfister",
          "email": ""
        }
      ],
      "doi": "arXiv:2310.11689",
      "venue": "Adaptation with self-evaluation to improve selective prediction in llms",
      "date": "2023"
    },
    {
      "index": "b28",
      "title": "Automated evaluation of large vision-language models on self-driving corner cases",
      "author": [
        {
          "forename": "Kai",
          "surname": "Chen",
          "name": "Kai Chen",
          "email": ""
        },
        {
          "forename": "Yanze",
          "surname": "Li",
          "name": "Yanze Li",
          "email": ""
        },
        {
          "forename": "Wenhua",
          "surname": "Zhang",
          "name": "Wenhua Zhang",
          "email": ""
        },
        {
          "forename": "Yanxin",
          "surname": "Liu",
          "name": "Yanxin Liu",
          "email": ""
        },
        {
          "forename": "Pengxiang",
          "surname": "Li",
          "name": "Pengxiang Li",
          "email": ""
        },
        {
          "forename": "Ruiyuan",
          "surname": "Gao",
          "name": "Ruiyuan Gao",
          "email": ""
        },
        {
          "forename": "Lanqing",
          "surname": "Hong",
          "name": "Lanqing Hong",
          "email": ""
        },
        {
          "forename": "Meng",
          "surname": "Tian",
          "name": "Meng Tian",
          "email": ""
        },
        {
          "forename": "Xinhai",
          "surname": "Zhao",
          "name": "Xinhai Zhao",
          "email": ""
        },
        {
          "forename": "Zhenguo",
          "surname": "Li",
          "name": "Zhenguo Li",
          "email": ""
        }
      ],
      "doi": "arXiv:2404.10595",
      "venue": "Automated evaluation of large vision-language models on self-driving corner cases",
      "date": "2024"
    },
    {
      "index": "b30",
      "title": "Internet of agents: Weaving a web of heterogeneous agents for collaborative intelligence",
      "author": [
        {
          "forename": "Weize",
          "surname": "Chen",
          "name": "Weize Chen",
          "email": ""
        },
        {
          "forename": "Ziming",
          "surname": "You",
          "name": "Ziming You",
          "email": ""
        },
        {
          "forename": "Ran",
          "surname": "Li",
          "name": "Ran Li",
          "email": ""
        },
        {
          "forename": "Yitong",
          "surname": "Guan",
          "name": "Yitong Guan",
          "email": ""
        },
        {
          "forename": "Chen",
          "surname": "Qian",
          "name": "Chen Qian",
          "email": ""
        },
        {
          "forename": "Chenyang",
          "surname": "Zhao",
          "name": "Chenyang Zhao",
          "email": ""
        },
        {
          "forename": "Cheng",
          "surname": "Yang",
          "name": "Cheng Yang",
          "email": ""
        },
        {
          "forename": "Ruobing",
          "surname": "Xie",
          "name": "Ruobing Xie",
          "email": ""
        },
        {
          "forename": "Zhiyuan",
          "surname": "Liu",
          "name": "Zhiyuan Liu",
          "email": ""
        },
        {
          "forename": "Maosong",
          "surname": "Sun",
          "name": "Maosong Sun",
          "email": ""
        }
      ],
      "doi": "arXiv:2407.07061",
      "venue": "Internet of agents: Weaving a web of heterogeneous agents for collaborative intelligence",
      "date": "2024"
    },
    {
      "index": "b31",
      "title": "Teaching large language models to self-debug",
      "author": [
        {
          "forename": "Xinyun",
          "surname": "Chen",
          "name": "Xinyun Chen",
          "email": ""
        },
        {
          "forename": "Maxwell",
          "surname": "Lin",
          "name": "Maxwell Lin",
          "email": ""
        },
        {
          "forename": "Nathanael",
          "surname": "Schärli",
          "name": "Nathanael Schärli",
          "email": ""
        },
        {
          "forename": "Denny",
          "surname": "Zhou",
          "name": "Denny Zhou",
          "email": ""
        }
      ],
      "doi": "arXiv:2304.05128",
      "venue": "Teaching large language models to self-debug",
      "date": "2023"
    },
    {
      "index": "b32",
      "title": "LLMs are Biased Evaluators But Not Biased for Retrieval Augmented Generation",
      "author": [
        {
          "forename": "Yen-Shan",
          "surname": "Chen",
          "name": "Yen-Shan Chen",
          "email": ""
        },
        {
          "forename": "Jing",
          "surname": "Jin",
          "name": "Jing Jin",
          "email": ""
        },
        {
          "forename": "Peng-Ting",
          "surname": "Kuo",
          "name": "Peng-Ting Kuo",
          "email": ""
        },
        {
          "forename": "Chao-Wei",
          "surname": "Huang",
          "name": "Chao-Wei Huang",
          "email": ""
        },
        {
          "forename": "Yun-Nung",
          "surname": "Chen",
          "name": "Yun-Nung Chen",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.20833",
      "venue": "LLMs are Biased Evaluators But Not Biased for Retrieval Augmented Generation",
      "date": "2024"
    },
    {
      "index": "b33",
      "title": "Of human criteria and automatic metrics: A benchmark of the evaluation of story generation",
      "author": [
        {
          "forename": "Cyril",
          "surname": "Chhun",
          "name": "Cyril Chhun",
          "email": ""
        },
        {
          "forename": "Pierre",
          "surname": "Colombo",
          "name": "Pierre Colombo",
          "email": ""
        },
        {
          "forename": "Chloé",
          "surname": "Clavel",
          "name": "Chloé Clavel",
          "email": ""
        },
        {
          "forename": "Fabian M.",
          "surname": "Suchanek",
          "name": "Fabian M. Suchanek",
          "email": ""
        }
      ],
      "doi": "arXiv:2208.11646",
      "venue": "Of human criteria and automatic metrics: A benchmark of the evaluation of story generation",
      "date": "2022"
    },
    {
      "index": "b34",
      "title": "Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course",
      "author": [
        {
          "forename": "Cheng-Han",
          "surname": "Chiang",
          "name": "Cheng-Han Chiang",
          "email": ""
        },
        {
          "forename": "Wei-Chih",
          "surname": "Chen",
          "name": "Wei-Chih Chen",
          "email": ""
        },
        {
          "forename": "Chun-Yi",
          "surname": "Kuan",
          "name": "Chun-Yi Kuan",
          "email": ""
        },
        {
          "forename": "Chienchou",
          "surname": "Yang",
          "name": "Chienchou Yang",
          "email": ""
        },
        {
          "forename": "Hung-Yi",
          "surname": "Lee",
          "name": "Hung-Yi Lee",
          "email": ""
        }
      ],
      "doi": "arXiv:2407.05216",
      "venue": "Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course",
      "date": "2024"
    },
    {
      "index": "b35",
      "title": "A closer look into automatic evaluation using large language models",
      "author": [
        {
          "forename": "Han",
          "surname": "Cheng",
          "name": "Han Cheng",
          "email": ""
        },
        {
          "forename": "Hung-Yi",
          "surname": "Chiang",
          "name": "Hung-Yi Chiang",
          "email": ""
        }
      ],
      "doi": "arXiv:2310.05657",
      "venue": "A closer look into automatic evaluation using large language models",
      "date": "2023"
    },
    {
      "index": "b36",
      "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
      "author": [
        {
          "forename": "Wei-Lin",
          "surname": "Chiang",
          "name": "Wei-Lin Chiang",
          "email": ""
        },
        {
          "forename": "Zhuohan",
          "surname": "Li",
          "name": "Zhuohan Li",
          "email": ""
        },
        {
          "forename": "Zi",
          "surname": "Lin",
          "name": "Zi Lin",
          "email": ""
        },
        {
          "forename": "Ying",
          "surname": "Sheng",
          "name": "Ying Sheng",
          "email": ""
        },
        {
          "forename": "Zhanghao",
          "surname": "Wu",
          "name": "Zhanghao Wu",
          "email": ""
        },
        {
          "forename": "Hao",
          "surname": "Zhang",
          "name": "Hao Zhang",
          "email": ""
        },
        {
          "forename": "Lianmin",
          "surname": "Zheng",
          "name": "Lianmin Zheng",
          "email": ""
        },
        {
          "forename": "Siyuan",
          "surname": "Zhuang",
          "name": "Siyuan Zhuang",
          "email": ""
        },
        {
          "forename": "Yonghao",
          "surname": "Zhuang",
          "name": "Yonghao Zhuang",
          "email": ""
        },
        {
          "forename": "Joseph E.",
          "surname": "Gonzalez",
          "name": "Joseph E. Gonzalez",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
      "date": "2023-04"
    },
    {
      "index": "b37",
      "title": "Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation",
      "author": [
        {
          "forename": "Juhwan",
          "surname": "Choi",
          "name": "Juhwan Choi",
          "email": ""
        },
        {
          "forename": "Jungmin",
          "surname": "Yun",
          "name": "Jungmin Yun",
          "email": ""
        },
        {
          "forename": "Kyohoon",
          "surname": "Jin",
          "name": "Kyohoon Jin",
          "email": ""
        },
        {
          "forename": "Youngbin",
          "surname": "Kim",
          "name": "Youngbin Kim",
          "email": ""
        }
      ],
      "doi": "arXiv:2404.09682",
      "venue": "Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation",
      "date": "2024"
    },
    {
      "index": "b38",
      "title": "Pre: A peer review based large language model evaluator",
      "author": [
        {
          "forename": "Zhumin",
          "surname": "Chu",
          "name": "Zhumin Chu",
          "email": ""
        },
        {
          "forename": "Qingyao",
          "surname": "Ai",
          "name": "Qingyao Ai",
          "email": ""
        },
        {
          "forename": "Yiteng",
          "surname": "Tu",
          "name": "Yiteng Tu",
          "email": ""
        },
        {
          "forename": "Haitao",
          "surname": "Li",
          "name": "Haitao Li",
          "email": ""
        },
        {
          "forename": "Yiqun",
          "surname": "Liu",
          "name": "Yiqun Liu",
          "email": ""
        }
      ],
      "doi": "arXiv:2401.15641",
      "venue": "Pre: A peer review based large language model evaluator",
      "date": "2024"
    },
    {
      "index": "b39",
      "title": "Scaling instruction-finetuned language models",
      "author": [
        {
          "forename": "Le",
          "surname": "Chung",
          "name": "Le Chung",
          "email": ""
        },
        {
          "forename": "Shayne",
          "surname": "Hou",
          "name": "Shayne Hou",
          "email": ""
        },
        {
          "forename": "Barret",
          "surname": "Longpre",
          "name": "Barret Longpre",
          "email": ""
        },
        {
          "forename": "Yi",
          "surname": "Zoph",
          "name": "Yi Zoph",
          "email": ""
        },
        {
          "forename": "William",
          "surname": "Tay",
          "name": "William Tay",
          "email": ""
        },
        {
          "forename": "Yunxuan",
          "surname": "Fedus",
          "name": "Yunxuan Fedus",
          "email": ""
        },
        {
          "forename": "Xuezhi",
          "surname": "Li",
          "name": "Xuezhi Li",
          "email": ""
        },
        {
          "forename": "Mostafa",
          "surname": "Wang",
          "name": "Mostafa Wang",
          "email": ""
        },
        {
          "forename": "Siddhartha",
          "surname": "Dehghani",
          "name": "Siddhartha Dehghani",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Journal of Machine Learning Research",
      "date": "2024"
    },
    {
      "index": "b40",
      "title": "Pearson correlation coefficient. Noise reduction in speech processing",
      "author": [
        {
          "forename": "Israel",
          "surname": "Cohen",
          "name": "Israel Cohen",
          "email": ""
        },
        {
          "forename": "Yiteng",
          "surname": "Huang",
          "name": "Yiteng Huang",
          "email": ""
        },
        {
          "forename": "Jingdong",
          "surname": "Chen",
          "name": "Jingdong Chen",
          "email": ""
        },
        {
          "forename": "Jacob",
          "surname": "Benesty",
          "name": "Jacob Benesty",
          "email": ""
        },
        {
          "forename": "Jacob",
          "surname": "Benesty",
          "name": "Jacob Benesty",
          "email": ""
        },
        {
          "forename": "Jingdong",
          "surname": "Chen",
          "name": "Jingdong Chen",
          "email": ""
        },
        {
          "forename": "Yiteng",
          "surname": "Huang",
          "name": "Yiteng Huang",
          "email": ""
        },
        {
          "forename": "Israel",
          "surname": "Cohen",
          "name": "Israel Cohen",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Pearson correlation coefficient. Noise reduction in speech processing",
      "date": "2009"
    },
    {
      "index": "b41",
      "title": "Overview of the TREC 2021 Deep Learning Track",
      "author": [
        {
          "forename": "Nick",
          "surname": "Craswell",
          "name": "Nick Craswell",
          "email": ""
        },
        {
          "forename": "Bhaskar",
          "surname": "Mitra",
          "name": "Bhaskar Mitra",
          "email": ""
        },
        {
          "forename": "Emine",
          "surname": "Yilmaz",
          "name": "Emine Yilmaz",
          "email": ""
        },
        {
          "forename": "Daniel",
          "surname": "Campos",
          "name": "Daniel Campos",
          "email": ""
        },
        {
          "forename": "Jimmy",
          "surname": "Lin",
          "name": "Jimmy Lin",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "TREC",
      "date": "2021"
    },
    {
      "index": "b42",
      "title": "Overview of the TREC 2022 Deep Learning Track",
      "author": [
        {
          "forename": "Nick",
          "surname": "Craswell",
          "name": "Nick Craswell",
          "email": ""
        },
        {
          "forename": "Bhaskar",
          "surname": "Mitra",
          "name": "Bhaskar Mitra",
          "email": ""
        },
        {
          "forename": "Emine",
          "surname": "Yilmaz",
          "name": "Emine Yilmaz",
          "email": ""
        },
        {
          "forename": "Daniel",
          "surname": "Campos",
          "name": "Daniel Campos",
          "email": ""
        },
        {
          "forename": "Jimmy",
          "surname": "Lin",
          "name": "Jimmy Lin",
          "email": ""
        },
        {
          "forename": "Ellen M.",
          "surname": "Voorhees",
          "name": "Ellen M. Voorhees",
          "email": ""
        },
        {
          "forename": "Ian",
          "surname": "Soboroff",
          "name": "Ian Soboroff",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "TREC",
      "date": "2022"
    },
    {
      "index": "b43",
      "title": "ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback",
      "author": [
        {
          "forename": "Ganqu",
          "surname": "Cui",
          "name": "Ganqu Cui",
          "email": ""
        },
        {
          "forename": "Lifan",
          "surname": "Yuan",
          "name": "Lifan Yuan",
          "email": ""
        },
        {
          "forename": "Ning",
          "surname": "Ding",
          "name": "Ning Ding",
          "email": ""
        },
        {
          "forename": "Guanming",
          "surname": "Yao",
          "name": "Guanming Yao",
          "email": ""
        },
        {
          "forename": "Bingxiang",
          "surname": "He",
          "name": "Bingxiang He",
          "email": ""
        },
        {
          "forename": "Wei",
          "surname": "Zhu",
          "name": "Wei Zhu",
          "email": ""
        },
        {
          "forename": "Yuan",
          "surname": "Ni",
          "name": "Yuan Ni",
          "email": ""
        },
        {
          "forename": "Guotong",
          "surname": "Xie",
          "name": "Guotong Xie",
          "email": ""
        },
        {
          "forename": "Ruobing",
          "surname": "Xie",
          "name": "Ruobing Xie",
          "email": ""
        },
        {
          "forename": "Yankai",
          "surname": "Lin",
          "name": "Yankai Lin",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Forty-first International Conference on Machine Learning",
      "date": "2024"
    },
    {
      "index": "b44",
      "title": "Aligning Model Evaluations with Human Preferences: Mitigating Token Count Bias in Language Model Assessments",
      "author": [
        {
          "forename": "Roland",
          "surname": "Daynauth",
          "name": "Roland Daynauth",
          "email": ""
        },
        {
          "forename": "Jason",
          "surname": "Mars",
          "name": "Jason Mars",
          "email": ""
        }
      ],
      "doi": "arXiv:2407.12847",
      "venue": "Aligning Model Evaluations with Human Preferences: Mitigating Token Count Bias in Language Model Assessments",
      "date": "2024"
    },
    {
      "index": "b45",
      "title": "Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach",
      "author": [
        {
          "forename": "Shijian",
          "surname": "Deng",
          "name": "Shijian Deng",
          "email": ""
        },
        {
          "forename": "Wentian",
          "surname": "Zhao",
          "name": "Wentian Zhao",
          "email": ""
        },
        {
          "forename": "Yu-Jhe",
          "surname": "Li",
          "name": "Yu-Jhe Li",
          "email": ""
        },
        {
          "forename": "Kun",
          "surname": "Wan",
          "name": "Kun Wan",
          "email": ""
        },
        {
          "forename": "Daniel",
          "surname": "Miranda",
          "name": "Daniel Miranda",
          "email": ""
        },
        {
          "forename": "Ajinkya",
          "surname": "Kale",
          "name": "Ajinkya Kale",
          "email": ""
        },
        {
          "forename": "Yapeng",
          "surname": "Tian",
          "name": "Yapeng Tian",
          "email": ""
        }
      ],
      "doi": "arXiv:2411.17760",
      "venue": "Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach",
      "date": "2024"
    },
    {
      "index": "b46",
      "title": "PHUDGE: Phi-3 as Scalable Judge",
      "author": [
        {
          "forename": "Mahesh",
          "surname": "Deshwal",
          "name": "Mahesh Deshwal",
          "email": ""
        },
        {
          "forename": "Apoorva",
          "surname": "Chawla",
          "name": "Apoorva Chawla",
          "email": ""
        }
      ],
      "doi": "arXiv:2405.08029",
      "venue": "PHUDGE: Phi-3 as Scalable Judge",
      "date": "2024"
    },
    {
      "index": "b47",
      "title": "Striking the balance in using LLMs for fact-checking: A narrative literature review",
      "author": [
        {
          "forename": "Laurence",
          "surname": "Dierickx",
          "name": "Laurence Dierickx",
          "email": ""
        },
        {
          "forename": "Arjen",
          "surname": "Van Dalen",
          "name": "Arjen Van Dalen",
          "email": ""
        },
        {
          "forename": "Andreas L.",
          "surname": "Opdahl",
          "name": "Andreas L. Opdahl",
          "email": ""
        },
        {
          "forename": "Carl-Gustav",
          "surname": "Lindén",
          "name": "Carl-Gustav Lindén",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Multidisciplinary International Symposium on Disinformation in Open Online Media",
      "date": "2024"
    },
    {
      "index": "b48",
      "title": "Enhancing chat language models by scaling high-quality instructional conversations",
      "author": [
        {
          "forename": "Ning",
          "surname": "Ding",
          "name": "Ning Ding",
          "email": ""
        },
        {
          "forename": "Yulin",
          "surname": "Chen",
          "name": "Yulin Chen",
          "email": ""
        },
        {
          "forename": "Bokai",
          "surname": "Xu",
          "name": "Bokai Xu",
          "email": ""
        },
        {
          "forename": "Yujia",
          "surname": "Qin",
          "name": "Yujia Qin",
          "email": ""
        },
        {
          "forename": "Zhi",
          "surname": "Zheng",
          "name": "Zhi Zheng",
          "email": ""
        },
        {
          "forename": "Shengding",
          "surname": "Hu",
          "name": "Shengding Hu",
          "email": ""
        },
        {
          "forename": "Zhiyuan",
          "surname": "Liu",
          "name": "Zhiyuan Liu",
          "email": ""
        },
        {
          "forename": "Maosong",
          "surname": "Sun",
          "name": "Maosong Sun",
          "email": ""
        },
        {
          "forename": "Bowen",
          "surname": "Zhou",
          "name": "Bowen Zhou",
          "email": ""
        }
      ],
      "doi": "arXiv:2305.14233",
      "venue": "Enhancing chat language models by scaling high-quality instructional conversations",
      "date": "2023"
    },
    {
      "index": "b49",
      "title": "Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion",
      "author": [
        {
          "forename": "Yangruibo",
          "surname": "Ding",
          "name": "Yangruibo Ding",
          "email": ""
        },
        {
          "forename": "Zijian",
          "surname": "Wang",
          "name": "Zijian Wang",
          "email": ""
        },
        {
          "forename": "Wasi",
          "surname": "Ahmad",
          "name": "Wasi Ahmad",
          "email": ""
        },
        {
          "forename": "Hantian",
          "surname": "Ding",
          "name": "Hantian Ding",
          "email": ""
        },
        {
          "forename": "Ming",
          "surname": "Tan",
          "name": "Ming Tan",
          "email": ""
        },
        {
          "forename": "Nihal",
          "surname": "Jain",
          "name": "Nihal Jain",
          "email": ""
        },
        {
          "forename": "Ramesh",
          "surname": "Murali Krishna Ramanathan",
          "name": "Ramesh Murali Krishna Ramanathan",
          "email": ""
        },
        {
          "forename": "Parminder",
          "surname": "Nallapati",
          "name": "Parminder Nallapati",
          "email": ""
        },
        {
          "forename": "Dan",
          "surname": "Bhatia",
          "name": "Dan Bhatia",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Advances in Neural Information Processing Systems",
      "date": "2024"
    },
    {
      "index": "b50",
      "title": "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists",
      "author": [
        {
          "forename": "Sumanth",
          "surname": "Doddapaneni",
          "name": "Sumanth Doddapaneni",
          "email": ""
        },
        {
          "forename": "Mohammed",
          "surname": "Safi Ur Rahman Khan",
          "name": "Mohammed Safi Ur Rahman Khan",
          "email": ""
        },
        {
          "forename": "Sshubam",
          "surname": "Verma",
          "name": "Sshubam Verma",
          "email": ""
        },
        {
          "forename": "Mitesh M",
          "surname": "Khapra",
          "name": "Mitesh M Khapra",
          "email": ""
        }
      ],
      "doi": "arXiv:2406.13439",
      "venue": "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists",
      "date": "2024"
    },
    {
      "index": "b51",
      "title": "Self-Boosting Large Language Models with Synthetic Preference Data",
      "author": [
        {
          "forename": "Qingxiu",
          "surname": "Dong",
          "name": "Qingxiu Dong",
          "email": ""
        },
        {
          "forename": "Li",
          "surname": "Dong",
          "name": "Li Dong",
          "email": ""
        },
        {
          "forename": "Xingxing",
          "surname": "Zhang",
          "name": "Xingxing Zhang",
          "email": ""
        },
        {
          "forename": "Zhifang",
          "surname": "Sui",
          "name": "Zhifang Sui",
          "email": ""
        },
        {
          "forename": "Furu",
          "surname": "Wei",
          "name": "Furu Wei",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.06961",
      "venue": "Self-Boosting Large Language Models with Synthetic Preference Data",
      "date": "2024"
    },
    {
      "index": "b53",
      "title": "Can LLM be a Personalized Judge",
      "author": [
        {
          "forename": "Yijiang River",
          "surname": "Dong",
          "name": "Yijiang River Dong",
          "email": ""
        },
        {
          "forename": "Tiancheng",
          "surname": "Hu",
          "name": "Tiancheng Hu",
          "email": ""
        },
        {
          "forename": "Nigel",
          "surname": "Collier",
          "name": "Nigel Collier",
          "email": ""
        }
      ],
      "doi": "arXiv:2406.11657",
      "venue": "Can LLM be a Personalized Judge",
      "date": "2024"
    },
    {
      "index": "b54",
      "title": "Limits to scalable evaluation at the frontier: LLM as Judge won't beat twice the data",
      "author": [
        {
          "forename": "Florian E.",
          "surname": "Dorner",
          "name": "Florian E. Dorner",
          "email": ""
        },
        {
          "forename": "Vivian Y.",
          "surname": "Nastl",
          "name": "Vivian Y. Nastl",
          "email": ""
        },
        {
          "forename": "Moritz",
          "surname": "Hardt",
          "name": "Moritz Hardt",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.13341",
      "venue": "Limits to scalable evaluation at the frontier: LLM as Judge won't beat twice the data",
      "date": "2024"
    },
    {
      "index": "b55",
      "title": "Length-controlled alpacaeval: A simple way to debias automatic evaluators",
      "author": [
        {
          "forename": "Yann",
          "surname": "Dubois",
          "name": "Yann Dubois",
          "email": ""
        },
        {
          "forename": "Balázs",
          "surname": "Galambosi",
          "name": "Balázs Galambosi",
          "email": ""
        },
        {
          "forename": "Percy",
          "surname": "Liang",
          "name": "Percy Liang",
          "email": ""
        },
        {
          "forename": "Tatsunori B",
          "surname": "Hashimoto",
          "name": "Tatsunori B Hashimoto",
          "email": ""
        }
      ],
      "doi": "arXiv:2404.04475",
      "venue": "Length-controlled alpacaeval: A simple way to debias automatic evaluators",
      "date": "2024"
    },
    {
      "index": "b56",
      "title": "Hotflip: White-box adversarial examples for text classification",
      "author": [
        {
          "forename": "Javid",
          "surname": "Ebrahimi",
          "name": "Javid Ebrahimi",
          "email": ""
        },
        {
          "forename": "Anyi",
          "surname": "Rao",
          "name": "Anyi Rao",
          "email": ""
        },
        {
          "forename": "Daniel",
          "surname": "Lowd",
          "name": "Daniel Lowd",
          "email": ""
        },
        {
          "forename": "Dejing",
          "surname": "Dou",
          "name": "Dejing Dou",
          "email": ""
        }
      ],
      "doi": "arXiv:1712.06751",
      "venue": "Hotflip: White-box adversarial examples for text classification",
      "date": "2017"
    },
    {
      "index": "b57",
      "title": "Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-ajudge",
      "author": [
        {
          "forename": "Aparna",
          "surname": "Elangovan",
          "name": "Aparna Elangovan",
          "email": ""
        },
        {
          "forename": "Jongwoo",
          "surname": "Ko",
          "name": "Jongwoo Ko",
          "email": ""
        },
        {
          "forename": "Lei",
          "surname": "Xu",
          "name": "Lei Xu",
          "email": ""
        },
        {
          "forename": "Mahsa",
          "surname": "Elyasi",
          "name": "Mahsa Elyasi",
          "email": ""
        },
        {
          "forename": "Ling",
          "surname": "Liu",
          "name": "Ling Liu",
          "email": ""
        },
        {
          "forename": "Sravan",
          "surname": "Bodapati",
          "name": "Sravan Bodapati",
          "email": ""
        },
        {
          "forename": "Dan",
          "surname": "Roth",
          "name": "Dan Roth",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.03775",
      "venue": "Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-ajudge",
      "date": "2024"
    },
    {
      "index": "b58",
      "title": "Summeval: Re-evaluating summarization evaluation",
      "author": [
        {
          "forename": "Wojciech",
          "surname": "Alexander R Fabbri",
          "name": "Wojciech Alexander R Fabbri",
          "email": ""
        },
        {
          "forename": "Bryan",
          "surname": "Kryściński",
          "name": "Bryan Kryściński",
          "email": ""
        },
        {
          "forename": "Caiming",
          "surname": "Mccann",
          "name": "Caiming Mccann",
          "email": ""
        },
        {
          "forename": "Richard",
          "surname": "Xiong",
          "name": "Richard Xiong",
          "email": ""
        },
        {
          "forename": "Dragomir",
          "surname": "Socher",
          "name": "Dragomir Socher",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Transactions of the Association for Computational Linguistics",
      "date": "2021"
    },
    {
      "index": "b59",
      "title": "Hierarchical neural story generation",
      "author": [
        {
          "forename": "Angela",
          "surname": "Fan",
          "name": "Angela Fan",
          "email": ""
        },
        {
          "forename": "Mike",
          "surname": "Lewis",
          "name": "Mike Lewis",
          "email": ""
        },
        {
          "forename": "Yann",
          "surname": "Dauphin",
          "name": "Yann Dauphin",
          "email": ""
        }
      ],
      "doi": "arXiv:1805.04833",
      "venue": "Hierarchical neural story generation",
      "date": "2018"
    },
    {
      "index": "b60",
      "title": "Biasalert: A plug-and-play tool for social bias detection in llms",
      "author": [
        {
          "forename": "Zhiting",
          "surname": "Fan",
          "name": "Zhiting Fan",
          "email": ""
        },
        {
          "forename": "Ruizhe",
          "surname": "Chen",
          "name": "Ruizhe Chen",
          "email": ""
        },
        {
          "forename": "Ruiling",
          "surname": "Xu",
          "name": "Ruiling Xu",
          "email": ""
        },
        {
          "forename": "Zuozhu",
          "surname": "Liu",
          "name": "Zuozhu Liu",
          "email": ""
        }
      ],
      "doi": "arXiv:2407.10241",
      "venue": "Biasalert: A plug-and-play tool for social bias detection in llms",
      "date": "2024"
    },
    {
      "index": "b61",
      "title": "Mitigating label biases for in-context learning",
      "author": [
        {
          "forename": "Yu",
          "surname": "Fei",
          "name": "Yu Fei",
          "email": ""
        },
        {
          "forename": "Yifan",
          "surname": "Hou",
          "name": "Yifan Hou",
          "email": ""
        },
        {
          "forename": "Zeming",
          "surname": "Chen",
          "name": "Zeming Chen",
          "email": ""
        },
        {
          "forename": "Antoine",
          "surname": "Bosselut",
          "name": "Antoine Bosselut",
          "email": ""
        }
      ],
      "doi": "arXiv:2305.19148",
      "venue": "Mitigating label biases for in-context learning",
      "date": "2023"
    },
    {
      "index": "b62",
      "title": "Knowledge solver: Teaching llms to search for domain knowledge from knowledge graphs",
      "author": [
        {
          "forename": "Chao",
          "surname": "Feng",
          "name": "Chao Feng",
          "email": ""
        },
        {
          "forename": "Xinyu",
          "surname": "Zhang",
          "name": "Xinyu Zhang",
          "email": ""
        },
        {
          "forename": "Zichu",
          "surname": "Fei",
          "name": "Zichu Fei",
          "email": ""
        }
      ],
      "doi": "arXiv:2309.03118",
      "venue": "Knowledge solver: Teaching llms to search for domain knowledge from knowledge graphs",
      "date": "2023"
    },
    {
      "index": "b63",
      "title": "Improving llm-based machine translation with systematic self-correction",
      "author": [
        {
          "forename": "Zhaopeng",
          "surname": "Feng",
          "name": "Zhaopeng Feng",
          "email": ""
        },
        {
          "forename": "Yan",
          "surname": "Zhang",
          "name": "Yan Zhang",
          "email": ""
        },
        {
          "forename": "Hao",
          "surname": "Li",
          "name": "Hao Li",
          "email": ""
        },
        {
          "forename": "Wenqiang",
          "surname": "Liu",
          "name": "Wenqiang Liu",
          "email": ""
        },
        {
          "forename": "Jun",
          "surname": "Lang",
          "name": "Jun Lang",
          "email": ""
        },
        {
          "forename": "Yang",
          "surname": "Feng",
          "name": "Yang Feng",
          "email": ""
        },
        {
          "forename": "Jian",
          "surname": "Wu",
          "name": "Jian Wu",
          "email": ""
        },
        {
          "forename": "Zuozhu",
          "surname": "Liu",
          "name": "Zuozhu Liu",
          "email": ""
        }
      ],
      "doi": "arXiv:2402.16379",
      "venue": "Improving llm-based machine translation with systematic self-correction",
      "date": "2024"
    },
    {
      "index": "b64",
      "title": "Qijun Tan, and Wolfgang Macherey. 2021. Experts, errors, and context: A large-scale study of human evaluation for machine translation",
      "author": [
        {
          "forename": "Markus",
          "surname": "Freitag",
          "name": "Markus Freitag",
          "email": ""
        },
        {
          "forename": "George",
          "surname": "Foster",
          "name": "George Foster",
          "email": ""
        },
        {
          "forename": "David",
          "surname": "Grangier",
          "name": "David Grangier",
          "email": ""
        },
        {
          "forename": "Viresh",
          "surname": "Ratnakar",
          "name": "Viresh Ratnakar",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Transactions of the Association for Computational Linguistics",
      "date": "2021"
    },
    {
      "index": "b65",
      "title": "Alon Lavie, and Ondřej Bojar. 2021. Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain",
      "author": [
        {
          "forename": "Markus",
          "surname": "Freitag",
          "name": "Markus Freitag",
          "email": ""
        },
        {
          "forename": "Ricardo",
          "surname": "Rei",
          "name": "Ricardo Rei",
          "email": ""
        },
        {
          "forename": "Nitika",
          "surname": "Mathur",
          "name": "Nitika Mathur",
          "email": ""
        },
        {
          "forename": "Chi-Kiu",
          "surname": "Lo",
          "name": "Chi-Kiu Lo",
          "email": ""
        },
        {
          "forename": "Craig",
          "surname": "Stewart",
          "name": "Craig Stewart",
          "email": ""
        },
        {
          "forename": "George",
          "surname": "Foster",
          "name": "George Foster",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the Sixth Conference on Machine Translation",
      "date": ""
    },
    {
      "index": "b66",
      "title": "The Turing Test: the first 50 years",
      "author": [
        {
          "forename": "M.",
          "surname": "Robert",
          "name": "M. Robert",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Trends in cognitive sciences",
      "date": "2000"
    },
    {
      "index": "b67",
      "title": "Gptscore: Evaluate as you desire",
      "author": [
        {
          "forename": "Jinlan",
          "surname": "Fu",
          "name": "Jinlan Fu",
          "email": ""
        },
        {
          "forename": "See-Kiong",
          "surname": "Ng",
          "name": "See-Kiong Ng",
          "email": ""
        },
        {
          "forename": "Zhengbao",
          "surname": "Jiang",
          "name": "Zhengbao Jiang",
          "email": ""
        },
        {
          "forename": "Pengfei",
          "surname": "Liu",
          "name": "Pengfei Liu",
          "email": ""
        }
      ],
      "doi": "arXiv:2302.04166",
      "venue": "Gptscore: Evaluate as you desire",
      "date": "2023"
    },
    {
      "index": "b68",
      "title": "Retrieval-augmented generation for large language models: A survey",
      "author": [
        {
          "forename": "Yunfan",
          "surname": "Gao",
          "name": "Yunfan Gao",
          "email": ""
        },
        {
          "forename": "Yun",
          "surname": "Xiong",
          "name": "Yun Xiong",
          "email": ""
        },
        {
          "forename": "Xinyu",
          "surname": "Gao",
          "name": "Xinyu Gao",
          "email": ""
        },
        {
          "forename": "Kangxiang",
          "surname": "Jia",
          "name": "Kangxiang Jia",
          "email": ""
        },
        {
          "forename": "Jinliu",
          "surname": "Pan",
          "name": "Jinliu Pan",
          "email": ""
        },
        {
          "forename": "Yuxi",
          "surname": "Bi",
          "name": "Yuxi Bi",
          "email": ""
        },
        {
          "forename": "Yi",
          "surname": "Dai",
          "name": "Yi Dai",
          "email": ""
        },
        {
          "forename": "Jiawei",
          "surname": "Sun",
          "name": "Jiawei Sun",
          "email": ""
        },
        {
          "forename": "Meng",
          "surname": "Wang",
          "name": "Meng Wang",
          "email": ""
        },
        {
          "forename": "Haofen",
          "surname": "Wang",
          "name": "Haofen Wang",
          "email": ""
        }
      ],
      "doi": "arXiv:2312.10997",
      "venue": "Retrieval-augmented generation for large language models: A survey",
      "date": "2023"
    },
    {
      "index": "b69",
      "title": "Bayesian Calibration of Win Rate Estimation with LLM Evaluators",
      "author": [
        {
          "forename": "Yicheng",
          "surname": "Gao",
          "name": "Yicheng Gao",
          "email": ""
        },
        {
          "forename": "Gonghan",
          "surname": "Xu",
          "name": "Gonghan Xu",
          "email": ""
        },
        {
          "forename": "Zhe",
          "surname": "Wang",
          "name": "Zhe Wang",
          "email": ""
        },
        {
          "forename": "Arman",
          "surname": "Cohan",
          "name": "Arman Cohan",
          "email": ""
        }
      ],
      "doi": "arXiv:2411.04424",
      "venue": "Bayesian Calibration of Win Rate Estimation with LLM Evaluators",
      "date": "2024"
    },
    {
      "index": "b70",
      "title": "ChatGPT outperforms crowd workers for text-annotation tasks",
      "author": [
        {
          "forename": "Fabrizio",
          "surname": "Gilardi",
          "name": "Fabrizio Gilardi",
          "email": ""
        },
        {
          "forename": "Meysam",
          "surname": "Alizadeh",
          "name": "Meysam Alizadeh",
          "email": ""
        },
        {
          "forename": "Maël",
          "surname": "Kubli",
          "name": "Maël Kubli",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the National Academy of Sciences",
      "date": "2023"
    },
    {
      "index": "b71",
      "title": "Anu Venkatesh, Raefer Gabriel, and Dilek Hakkani-Tur. 2023. Topical-chat: Towards knowledge-grounded open-domain conversations",
      "author": [
        {
          "forename": "Karthik",
          "surname": "Gopalakrishnan",
          "name": "Karthik Gopalakrishnan",
          "email": ""
        },
        {
          "forename": "Behnam",
          "surname": "Hedayatnia",
          "name": "Behnam Hedayatnia",
          "email": ""
        },
        {
          "forename": "Qinlang",
          "surname": "Chen",
          "name": "Qinlang Chen",
          "email": ""
        },
        {
          "forename": "Anna",
          "surname": "Gottardi",
          "name": "Anna Gottardi",
          "email": ""
        },
        {
          "forename": "Sanjeev",
          "surname": "Kwatra",
          "name": "Sanjeev Kwatra",
          "email": ""
        }
      ],
      "doi": "arXiv:2308.11995",
      "venue": "Anu Venkatesh, Raefer Gabriel, and Dilek Hakkani-Tur. 2023. Topical-chat: Towards knowledge-grounded open-domain conversations",
      "date": "2023"
    },
    {
      "index": "b72",
      "title": "OpenMEVA: A benchmark for evaluating open-ended story generation metrics",
      "author": [
        {
          "forename": "Jian",
          "surname": "Guan",
          "name": "Jian Guan",
          "email": ""
        },
        {
          "forename": "Zhexin",
          "surname": "Zhang",
          "name": "Zhexin Zhang",
          "email": ""
        },
        {
          "forename": "Zhuoer",
          "surname": "Feng",
          "name": "Zhuoer Feng",
          "email": ""
        },
        {
          "forename": "Zitao",
          "surname": "Liu",
          "name": "Zitao Liu",
          "email": ""
        },
        {
          "forename": "Wenbiao",
          "surname": "Ding",
          "name": "Wenbiao Ding",
          "email": ""
        },
        {
          "forename": "Xiaoxi",
          "surname": "Mao",
          "name": "Xiaoxi Mao",
          "email": ""
        },
        {
          "forename": "Changjie",
          "surname": "Fan",
          "name": "Changjie Fan",
          "email": ""
        },
        {
          "forename": "Minlie",
          "surname": "Huang",
          "name": "Minlie Huang",
          "email": ""
        }
      ],
      "doi": "arXiv:2105.08920",
      "venue": "OpenMEVA: A benchmark for evaluating open-ended story generation metrics",
      "date": "2021"
    },
    {
      "index": "b73",
      "title": "Direct language model alignment from online ai feedback",
      "author": [
        {
          "forename": "Shangmin",
          "surname": "Guo",
          "name": "Shangmin Guo",
          "email": ""
        },
        {
          "forename": "Biao",
          "surname": "Zhang",
          "name": "Biao Zhang",
          "email": ""
        },
        {
          "forename": "Tianlin",
          "surname": "Liu",
          "name": "Tianlin Liu",
          "email": ""
        },
        {
          "forename": "Tianqi",
          "surname": "Liu",
          "name": "Tianqi Liu",
          "email": ""
        },
        {
          "forename": "Misha",
          "surname": "Khalman",
          "name": "Misha Khalman",
          "email": ""
        },
        {
          "forename": "Felipe",
          "surname": "Llinares",
          "name": "Felipe Llinares",
          "email": ""
        },
        {
          "forename": "Alexandre",
          "surname": "Rame",
          "name": "Alexandre Rame",
          "email": ""
        },
        {
          "forename": "Thomas",
          "surname": "Mesnard",
          "name": "Thomas Mesnard",
          "email": ""
        },
        {
          "forename": "Yao",
          "surname": "Zhao",
          "name": "Yao Zhao",
          "email": ""
        },
        {
          "forename": "Bilal",
          "surname": "Piot",
          "name": "Bilal Piot",
          "email": ""
        }
      ],
      "doi": "arXiv:2402.04792",
      "venue": "Direct language model alignment from online ai feedback",
      "date": "2024"
    },
    {
      "index": "b75",
      "title": "Unveiling Context-Aware Criteria in Self",
      "author": [
        {
          "forename": "Taneesh",
          "surname": "Gupta",
          "name": "Taneesh Gupta",
          "email": ""
        },
        {
          "forename": "Shivam",
          "surname": "Shandilya",
          "name": "Shivam Shandilya",
          "email": ""
        },
        {
          "forename": "Xuchao",
          "surname": "Zhang",
          "name": "Xuchao Zhang",
          "email": ""
        },
        {
          "forename": "Supriyo",
          "surname": "Ghosh",
          "name": "Supriyo Ghosh",
          "email": ""
        },
        {
          "forename": "Chetan",
          "surname": "Bansal",
          "name": "Chetan Bansal",
          "email": ""
        },
        {
          "forename": "Huaxiu",
          "surname": "Yao",
          "name": "Huaxiu Yao",
          "email": ""
        },
        {
          "forename": "Saravan",
          "surname": "Rajmohan",
          "name": "Saravan Rajmohan",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.21545",
      "venue": "Unveiling Context-Aware Criteria in Self",
      "date": "2024"
    },
    {
      "index": "b76",
      "title": "Are large language model-based evaluators the solution to scaling up multilingual evaluation?",
      "author": [
        {
          "forename": "Rishav",
          "surname": "Hada",
          "name": "Rishav Hada",
          "email": ""
        },
        {
          "forename": "Varun",
          "surname": "Gumma",
          "name": "Varun Gumma",
          "email": ""
        },
        {
          "forename": "Adrian",
          "surname": "De Wynter",
          "name": "Adrian De Wynter",
          "email": ""
        },
        {
          "forename": "Harshita",
          "surname": "Diddee",
          "name": "Harshita Diddee",
          "email": ""
        },
        {
          "forename": "Mohamed",
          "surname": "Ahmed",
          "name": "Mohamed Ahmed",
          "email": ""
        },
        {
          "forename": "Monojit",
          "surname": "Choudhury",
          "name": "Monojit Choudhury",
          "email": ""
        },
        {
          "forename": "Kalika",
          "surname": "Bali",
          "name": "Kalika Bali",
          "email": ""
        },
        {
          "forename": "Sunayana",
          "surname": "Sitaram",
          "name": "Sunayana Sitaram",
          "email": ""
        }
      ],
      "doi": "arXiv:2309.07462",
      "venue": "Are large language model-based evaluators the solution to scaling up multilingual evaluation?",
      "date": "2023"
    },
    {
      "index": "b77",
      "title": "Prototypical calibration for few-shot learning of language models",
      "author": [
        {
          "forename": "Zhixiong",
          "surname": "Han",
          "name": "Zhixiong Han",
          "email": ""
        },
        {
          "forename": "Yaru",
          "surname": "Hao",
          "name": "Yaru Hao",
          "email": ""
        },
        {
          "forename": "Li",
          "surname": "Dong",
          "name": "Li Dong",
          "email": ""
        },
        {
          "forename": "Yutao",
          "surname": "Sun",
          "name": "Yutao Sun",
          "email": ""
        },
        {
          "forename": "Furu",
          "surname": "Wei",
          "name": "Furu Wei",
          "email": ""
        }
      ],
      "doi": "arXiv:2205.10183",
      "venue": "Prototypical calibration for few-shot learning of language models",
      "date": "2022"
    },
    {
      "index": "b78",
      "title": "Fullanno: A data engine for enhancing image comprehension of mllms",
      "author": [
        {
          "forename": "Jing",
          "surname": "Hao",
          "name": "Jing Hao",
          "email": ""
        },
        {
          "forename": "Yuxiang",
          "surname": "Zhao",
          "name": "Yuxiang Zhao",
          "email": ""
        },
        {
          "forename": "Song",
          "surname": "Chen",
          "name": "Song Chen",
          "email": ""
        },
        {
          "forename": "Yanpeng",
          "surname": "Sun",
          "name": "Yanpeng Sun",
          "email": ""
        },
        {
          "forename": "Qiang",
          "surname": "Chen",
          "name": "Qiang Chen",
          "email": ""
        },
        {
          "forename": "Gang",
          "surname": "Zhang",
          "name": "Gang Zhang",
          "email": ""
        },
        {
          "forename": "Kun",
          "surname": "Yao",
          "name": "Kun Yao",
          "email": ""
        },
        {
          "forename": "Errui",
          "surname": "Ding",
          "name": "Errui Ding",
          "email": ""
        },
        {
          "forename": "Jingdong",
          "surname": "Wang",
          "name": "Jingdong Wang",
          "email": ""
        }
      ],
      "doi": "arXiv:2409.13540",
      "venue": "Fullanno: A data engine for enhancing image comprehension of mllms",
      "date": "2024"
    },
    {
      "index": "b79",
      "title": "The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis)",
      "author": [
        {
          "forename": "Maxwell",
          "surname": "Harper",
          "name": "Maxwell Harper",
          "email": ""
        },
        {
          "forename": "Joseph A.",
          "surname": "Konstan",
          "name": "Joseph A. Konstan",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis)",
      "date": "2015"
    },
    {
      "index": "b80",
      "title": "ALLURE: auditing and improving llm-based evaluation of text using iterative in-context-learning",
      "author": [
        {
          "forename": "Hosein",
          "surname": "Hasanbeig",
          "name": "Hosein Hasanbeig",
          "email": ""
        },
        {
          "forename": "Hiteshi",
          "surname": "Sharma",
          "name": "Hiteshi Sharma",
          "email": ""
        },
        {
          "forename": "Leo",
          "surname": "Betthauser",
          "name": "Leo Betthauser",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "ALLURE: auditing and improving llm-based evaluation of text using iterative in-context-learning",
      "date": "2023"
    },
    {
      "index": "b81",
      "title": "Socreval: Large language models with the socratic method for reference-free reasoning evaluation",
      "author": [
        {
          "forename": "Hangfeng",
          "surname": "He",
          "name": "Hangfeng He",
          "email": ""
        },
        {
          "forename": "Hongming",
          "surname": "Zhang",
          "name": "Hongming Zhang",
          "email": ""
        },
        {
          "forename": "Dan",
          "surname": "Roth",
          "name": "Dan Roth",
          "email": ""
        }
      ],
      "doi": "arXiv:2310.00074",
      "venue": "Socreval: Large language models with the socratic method for reference-free reasoning evaluation",
      "date": "2023"
    },
    {
      "index": "b82",
      "title": "Annollm: Making large language models to be better crowdsourced annotators",
      "author": [
        {
          "forename": "Xingwei",
          "surname": "He",
          "name": "Xingwei He",
          "email": ""
        },
        {
          "forename": "Zhenghao",
          "surname": "Lin",
          "name": "Zhenghao Lin",
          "email": ""
        },
        {
          "forename": "Yeyun",
          "surname": "Gong",
          "name": "Yeyun Gong",
          "email": ""
        },
        {
          "forename": "Alex",
          "surname": "Jin",
          "name": "Alex Jin",
          "email": ""
        },
        {
          "forename": "Hang",
          "surname": "Zhang",
          "name": "Hang Zhang",
          "email": ""
        },
        {
          "forename": "Chen",
          "surname": "Lin",
          "name": "Chen Lin",
          "email": ""
        },
        {
          "forename": "Jian",
          "surname": "Jiao",
          "name": "Jian Jiao",
          "email": ""
        },
        {
          "forename": "Siu Ming ",
          "surname": "Yiu",
          "name": "Siu Ming  Yiu",
          "email": ""
        },
        {
          "forename": "Nan",
          "surname": "Duan",
          "name": "Nan Duan",
          "email": ""
        },
        {
          "forename": "Weizhu",
          "surname": "Chen",
          "name": "Weizhu Chen",
          "email": ""
        }
      ],
      "doi": "arXiv:2303.16854",
      "venue": "Annollm: Making large language models to be better crowdsourced annotators",
      "date": "2023"
    },
    {
      "index": "b83",
      "title": "Lixin Fan, and Qiang Yang. 2024. FedEval-LLM: Federated Evaluation of Large Language Models on Downstream Tasks with Collective Wisdom",
      "author": [
        {
          "forename": "Yuanqin",
          "surname": "He",
          "name": "Yuanqin He",
          "email": ""
        },
        {
          "forename": "Yan",
          "surname": "Kang",
          "name": "Yan Kang",
          "email": ""
        }
      ],
      "doi": "arXiv:2404.12273",
      "venue": "Lixin Fan, and Qiang Yang. 2024. FedEval-LLM: Federated Evaluation of Large Language Models on Downstream Tasks with Collective Wisdom",
      "date": "2024"
    },
    {
      "index": "b84",
      "title": "If in a Crowdsourced Data Annotation Pipeline, a GPT-4",
      "author": [
        {
          "forename": "Zeyu",
          "surname": "He",
          "name": "Zeyu He",
          "email": ""
        },
        {
          "forename": "Chieh-Yang",
          "surname": "Huang",
          "name": "Chieh-Yang Huang",
          "email": ""
        },
        {
          "forename": "Chien-Kuang Cornelia",
          "surname": "Ding",
          "name": "Chien-Kuang Cornelia Ding",
          "email": ""
        },
        {
          "forename": "Shaurya",
          "surname": "Rohatgi",
          "name": "Shaurya Rohatgi",
          "email": ""
        },
        {
          "forename": "Ting-Hao Kenneth",
          "surname": "Huang",
          "name": "Ting-Hao Kenneth Huang",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the CHI Conference on Human Factors in Computing Systems",
      "date": "2024"
    },
    {
      "index": "b85",
      "title": "Using Large Language Models to Evaluate Biomedical Query-Focused Summarisation",
      "author": [
        {
          "forename": "Hashem",
          "surname": "Hijazi",
          "name": "Hashem Hijazi",
          "email": ""
        },
        {
          "forename": "Diego",
          "surname": "Molla",
          "name": "Diego Molla",
          "email": ""
        },
        {
          "forename": "Vincent",
          "surname": "Nguyen",
          "name": "Vincent Nguyen",
          "email": ""
        },
        {
          "forename": "Sarvnaz",
          "surname": "Karimi",
          "name": "Sarvnaz Karimi",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the 23rd Workshop on Biomedical Natural Language Processing",
      "date": "2024"
    },
    {
      "index": "b86",
      "title": "Large language models are zero-shot rankers for recommender systems",
      "author": [
        {
          "forename": "Yupeng",
          "surname": "Hou",
          "name": "Yupeng Hou",
          "email": ""
        },
        {
          "forename": "Junjie",
          "surname": "Zhang",
          "name": "Junjie Zhang",
          "email": ""
        },
        {
          "forename": "Zihan",
          "surname": "Lin",
          "name": "Zihan Lin",
          "email": ""
        },
        {
          "forename": "Hongyu",
          "surname": "Lu",
          "name": "Hongyu Lu",
          "email": ""
        },
        {
          "forename": "Ruobing",
          "surname": "Xie",
          "name": "Ruobing Xie",
          "email": ""
        },
        {
          "forename": "Julian",
          "surname": "Mcauley",
          "name": "Julian Mcauley",
          "email": ""
        },
        {
          "forename": "Wayne Xin",
          "surname": "Zhao",
          "name": "Wayne Xin Zhao",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "European Conference on Information Retrieval",
      "date": "2024"
    },
    {
      "index": "b87",
      "title": "Xunjian Yin, and Xiaojun Wan",
      "author": [
        {
          "forename": "Xinyu",
          "surname": "Hu",
          "name": "Xinyu Hu",
          "email": ""
        },
        {
          "forename": "Li",
          "surname": "Lin",
          "name": "Li Lin",
          "email": ""
        },
        {
          "forename": "Mingqi",
          "surname": "Gao",
          "name": "Mingqi Gao",
          "email": ""
        }
      ],
      "doi": "arXiv:2406.18365",
      "venue": "Themis: A reference-free nlg evaluation language model with flexibility and interpretability",
      "date": "2024"
    },
    {
      "index": "b88",
      "title": "Rethinking llm-based preference evaluation",
      "author": [
        {
          "forename": "Zhengyu",
          "surname": "Hu",
          "name": "Zhengyu Hu",
          "email": ""
        },
        {
          "forename": "Linxin",
          "surname": "Song",
          "name": "Linxin Song",
          "email": ""
        },
        {
          "forename": "Jieyu",
          "surname": "Zhang",
          "name": "Jieyu Zhang",
          "email": ""
        },
        {
          "forename": "Zheyuan",
          "surname": "Xiao",
          "name": "Zheyuan Xiao",
          "email": ""
        },
        {
          "forename": "Jingang",
          "surname": "Wang",
          "name": "Jingang Wang",
          "email": ""
        },
        {
          "forename": "Zhenyu",
          "surname": "Chen",
          "name": "Zhenyu Chen",
          "email": ""
        },
        {
          "forename": "Hui",
          "surname": "Xiong",
          "name": "Hui Xiong",
          "email": ""
        }
      ],
      "doi": "arXiv:2407.01085",
      "venue": "Rethinking llm-based preference evaluation",
      "date": "2024"
    },
    {
      "index": "b89",
      "title": "Language Model Preference Evaluation with Multiple Weak Evaluators",
      "author": [
        {
          "forename": "Zhengyu",
          "surname": "Hu",
          "name": "Zhengyu Hu",
          "email": ""
        },
        {
          "forename": "Jieyu",
          "surname": "Zhang",
          "name": "Jieyu Zhang",
          "email": ""
        },
        {
          "forename": "Zhihan",
          "surname": "Xiong",
          "name": "Zhihan Xiong",
          "email": ""
        },
        {
          "forename": "Alexander",
          "surname": "Ratner",
          "name": "Alexander Ratner",
          "email": ""
        },
        {
          "forename": "Hui",
          "surname": "Xiong",
          "name": "Hui Xiong",
          "email": ""
        },
        {
          "forename": "Ranjay",
          "surname": "Krishna",
          "name": "Ranjay Krishna",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.12869",
      "venue": "Language Model Preference Evaluation with Multiple Weak Evaluators",
      "date": "2024"
    },
    {
      "index": "b90",
      "title": "An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers",
      "author": [
        {
          "forename": "Hui",
          "surname": "Huang",
          "name": "Hui Huang",
          "email": ""
        },
        {
          "forename": "Yingqi",
          "surname": "Qu",
          "name": "Yingqi Qu",
          "email": ""
        },
        {
          "forename": "Jing",
          "surname": "Liu",
          "name": "Jing Liu",
          "email": ""
        },
        {
          "forename": "Muyun",
          "surname": "Yang",
          "name": "Muyun Yang",
          "email": ""
        },
        {
          "forename": "Tiejun",
          "surname": "Zhao",
          "name": "Tiejun Zhao",
          "email": ""
        }
      ],
      "doi": "arXiv:2403.02839",
      "venue": "An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers",
      "date": "2024"
    },
    {
      "index": "b91",
      "title": "On the limitations of fine-tuned judge models for llm evaluation",
      "author": [
        {
          "forename": "Hui",
          "surname": "Huang",
          "name": "Hui Huang",
          "email": ""
        },
        {
          "forename": "Yingqi",
          "surname": "Qu",
          "name": "Yingqi Qu",
          "email": ""
        },
        {
          "forename": "Hongli",
          "surname": "Zhou",
          "name": "Hongli Zhou",
          "email": ""
        },
        {
          "forename": "Jing",
          "surname": "Liu",
          "name": "Jing Liu",
          "email": ""
        },
        {
          "forename": "Muyun",
          "surname": "Yang",
          "name": "Muyun Yang",
          "email": ""
        },
        {
          "forename": "Bing",
          "surname": "Xu",
          "name": "Bing Xu",
          "email": ""
        },
        {
          "forename": "Tiejun",
          "surname": "Zhao",
          "name": "Tiejun Zhao",
          "email": ""
        }
      ],
      "doi": "arXiv:2403.02839",
      "venue": "On the limitations of fine-tuned judge models for llm evaluation",
      "date": "2024"
    },
    {
      "index": "b92",
      "title": "Large language models cannot self-correct reasoning yet",
      "author": [
        {
          "forename": "Jie",
          "surname": "Huang",
          "name": "Jie Huang",
          "email": ""
        },
        {
          "forename": "Xinyun",
          "surname": "Chen",
          "name": "Xinyun Chen",
          "email": ""
        },
        {
          "forename": "Swaroop",
          "surname": "Mishra",
          "name": "Swaroop Mishra",
          "email": ""
        },
        {
          "forename": "Huaixiu",
          "surname": "Steven Zheng",
          "name": "Huaixiu Steven Zheng",
          "email": ""
        },
        {
          "forename": "Adams Wei ",
          "surname": "Yu",
          "name": "Adams Wei  Yu",
          "email": ""
        },
        {
          "forename": "Xinying",
          "surname": "Song",
          "name": "Xinying Song",
          "email": ""
        },
        {
          "forename": "Denny",
          "surname": "Zhou",
          "name": "Denny Zhou",
          "email": ""
        }
      ],
      "doi": "arXiv:2310.01798",
      "venue": "Large language models cannot self-correct reasoning yet",
      "date": "2023"
    },
    {
      "index": "b93",
      "title": "Multi-dimensional evaluation of text summarization with in-context learning",
      "author": [
        {
          "forename": "Sameer",
          "surname": "Jain",
          "name": "Sameer Jain",
          "email": ""
        },
        {
          "forename": "Vaishakh",
          "surname": "Keshava",
          "name": "Vaishakh Keshava",
          "email": ""
        },
        {
          "forename": "Patrick",
          "surname": "Sathyendra",
          "name": "Patrick Sathyendra",
          "email": ""
        },
        {
          "forename": "Pengfei",
          "surname": "Fernandes",
          "name": "Pengfei Fernandes",
          "email": ""
        },
        {
          "forename": "Graham",
          "surname": "Liu",
          "name": "Graham Liu",
          "email": ""
        },
        {
          "forename": "Chunting",
          "surname": "Neubig",
          "name": "Chunting Neubig",
          "email": ""
        }
      ],
      "doi": "arXiv:2306.01200",
      "venue": "Multi-dimensional evaluation of text summarization with in-context learning",
      "date": "2023"
    },
    {
      "index": "b94",
      "title": "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models",
      "author": [
        {
          "forename": "Minbyul",
          "surname": "Jeong",
          "name": "Minbyul Jeong",
          "email": ""
        },
        {
          "forename": "Jiwoong",
          "surname": "Sohn",
          "name": "Jiwoong Sohn",
          "email": ""
        },
        {
          "forename": "Mujeen",
          "surname": "Sung",
          "name": "Mujeen Sung",
          "email": ""
        },
        {
          "forename": "Jaewoo",
          "surname": "Kang",
          "name": "Jaewoo Kang",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Bioinformatics",
      "date": "2024"
    },
    {
      "index": "b95",
      "title": "PKU-SafeRLHF: Towards Multi-Level Safety Alignment for LLMs with Human Preference",
      "author": [
        {
          "forename": "Jiaming",
          "surname": "Ji",
          "name": "Jiaming Ji",
          "email": ""
        },
        {
          "forename": "Donghai",
          "surname": "Hong",
          "name": "Donghai Hong",
          "email": ""
        },
        {
          "forename": "Borong",
          "surname": "Zhang",
          "name": "Borong Zhang",
          "email": ""
        },
        {
          "forename": "Boyuan",
          "surname": "Chen",
          "name": "Boyuan Chen",
          "email": ""
        },
        {
          "forename": "Josef",
          "surname": "Dai",
          "name": "Josef Dai",
          "email": ""
        },
        {
          "forename": "Boren",
          "surname": "Zheng",
          "name": "Boren Zheng",
          "email": ""
        },
        {
          "forename": "Tianyi",
          "surname": "Qiu",
          "name": "Tianyi Qiu",
          "email": ""
        },
        {
          "forename": "Boxun",
          "surname": "Li",
          "name": "Boxun Li",
          "email": ""
        },
        {
          "forename": "Yaodong",
          "surname": "Yang",
          "name": "Yaodong Yang",
          "email": ""
        }
      ],
      "doi": "arXiv:2406.15513",
      "venue": "PKU-SafeRLHF: Towards Multi-Level Safety Alignment for LLMs with Human Preference",
      "date": "2024"
    },
    {
      "index": "b96",
      "title": "Survey of hallucination in natural language generation",
      "author": [
        {
          "forename": "Ziwei",
          "surname": "Ji",
          "name": "Ziwei Ji",
          "email": ""
        },
        {
          "forename": "Nayeon",
          "surname": "Lee",
          "name": "Nayeon Lee",
          "email": ""
        },
        {
          "forename": "Rita",
          "surname": "Frieske",
          "name": "Rita Frieske",
          "email": ""
        },
        {
          "forename": "Tiezheng",
          "surname": "Yu",
          "name": "Tiezheng Yu",
          "email": ""
        },
        {
          "forename": "Dan",
          "surname": "Su",
          "name": "Dan Su",
          "email": ""
        },
        {
          "forename": "Yan",
          "surname": "Xu",
          "name": "Yan Xu",
          "email": ""
        },
        {
          "forename": "Etsuko",
          "surname": "Ishii",
          "name": "Etsuko Ishii",
          "email": ""
        },
        {
          "forename": "Ye Jin ",
          "surname": "Bang",
          "name": "Ye Jin  Bang",
          "email": ""
        },
        {
          "forename": "Andrea",
          "surname": "Madotto",
          "name": "Andrea Madotto",
          "email": ""
        },
        {
          "forename": "Pascale",
          "surname": "Fung",
          "name": "Pascale Fung",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Comput. Surveys",
      "date": "2023"
    },
    {
      "index": "b97",
      "title": "A Peek into Token Bias: Large Language Models Are",
      "author": [
        {
          "forename": "Bowen",
          "surname": "Jiang",
          "name": "Bowen Jiang",
          "email": ""
        },
        {
          "forename": "Yangxinyu",
          "surname": "Xie",
          "name": "Yangxinyu Xie",
          "email": ""
        },
        {
          "forename": "Zhuoqun",
          "surname": "Hao",
          "name": "Zhuoqun Hao",
          "email": ""
        },
        {
          "forename": "Xiaomeng",
          "surname": "Wang",
          "name": "Xiaomeng Wang",
          "email": ""
        },
        {
          "forename": "Tanwi",
          "surname": "Mallick",
          "name": "Tanwi Mallick",
          "email": ""
        },
        {
          "forename": "J.",
          "surname": "Weijie",
          "name": "J. Weijie",
          "email": ""
        },
        {
          "forename": "J.",
          "surname": "Camillo",
          "name": "J. Camillo",
          "email": ""
        },
        {
          "forename": "Dan",
          "surname": "Taylor",
          "name": "Dan Taylor",
          "email": ""
        }
      ],
      "doi": "arXiv:2406.11050",
      "venue": "A Peek into Token Bias: Large Language Models Are",
      "date": "2024"
    },
    {
      "index": "b98",
      "title": "Tigerscore: Towards building explainable metric for all text generation tasks",
      "author": [
        {
          "forename": "Dongfu",
          "surname": "Jiang",
          "name": "Dongfu Jiang",
          "email": ""
        },
        {
          "forename": "Yishan",
          "surname": "Li",
          "name": "Yishan Li",
          "email": ""
        },
        {
          "forename": "Ge",
          "surname": "Zhang",
          "name": "Ge Zhang",
          "email": ""
        },
        {
          "forename": "Wenhao",
          "surname": "Huang",
          "name": "Wenhao Huang",
          "email": ""
        },
        {
          "forename": "Wenhu",
          "surname": "Bill Yuchen Lin",
          "name": "Wenhu Bill Yuchen Lin",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Transactions on Machine Learning Research",
      "date": "2023"
    },
    {
      "index": "b99",
      "title": "Prompt packer: Deceiving llms through compositional instruction with hidden attacks",
      "author": [
        {
          "forename": "Shuyu",
          "surname": "Jiang",
          "name": "Shuyu Jiang",
          "email": ""
        },
        {
          "forename": "Xingshu",
          "surname": "Chen",
          "name": "Xingshu Chen",
          "email": ""
        },
        {
          "forename": "Rui",
          "surname": "Tang",
          "name": "Rui Tang",
          "email": ""
        }
      ],
      "doi": "arXiv:2310.10077",
      "venue": "Prompt packer: Deceiving llms through compositional instruction with hidden attacks",
      "date": "2023"
    },
    {
      "index": "b100",
      "title": "Swe-bench: Can language models resolve real-world github issues?",
      "author": [
        {
          "forename": "E.",
          "surname": "Carlos",
          "name": "E. Carlos",
          "email": ""
        },
        {
          "forename": "John",
          "surname": "Jimenez",
          "name": "John Jimenez",
          "email": ""
        },
        {
          "forename": "Alexander",
          "surname": "Yang",
          "name": "Alexander Yang",
          "email": ""
        },
        {
          "forename": "Shunyu",
          "surname": "Wettig",
          "name": "Shunyu Wettig",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Swe-bench: Can language models resolve real-world github issues?",
      "date": "2023"
    },
    {
      "index": "b101",
      "title": "Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment",
      "author": [
        {
          "forename": "Yuu",
          "surname": "Jinnai",
          "name": "Yuu Jinnai",
          "email": ""
        },
        {
          "forename": "Tetsuro",
          "surname": "Morimura",
          "name": "Tetsuro Morimura",
          "email": ""
        },
        {
          "forename": "Kaito",
          "surname": "Ariu",
          "name": "Kaito Ariu",
          "email": ""
        },
        {
          "forename": "Kenshi",
          "surname": "Abe",
          "name": "Kenshi Abe",
          "email": ""
        }
      ],
      "doi": "arXiv:2404.01054",
      "venue": "Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment",
      "date": "2024"
    },
    {
      "index": "b102",
      "title": "Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement",
      "author": [
        {
          "forename": "Jaehun",
          "surname": "Jung",
          "name": "Jaehun Jung",
          "email": ""
        },
        {
          "forename": "Faeze",
          "surname": "Brahman",
          "name": "Faeze Brahman",
          "email": ""
        },
        {
          "forename": "Yejin",
          "surname": "Choi",
          "name": "Yejin Choi",
          "email": ""
        }
      ],
      "doi": "arXiv:2407.18370",
      "venue": "Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement",
      "date": "2024"
    },
    {
      "index": "b103",
      "title": "Large language models effectively leverage document-level context for literary translation, but critical errors persist",
      "author": [
        {
          "forename": "Marzena",
          "surname": "Karpinska",
          "name": "Marzena Karpinska",
          "email": ""
        },
        {
          "forename": "Mohit",
          "surname": "Iyyer",
          "name": "Mohit Iyyer",
          "email": ""
        }
      ],
      "doi": "arXiv:2304.03245",
      "venue": "Large language models effectively leverage document-level context for literary translation, but critical errors persist",
      "date": "2023"
    },
    {
      "index": "b104",
      "title": "Rationale-Aware Answer Verification by Pairwise Self-Evaluation",
      "author": [
        {
          "forename": "Akira",
          "surname": "Kawabata",
          "name": "Akira Kawabata",
          "email": ""
        },
        {
          "forename": "Saku",
          "surname": "Sugawara",
          "name": "Saku Sugawara",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.04838",
      "venue": "Rationale-Aware Answer Verification by Pairwise Self-Evaluation",
      "date": "2024"
    },
    {
      "index": "b105",
      "title": "Critiquellm: Towards an informative critique generation model for evaluation of large language model generation",
      "author": [
        {
          "forename": "Pei",
          "surname": "Ke",
          "name": "Pei Ke",
          "email": ""
        },
        {
          "forename": "Bosi",
          "surname": "Wen",
          "name": "Bosi Wen",
          "email": ""
        },
        {
          "forename": "Andrew",
          "surname": "Feng",
          "name": "Andrew Feng",
          "email": ""
        },
        {
          "forename": "Xiao",
          "surname": "Liu",
          "name": "Xiao Liu",
          "email": ""
        },
        {
          "forename": "Xuanyu",
          "surname": "Lei",
          "name": "Xuanyu Lei",
          "email": ""
        },
        {
          "forename": "Jiale",
          "surname": "Cheng",
          "name": "Jiale Cheng",
          "email": ""
        },
        {
          "forename": "Shengyuan",
          "surname": "Wang",
          "name": "Shengyuan Wang",
          "email": ""
        },
        {
          "forename": "Aohan",
          "surname": "Zeng",
          "name": "Aohan Zeng",
          "email": ""
        },
        {
          "forename": "Yuxiao",
          "surname": "Dong",
          "name": "Yuxiao Dong",
          "email": ""
        },
        {
          "forename": "Hongning",
          "surname": "Wang",
          "name": "Hongning Wang",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics",
      "date": "2024"
    },
    {
      "index": "b106",
      "title": "Debating with more persuasive llms leads to more truthful answers",
      "author": [
        {
          "forename": "Akbir",
          "surname": "Khan",
          "name": "Akbir Khan",
          "email": ""
        },
        {
          "forename": "John",
          "surname": "Hughes",
          "name": "John Hughes",
          "email": ""
        },
        {
          "forename": "Dan",
          "surname": "Valentine",
          "name": "Dan Valentine",
          "email": ""
        },
        {
          "forename": "Laura",
          "surname": "Ruis",
          "name": "Laura Ruis",
          "email": ""
        },
        {
          "forename": "Kshitij",
          "surname": "Sachan",
          "name": "Kshitij Sachan",
          "email": ""
        },
        {
          "forename": "Ansh",
          "surname": "Radhakrishnan",
          "name": "Ansh Radhakrishnan",
          "email": ""
        },
        {
          "forename": "Edward",
          "surname": "Grefenstette",
          "name": "Edward Grefenstette",
          "email": ""
        },
        {
          "forename": "R.",
          "surname": "Samuel",
          "name": "R. Samuel",
          "email": ""
        },
        {
          "forename": "Tim",
          "surname": "Bowman",
          "name": "Tim Bowman",
          "email": ""
        },
        {
          "forename": "Ethan",
          "surname": "Rocktäschel",
          "name": "Ethan Rocktäschel",
          "email": ""
        }
      ],
      "doi": "arXiv:2402.06782",
      "venue": "Debating with more persuasive llms leads to more truthful answers",
      "date": "2024"
    },
    {
      "index": "b107",
      "title": "Aligning Large Language Models with Selfgenerated Preference Data",
      "author": [
        {
          "forename": "Dongyoung",
          "surname": "Kim",
          "name": "Dongyoung Kim",
          "email": ""
        },
        {
          "forename": "Kimin",
          "surname": "Lee",
          "name": "Kimin Lee",
          "email": ""
        },
        {
          "forename": "Jinwoo",
          "surname": "Shin",
          "name": "Jinwoo Shin",
          "email": ""
        },
        {
          "forename": "Jaehyung",
          "surname": "Kim",
          "name": "Jaehyung Kim",
          "email": ""
        }
      ],
      "doi": "arXiv:2406.04412",
      "venue": "Aligning Large Language Models with Selfgenerated Preference Data",
      "date": "2024"
    },
    {
      "index": "b108",
      "title": "Prometheus: Inducing fine-grained evaluation capability in language models",
      "author": [
        {
          "forename": "Seungone",
          "surname": "Kim",
          "name": "Seungone Kim",
          "email": ""
        },
        {
          "forename": "Jamin",
          "surname": "Shin",
          "name": "Jamin Shin",
          "email": ""
        },
        {
          "forename": "Yejin",
          "surname": "Cho",
          "name": "Yejin Cho",
          "email": ""
        },
        {
          "forename": "Joel",
          "surname": "Jang",
          "name": "Joel Jang",
          "email": ""
        },
        {
          "forename": "Shayne",
          "surname": "Longpre",
          "name": "Shayne Longpre",
          "email": ""
        },
        {
          "forename": "Hwaran",
          "surname": "Lee",
          "name": "Hwaran Lee",
          "email": ""
        },
        {
          "forename": "Sangdoo",
          "surname": "Yun",
          "name": "Sangdoo Yun",
          "email": ""
        },
        {
          "forename": "Seongjin",
          "surname": "Shin",
          "name": "Seongjin Shin",
          "email": ""
        },
        {
          "forename": "Sungdong",
          "surname": "Kim",
          "name": "Sungdong Kim",
          "email": ""
        },
        {
          "forename": "James",
          "surname": "Thorne",
          "name": "James Thorne",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "The Twelfth International Conference on Learning Representations",
      "date": "2023"
    },
    {
      "index": "b109",
      "title": "Prometheus 2: An open source language model specialized in evaluating other language models",
      "author": [
        {
          "forename": "Seungone",
          "surname": "Kim",
          "name": "Seungone Kim",
          "email": ""
        },
        {
          "forename": "Juyoung",
          "surname": "Suk",
          "name": "Juyoung Suk",
          "email": ""
        },
        {
          "forename": "Shayne",
          "surname": "Longpre",
          "name": "Shayne Longpre",
          "email": ""
        },
        {
          "forename": "Jamin",
          "surname": "Bill Yuchen Lin",
          "name": "Jamin Bill Yuchen Lin",
          "email": ""
        },
        {
          "forename": "Sean",
          "surname": "Shin",
          "name": "Sean Shin",
          "email": ""
        },
        {
          "forename": "Graham",
          "surname": "Welleck",
          "name": "Graham Welleck",
          "email": ""
        },
        {
          "forename": "Moontae",
          "surname": "Neubig",
          "name": "Moontae Neubig",
          "email": ""
        },
        {
          "forename": "Kyungjae",
          "surname": "Lee",
          "name": "Kyungjae Lee",
          "email": ""
        },
        {
          "forename": "Minjoon",
          "surname": "Lee",
          "name": "Minjoon Lee",
          "email": ""
        }
      ],
      "doi": "arXiv:2405.01535",
      "venue": "Prometheus 2: An open source language model specialized in evaluating other language models",
      "date": "2024"
    },
    {
      "index": "b110",
      "title": "Look at the first sentence: Position bias in question answering",
      "author": [
        {
          "forename": "Miyoung",
          "surname": "Ko",
          "name": "Miyoung Ko",
          "email": ""
        },
        {
          "forename": "Jinhyuk",
          "surname": "Lee",
          "name": "Jinhyuk Lee",
          "email": ""
        },
        {
          "forename": "Hyunjae",
          "surname": "Kim",
          "name": "Hyunjae Kim",
          "email": ""
        },
        {
          "forename": "Gangwoo",
          "surname": "Kim",
          "name": "Gangwoo Kim",
          "email": ""
        },
        {
          "forename": "Jaewoo",
          "surname": "Kang",
          "name": "Jaewoo Kang",
          "email": ""
        }
      ],
      "doi": "arXiv:2004.14602",
      "venue": "Look at the first sentence: Position bias in question answering",
      "date": "2020"
    },
    {
      "index": "b111",
      "title": "Benchmarking cognitive biases in large language models as evaluators",
      "author": [
        {
          "forename": "Ryan",
          "surname": "Koo",
          "name": "Ryan Koo",
          "email": ""
        },
        {
          "forename": "Minhwa",
          "surname": "Lee",
          "name": "Minhwa Lee",
          "email": ""
        },
        {
          "forename": "Vipul",
          "surname": "Raheja",
          "name": "Vipul Raheja",
          "email": ""
        },
        {
          "forename": "Jong Inn ",
          "surname": "Park",
          "name": "Jong Inn  Park",
          "email": ""
        },
        {
          "forename": "Zae Myung ",
          "surname": "Kim",
          "name": "Zae Myung  Kim",
          "email": ""
        },
        {
          "forename": "Dongyeop",
          "surname": "Kang",
          "name": "Dongyeop Kang",
          "email": ""
        }
      ],
      "doi": "arXiv:2309.17012",
      "venue": "Benchmarking cognitive biases in large language models as evaluators",
      "date": "2023"
    },
    {
      "index": "b112",
      "title": "Little giants: Exploring the potential of small llms as evaluation metrics in summarization in the",
      "author": [
        {
          "forename": "Neema",
          "surname": "Kotonya",
          "name": "Neema Kotonya",
          "email": ""
        },
        {
          "forename": "Saran",
          "surname": "Krishnasamy",
          "name": "Saran Krishnasamy",
          "email": ""
        },
        {
          "forename": "Joel",
          "surname": "Tetreault",
          "name": "Joel Tetreault",
          "email": ""
        },
        {
          "forename": "Alejandro",
          "surname": "Jaimes",
          "name": "Alejandro Jaimes",
          "email": ""
        }
      ],
      "doi": "arXiv:2311.00686",
      "venue": "2023 shared task",
      "date": "2023"
    },
    {
      "index": "b113",
      "title": "Towards Leveraging Large Language Models for Automated Medical Q&A Evaluation",
      "author": [
        {
          "forename": "Jack",
          "surname": "Krolik",
          "name": "Jack Krolik",
          "email": ""
        },
        {
          "forename": "Herprit",
          "surname": "Mahal",
          "name": "Herprit Mahal",
          "email": ""
        },
        {
          "forename": "Feroz",
          "surname": "Ahmad",
          "name": "Feroz Ahmad",
          "email": ""
        },
        {
          "forename": "Gaurav",
          "surname": "Trivedi",
          "name": "Gaurav Trivedi",
          "email": ""
        },
        {
          "forename": "Bahador",
          "surname": "Saket",
          "name": "Bahador Saket",
          "email": ""
        }
      ],
      "doi": "arXiv:2409.01941",
      "venue": "Towards Leveraging Large Language Models for Automated Medical Q&A Evaluation",
      "date": "2024"
    },
    {
      "index": "b114",
      "title": "LLMs as Evaluators: A Novel Approach to Evaluate Bug Report Summarization",
      "author": [
        {
          "forename": "Abhishek",
          "surname": "Kumar",
          "name": "Abhishek Kumar",
          "email": ""
        },
        {
          "forename": "Sonia",
          "surname": "Haiduc",
          "name": "Sonia Haiduc",
          "email": ""
        },
        {
          "forename": "Partha Pratim ",
          "surname": "Das",
          "name": "Partha Pratim  Das",
          "email": ""
        },
        {
          "forename": "Partha Pratim ",
          "surname": "Chakrabarti",
          "name": "Partha Pratim  Chakrabarti",
          "email": ""
        }
      ],
      "doi": "arXiv:2409.00630",
      "venue": "LLMs as Evaluators: A Novel Approach to Evaluate Bug Report Summarization",
      "date": "2024"
    },
    {
      "index": "b115",
      "title": "Rewardbench: Evaluating reward models for language modeling",
      "author": [
        {
          "forename": "Nathan",
          "surname": "Lambert",
          "name": "Nathan Lambert",
          "email": ""
        },
        {
          "forename": "Valentina",
          "surname": "Pyatkin",
          "name": "Valentina Pyatkin",
          "email": ""
        },
        {
          "forename": "Jacob",
          "surname": "Morrison",
          "name": "Jacob Morrison",
          "email": ""
        },
        {
          "forename": "Khyathi",
          "surname": "Bill Yuchen Lin",
          "name": "Khyathi Bill Yuchen Lin",
          "email": ""
        },
        {
          "forename": "Nouha",
          "surname": "Chandu",
          "name": "Nouha Chandu",
          "email": ""
        },
        {
          "forename": "Sachin",
          "surname": "Dziri",
          "name": "Sachin Dziri",
          "email": ""
        },
        {
          "forename": "Tom",
          "surname": "Kumar",
          "name": "Tom Kumar",
          "email": ""
        },
        {
          "forename": "Yejin",
          "surname": "Zick",
          "name": "Yejin Zick",
          "email": ""
        }
      ],
      "doi": "arXiv:2403.13787",
      "venue": "Rewardbench: Evaluating reward models for language modeling",
      "date": "2024"
    },
    {
      "index": "b116",
      "title": "Can large language models aid in annotating speech emotional data?",
      "author": [
        {
          "forename": "Siddique",
          "surname": "Latif",
          "name": "Siddique Latif",
          "email": ""
        },
        {
          "forename": "Muhammad",
          "surname": "Usama",
          "name": "Muhammad Usama",
          "email": ""
        },
        {
          "forename": "Mohammad Ibrahim ",
          "surname": "Malik",
          "name": "Mohammad Ibrahim  Malik",
          "email": ""
        },
        {
          "forename": "W.",
          "surname": "Björn",
          "name": "W. Björn",
          "email": ""
        }
      ],
      "doi": "arXiv:2307.06090",
      "venue": "uncovering new frontiers",
      "date": "2023"
    },
    {
      "index": "b117",
      "title": "Overview of the TREC 2023 NeuCLIR Track",
      "author": [
        {
          "forename": "Dawn",
          "surname": "Lawrie",
          "name": "Dawn Lawrie",
          "email": ""
        },
        {
          "forename": "Sean",
          "surname": "Macavaney",
          "name": "Sean Macavaney",
          "email": ""
        },
        {
          "forename": "James",
          "surname": "Mayfield",
          "name": "James Mayfield",
          "email": ""
        },
        {
          "forename": "Paul",
          "surname": "Mcnamee",
          "name": "Paul Mcnamee",
          "email": ""
        },
        {
          "forename": "W.",
          "surname": "Douglas",
          "name": "W. Douglas",
          "email": ""
        },
        {
          "forename": "Luca",
          "surname": "Oard",
          "name": "Luca Oard",
          "email": ""
        },
        {
          "forename": "Eugene",
          "surname": "Soldaini",
          "name": "Eugene Soldaini",
          "email": ""
        }
      ],
      "doi": "arXiv:2404.08071",
      "venue": "Overview of the TREC 2023 NeuCLIR Track",
      "date": "2024"
    },
    {
      "index": "b118",
      "title": "Deep learning",
      "author": [
        {
          "forename": "Yann",
          "surname": "Lecun",
          "name": "Yann Lecun",
          "email": ""
        },
        {
          "forename": "Yoshua",
          "surname": "Bengio",
          "name": "Yoshua Bengio",
          "email": ""
        },
        {
          "forename": "Geoffrey",
          "surname": "Hinton",
          "name": "Geoffrey Hinton",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "nature",
      "date": "2015"
    },
    {
      "index": "b119",
      "title": "Query-efficient and scalable black-box adversarial attacks on discrete sequential data via bayesian optimization",
      "author": [
        {
          "forename": "Deokjae",
          "surname": "Lee",
          "name": "Deokjae Lee",
          "email": ""
        },
        {
          "forename": "Seungyong",
          "surname": "Moon",
          "name": "Seungyong Moon",
          "email": ""
        },
        {
          "forename": "Junhyeok",
          "surname": "Lee",
          "name": "Junhyeok Lee",
          "email": ""
        },
        {
          "forename": "Hyun Oh",
          "surname": "Song",
          "name": "Hyun Oh Song",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "International Conference on Machine Learning. PMLR",
      "date": "2022"
    },
    {
      "index": "b120",
      "title": "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
      "author": [
        {
          "forename": "Harrison",
          "surname": "Lee",
          "name": "Harrison Lee",
          "email": ""
        },
        {
          "forename": "Samrat",
          "surname": "Phatale",
          "name": "Samrat Phatale",
          "email": ""
        },
        {
          "forename": "Hassan",
          "surname": "Mansoor",
          "name": "Hassan Mansoor",
          "email": ""
        },
        {
          "forename": "Kellie Ren ",
          "surname": "Lu",
          "name": "Kellie Ren  Lu",
          "email": ""
        },
        {
          "forename": "Thomas",
          "surname": "Mesnard",
          "name": "Thomas Mesnard",
          "email": ""
        },
        {
          "forename": "Johan",
          "surname": "Ferret",
          "name": "Johan Ferret",
          "email": ""
        },
        {
          "forename": "Colton",
          "surname": "Bishop",
          "name": "Colton Bishop",
          "email": ""
        },
        {
          "forename": "Ethan",
          "surname": "Hall",
          "name": "Ethan Hall",
          "email": ""
        },
        {
          "forename": "Victor",
          "surname": "Carbune",
          "name": "Victor Carbune",
          "email": ""
        },
        {
          "forename": "Abhinav",
          "surname": "Rastogi",
          "name": "Abhinav Rastogi",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
      "date": "2023"
    },
    {
      "index": "b121",
      "title": "Prometheusvision: Visionlanguage model as a judge for fine-grained evaluation",
      "author": [
        {
          "forename": "Seongyun",
          "surname": "Lee",
          "name": "Seongyun Lee",
          "email": ""
        },
        {
          "forename": "Seungone",
          "surname": "Kim",
          "name": "Seungone Kim",
          "email": ""
        },
        {
          "forename": "Sue Hyun ",
          "surname": "Park",
          "name": "Sue Hyun  Park",
          "email": ""
        },
        {
          "forename": "Geewook",
          "surname": "Kim",
          "name": "Geewook Kim",
          "email": ""
        },
        {
          "forename": "Minjoon",
          "surname": "Seo",
          "name": "Minjoon Seo",
          "email": ""
        }
      ],
      "doi": "arXiv:2401.06591",
      "venue": "Prometheusvision: Visionlanguage model as a judge for fine-grained evaluation",
      "date": "2024"
    },
    {
      "index": "b122",
      "title": "Aligning Large Language Models by On",
      "author": [
        {
          "forename": "Sangkyu",
          "surname": "Lee",
          "name": "Sangkyu Lee",
          "email": ""
        },
        {
          "forename": "Sungdong",
          "surname": "Kim",
          "name": "Sungdong Kim",
          "email": ""
        },
        {
          "forename": "Ashkan",
          "surname": "Yousefpour",
          "name": "Ashkan Yousefpour",
          "email": ""
        },
        {
          "forename": "Minjoon",
          "surname": "Seo",
          "name": "Minjoon Seo",
          "email": ""
        },
        {
          "forename": "Youngjae",
          "surname": "Kang Min Yoo",
          "name": "Youngjae Kang Min Yoo",
          "email": ""
        }
      ],
      "doi": "arXiv:2402.11253",
      "venue": "Aligning Large Language Models by On",
      "date": "2024"
    },
    {
      "index": "b123",
      "title": "RecExplainer: Aligning Large Language Models for Explaining Recommendation Models",
      "author": [
        {
          "forename": "Yuxuan",
          "surname": "Lei",
          "name": "Yuxuan Lei",
          "email": ""
        },
        {
          "forename": "Jianxun",
          "surname": "Lian",
          "name": "Jianxun Lian",
          "email": ""
        },
        {
          "forename": "Jing",
          "surname": "Yao",
          "name": "Jing Yao",
          "email": ""
        },
        {
          "forename": "Xu",
          "surname": "Huang",
          "name": "Xu Huang",
          "email": ""
        },
        {
          "forename": "Defu",
          "surname": "Lian",
          "name": "Defu Lian",
          "email": ""
        },
        {
          "forename": "Xing",
          "surname": "Xie",
          "name": "Xing Xie",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining",
      "date": "2024"
    },
    {
      "index": "b124",
      "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
      "author": [
        {
          "forename": "Patrick",
          "surname": "Lewis",
          "name": "Patrick Lewis",
          "email": ""
        },
        {
          "forename": "Ethan",
          "surname": "Perez",
          "name": "Ethan Perez",
          "email": ""
        },
        {
          "forename": "Aleksandra",
          "surname": "Piktus",
          "name": "Aleksandra Piktus",
          "email": ""
        },
        {
          "forename": "Fabio",
          "surname": "Petroni",
          "name": "Fabio Petroni",
          "email": ""
        },
        {
          "forename": "Vladimir",
          "surname": "Karpukhin",
          "name": "Vladimir Karpukhin",
          "email": ""
        },
        {
          "forename": "Naman",
          "surname": "Goyal",
          "name": "Naman Goyal",
          "email": ""
        },
        {
          "forename": "Heinrich",
          "surname": "Küttler",
          "name": "Heinrich Küttler",
          "email": ""
        },
        {
          "forename": "Mike",
          "surname": "Lewis",
          "name": "Mike Lewis",
          "email": ""
        },
        {
          "forename": "Wen-Tau",
          "surname": "Yih",
          "name": "Wen-Tau Yih",
          "email": ""
        },
        {
          "forename": "Tim",
          "surname": "Rocktäschel",
          "name": "Tim Rocktäschel",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Advances in Neural Information Processing Systems",
      "date": "2020"
    },
    {
      "index": "b125",
      "title": "Automatic evaluation for mental health counseling using llms",
      "author": [
        {
          "forename": "Anqi",
          "surname": "Li",
          "name": "Anqi Li",
          "email": ""
        },
        {
          "forename": "Yu",
          "surname": "Lu",
          "name": "Yu Lu",
          "email": ""
        },
        {
          "forename": "Nirui",
          "surname": "Song",
          "name": "Nirui Song",
          "email": ""
        },
        {
          "forename": "Shuai",
          "surname": "Zhang",
          "name": "Shuai Zhang",
          "email": ""
        },
        {
          "forename": "Lizhi",
          "surname": "Ma",
          "name": "Lizhi Ma",
          "email": ""
        },
        {
          "forename": "Zhenzhong",
          "surname": "Lan",
          "name": "Zhenzhong Lan",
          "email": ""
        }
      ],
      "doi": "arXiv:2402.11958",
      "venue": "Automatic evaluation for mental health counseling using llms",
      "date": "2024"
    },
    {
      "index": "b126",
      "title": "Calibraeval: Calibrating prediction distribution to mitigate selection bias in llms",
      "author": [
        {
          "forename": "Haitao",
          "surname": "Li",
          "name": "Haitao Li",
          "email": ""
        },
        {
          "forename": "Junjie",
          "surname": "Chen",
          "name": "Junjie Chen",
          "email": ""
        },
        {
          "forename": "Qingyao",
          "surname": "Ai",
          "name": "Qingyao Ai",
          "email": ""
        },
        {
          "forename": "Zhumin",
          "surname": "Chu",
          "name": "Zhumin Chu",
          "email": ""
        },
        {
          "forename": "Yujia",
          "surname": "Zhou",
          "name": "Yujia Zhou",
          "email": ""
        },
        {
          "forename": "Qian",
          "surname": "Dong",
          "name": "Qian Dong",
          "email": ""
        },
        {
          "forename": "Yiqun",
          "surname": "Liu",
          "name": "Yiqun Liu",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.15393",
      "venue": "Calibraeval: Calibrating prediction distribution to mitigate selection bias in llms",
      "date": "2024"
    },
    {
      "index": "b127",
      "title": "LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models",
      "author": [
        {
          "forename": "Haitao",
          "surname": "Li",
          "name": "Haitao Li",
          "email": ""
        },
        {
          "forename": "You",
          "surname": "Chen",
          "name": "You Chen",
          "email": ""
        },
        {
          "forename": "Qingyao",
          "surname": "Ai",
          "name": "Qingyao Ai",
          "email": ""
        },
        {
          "forename": "Yueyue",
          "surname": "Wu",
          "name": "Yueyue Wu",
          "email": ""
        },
        {
          "forename": "Ruizhe",
          "surname": "Zhang",
          "name": "Ruizhe Zhang",
          "email": ""
        },
        {
          "forename": "Yiqun",
          "surname": "Liu",
          "name": "Yiqun Liu",
          "email": ""
        }
      ],
      "doi": "arXiv:2409.20288",
      "venue": "LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models",
      "date": "2024"
    },
    {
      "index": "b128",
      "title": "Lecardv2: A large-scale chinese legal case retrieval dataset",
      "author": [
        {
          "forename": "Haitao",
          "surname": "Li",
          "name": "Haitao Li",
          "email": ""
        },
        {
          "forename": "Yunqiu",
          "surname": "Shao",
          "name": "Yunqiu Shao",
          "email": ""
        },
        {
          "forename": "Yueyue",
          "surname": "Wu",
          "name": "Yueyue Wu",
          "email": ""
        },
        {
          "forename": "Qingyao",
          "surname": "Ai",
          "name": "Qingyao Ai",
          "email": ""
        },
        {
          "forename": "Yixiao",
          "surname": "Ma",
          "name": "Yixiao Ma",
          "email": ""
        },
        {
          "forename": "Yiqun",
          "surname": "Liu",
          "name": "Yiqun Liu",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "date": "2024"
    },
    {
      "index": "b129",
      "title": "Generative judge for evaluating alignment",
      "author": [
        {
          "forename": "Junlong",
          "surname": "Li",
          "name": "Junlong Li",
          "email": ""
        },
        {
          "forename": "Shichao",
          "surname": "Sun",
          "name": "Shichao Sun",
          "email": ""
        },
        {
          "forename": "Weizhe",
          "surname": "Yuan",
          "name": "Weizhe Yuan",
          "email": ""
        },
        {
          "forename": "Hai",
          "surname": "Run-Ze Fan",
          "name": "Hai Run-Ze Fan",
          "email": ""
        },
        {
          "forename": "Pengfei",
          "surname": "Zhao",
          "name": "Pengfei Zhao",
          "email": ""
        }
      ],
      "doi": "arXiv:2310.05470",
      "venue": "Generative judge for evaluating alignment",
      "date": "2023"
    },
    {
      "index": "b130",
      "title": "Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation",
      "author": [
        {
          "forename": "Qintong",
          "surname": "Li",
          "name": "Qintong Li",
          "email": ""
        },
        {
          "forename": "Leyang",
          "surname": "Cui",
          "name": "Leyang Cui",
          "email": ""
        },
        {
          "forename": "Lingpeng",
          "surname": "Kong",
          "name": "Lingpeng Kong",
          "email": ""
        },
        {
          "forename": "Wei",
          "surname": "Bi",
          "name": "Wei Bi",
          "email": ""
        }
      ],
      "doi": "arXiv:2310.19740",
      "venue": "Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation",
      "date": "2023"
    },
    {
      "index": "b131",
      "title": "Prd: Peer rank and discussion improve large language model based evaluations",
      "author": [
        {
          "forename": "Ruosen",
          "surname": "Li",
          "name": "Ruosen Li",
          "email": ""
        },
        {
          "forename": "Teerth",
          "surname": "Patel",
          "name": "Teerth Patel",
          "email": ""
        },
        {
          "forename": "Xinya",
          "surname": "Du",
          "name": "Xinya Du",
          "email": ""
        }
      ],
      "doi": "arXiv:2307.02762",
      "venue": "Prd: Peer rank and discussion improve large language model based evaluations",
      "date": "2023"
    },
    {
      "index": "b132",
      "title": "DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset",
      "author": [
        {
          "forename": "Yanran",
          "surname": "Li",
          "name": "Yanran Li",
          "email": ""
        },
        {
          "forename": "Hui",
          "surname": "Su",
          "name": "Hui Su",
          "email": ""
        },
        {
          "forename": "Xiaoyu",
          "surname": "Shen",
          "name": "Xiaoyu Shen",
          "email": ""
        },
        {
          "forename": "Wenjie",
          "surname": "Li",
          "name": "Wenjie Li",
          "email": ""
        },
        {
          "forename": "Ziqiang",
          "surname": "Cao",
          "name": "Ziqiang Cao",
          "email": ""
        },
        {
          "forename": "Shuzi",
          "surname": "Niu",
          "name": "Shuzi Niu",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of The 8th International Joint Conference on Natural Language Processing",
      "date": "2017"
    },
    {
      "index": "b133",
      "title": "Split and merge: Aligning position biases in large language model based evaluators",
      "author": [
        {
          "forename": "Zongjie",
          "surname": "Li",
          "name": "Zongjie Li",
          "email": ""
        },
        {
          "forename": "Chaozheng",
          "surname": "Wang",
          "name": "Chaozheng Wang",
          "email": ""
        },
        {
          "forename": "Pingchuan",
          "surname": "Ma",
          "name": "Pingchuan Ma",
          "email": ""
        },
        {
          "forename": "Daoyuan",
          "surname": "Wu",
          "name": "Daoyuan Wu",
          "email": ""
        },
        {
          "forename": "Shuai",
          "surname": "Wang",
          "name": "Shuai Wang",
          "email": ""
        },
        {
          "forename": "Cuiyun",
          "surname": "Gao",
          "name": "Cuiyun Gao",
          "email": ""
        },
        {
          "forename": "Yang",
          "surname": "Liu",
          "name": "Yang Liu",
          "email": ""
        }
      ],
      "doi": "arXiv:2310.01432",
      "venue": "Split and merge: Aligning position biases in large language model based evaluators",
      "date": "2023"
    },
    {
      "index": "b134",
      "title": "Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM",
      "author": [
        {
          "forename": "Jingcong",
          "surname": "Liang",
          "name": "Jingcong Liang",
          "email": ""
        },
        {
          "forename": "Rong",
          "surname": "Ye",
          "name": "Rong Ye",
          "email": ""
        },
        {
          "forename": "Meng",
          "surname": "Han",
          "name": "Meng Han",
          "email": ""
        },
        {
          "forename": "Ruofei",
          "surname": "Lai",
          "name": "Ruofei Lai",
          "email": ""
        },
        {
          "forename": "Xinyu",
          "surname": "Zhang",
          "name": "Xinyu Zhang",
          "email": ""
        },
        {
          "forename": "Xuanjing",
          "surname": "Huang",
          "name": "Xuanjing Huang",
          "email": ""
        },
        {
          "forename": "Zhongyu",
          "surname": "Wei",
          "name": "Zhongyu Wei",
          "email": ""
        }
      ],
      "doi": "arXiv:2403.08010",
      "venue": "Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM",
      "date": "2024"
    },
    {
      "index": "b135",
      "title": "ABSEval: An Agent-based Framework for Script Evaluation",
      "author": [
        {
          "forename": "Sirui",
          "surname": "Liang",
          "name": "Sirui Liang",
          "email": ""
        },
        {
          "forename": "Baoli",
          "surname": "Zhang",
          "name": "Baoli Zhang",
          "email": ""
        },
        {
          "forename": "Jun",
          "surname": "Zhao",
          "name": "Jun Zhao",
          "email": ""
        },
        {
          "forename": "Kang",
          "surname": "Liu",
          "name": "Kang Liu",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
      "date": "2024"
    },
    {
      "index": "b136",
      "title": "John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step",
      "author": [
        {
          "forename": "Vineet",
          "surname": "Hunter Lightman",
          "name": "Vineet Hunter Lightman",
          "email": ""
        },
        {
          "forename": "Yura",
          "surname": "Kosaraju",
          "name": "Yura Kosaraju",
          "email": ""
        },
        {
          "forename": "Harri",
          "surname": "Burda",
          "name": "Harri Burda",
          "email": ""
        }
      ],
      "doi": "arXiv:2305.20050",
      "venue": "John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step",
      "date": "2023"
    },
    {
      "index": "b137",
      "title": "WILDBENCH: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild",
      "author": [
        {
          "forename": "Yuntian",
          "surname": "Bill Yuchen Lin",
          "name": "Yuntian Bill Yuchen Lin",
          "email": ""
        },
        {
          "forename": "Khyathi",
          "surname": "Deng",
          "name": "Khyathi Deng",
          "email": ""
        },
        {
          "forename": "Faeze",
          "surname": "Chandu",
          "name": "Faeze Chandu",
          "email": ""
        },
        {
          "forename": "Abhilasha",
          "surname": "Brahman",
          "name": "Abhilasha Brahman",
          "email": ""
        },
        {
          "forename": "Valentina",
          "surname": "Ravichander",
          "name": "Valentina Ravichander",
          "email": ""
        },
        {
          "forename": "Nouha",
          "surname": "Pyatkin",
          "name": "Nouha Pyatkin",
          "email": ""
        },
        {
          "forename": "Yejin",
          "surname": "Ronan Le Bras",
          "name": "Yejin Ronan Le Bras",
          "email": ""
        }
      ],
      "doi": "arXiv:2406.04770",
      "venue": "WILDBENCH: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild",
      "date": "2024"
    },
    {
      "index": "b138",
      "title": "Rouge: A package for automatic evaluation of summaries",
      "author": [
        {
          "forename": "Chin-Yew",
          "surname": "Lin",
          "name": "Chin-Yew Lin",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Text summarization branches out",
      "date": "2004"
    },
    {
      "index": "b139",
      "title": "Truthfulqa: Measuring how models mimic human falsehoods",
      "author": [
        {
          "forename": "Stephanie",
          "surname": "Lin",
          "name": "Stephanie Lin",
          "email": ""
        },
        {
          "forename": "Jacob",
          "surname": "Hilton",
          "name": "Jacob Hilton",
          "email": ""
        },
        {
          "forename": "Owain",
          "surname": "Evans",
          "name": "Owain Evans",
          "email": ""
        }
      ],
      "doi": "arXiv:2109.07958",
      "venue": "Truthfulqa: Measuring how models mimic human falsehoods",
      "date": "2021"
    },
    {
      "index": "b140",
      "title": "Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models",
      "author": [
        {
          "forename": "Yen-Ting",
          "surname": "Lin",
          "name": "Yen-Ting Lin",
          "email": ""
        },
        {
          "forename": "Yun-Nung",
          "surname": "Chen",
          "name": "Yun-Nung Chen",
          "email": ""
        }
      ],
      "doi": "arXiv:2305.13711",
      "venue": "Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models",
      "date": "2023"
    },
    {
      "index": "b141",
      "title": "X-eval: Generalizable multi-aspect text evaluation via augmented instruction tuning with auxiliary evaluation aspects",
      "author": [
        {
          "forename": "Minqian",
          "surname": "Liu",
          "name": "Minqian Liu",
          "email": ""
        },
        {
          "forename": "Ying",
          "surname": "Shen",
          "name": "Ying Shen",
          "email": ""
        },
        {
          "forename": "Zhiyang",
          "surname": "Xu",
          "name": "Zhiyang Xu",
          "email": ""
        },
        {
          "forename": "Yixin",
          "surname": "Cao",
          "name": "Yixin Cao",
          "email": ""
        },
        {
          "forename": "Eunah",
          "surname": "Cho",
          "name": "Eunah Cho",
          "email": ""
        },
        {
          "forename": "Vaibhav",
          "surname": "Kumar",
          "name": "Vaibhav Kumar",
          "email": ""
        },
        {
          "forename": "Reza",
          "surname": "Ghanadan",
          "name": "Reza Ghanadan",
          "email": ""
        },
        {
          "forename": "Lifu",
          "surname": "Huang",
          "name": "Lifu Huang",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "X-eval: Generalizable multi-aspect text evaluation via augmented instruction tuning with auxiliary evaluation aspects",
      "date": "2023-12"
    },
    {
      "index": "b143",
      "title": "Alignbench: Benchmarking chinese alignment of large language models",
      "author": [
        {
          "forename": "Xiao",
          "surname": "Liu",
          "name": "Xiao Liu",
          "email": ""
        },
        {
          "forename": "Xuanyu",
          "surname": "Lei",
          "name": "Xuanyu Lei",
          "email": ""
        },
        {
          "forename": "Shengyuan",
          "surname": "Wang",
          "name": "Shengyuan Wang",
          "email": ""
        },
        {
          "forename": "Yue",
          "surname": "Huang",
          "name": "Yue Huang",
          "email": ""
        },
        {
          "forename": "Zhuoer",
          "surname": "Feng",
          "name": "Zhuoer Feng",
          "email": ""
        },
        {
          "forename": "Bosi",
          "surname": "Wen",
          "name": "Bosi Wen",
          "email": ""
        },
        {
          "forename": "Jiale",
          "surname": "Cheng",
          "name": "Jiale Cheng",
          "email": ""
        },
        {
          "forename": "Pei",
          "surname": "Ke",
          "name": "Pei Ke",
          "email": ""
        },
        {
          "forename": "Yifan",
          "surname": "Xu",
          "name": "Yifan Xu",
          "email": ""
        },
        {
          "forename": "Weng Lam",
          "surname": "Tam",
          "name": "Weng Lam Tam",
          "email": ""
        }
      ],
      "doi": "arXiv:2311.18743",
      "venue": "Alignbench: Benchmarking chinese alignment of large language models",
      "date": "2023"
    },
    {
      "index": "b144",
      "title": "Agentbench: Evaluating llms as agents",
      "author": [
        {
          "forename": "Xiao",
          "surname": "Liu",
          "name": "Xiao Liu",
          "email": ""
        },
        {
          "forename": "Hao",
          "surname": "Yu",
          "name": "Hao Yu",
          "email": ""
        },
        {
          "forename": "Hanchen",
          "surname": "Zhang",
          "name": "Hanchen Zhang",
          "email": ""
        },
        {
          "forename": "Yifan",
          "surname": "Xu",
          "name": "Yifan Xu",
          "email": ""
        },
        {
          "forename": "Xuanyu",
          "surname": "Lei",
          "name": "Xuanyu Lei",
          "email": ""
        },
        {
          "forename": "Hanyu",
          "surname": "Lai",
          "name": "Hanyu Lai",
          "email": ""
        },
        {
          "forename": "Yu",
          "surname": "Gu",
          "name": "Yu Gu",
          "email": ""
        },
        {
          "forename": "Hangliang",
          "surname": "Ding",
          "name": "Hangliang Ding",
          "email": ""
        },
        {
          "forename": "Kaiwen",
          "surname": "Men",
          "name": "Kaiwen Men",
          "email": ""
        },
        {
          "forename": "Kejuan",
          "surname": "Yang",
          "name": "Kejuan Yang",
          "email": ""
        }
      ],
      "doi": "arXiv:2308.03688",
      "venue": "Agentbench: Evaluating llms as agents",
      "date": "2023"
    },
    {
      "index": "b145",
      "title": "Ruochen Xu, and Chenguang Zhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human alignment",
      "author": [
        {
          "forename": "Yang",
          "surname": "Liu",
          "name": "Yang Liu",
          "email": ""
        },
        {
          "forename": "Dan",
          "surname": "Iter",
          "name": "Dan Iter",
          "email": ""
        },
        {
          "forename": "Yichong",
          "surname": "Xu",
          "name": "Yichong Xu",
          "email": ""
        },
        {
          "forename": "Shuohang",
          "surname": "Wang",
          "name": "Shuohang Wang",
          "email": ""
        }
      ],
      "doi": "arXiv:2303.16634",
      "venue": "Ruochen Xu, and Chenguang Zhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human alignment",
      "date": "2023"
    },
    {
      "index": "b146",
      "title": "Feng Sun, and Qi Zhang. 2023. Calibrating llm-based evaluator",
      "author": [
        {
          "forename": "Yuxuan",
          "surname": "Liu",
          "name": "Yuxuan Liu",
          "email": ""
        },
        {
          "forename": "Tianchi",
          "surname": "Yang",
          "name": "Tianchi Yang",
          "email": ""
        },
        {
          "forename": "Shaohan",
          "surname": "Huang",
          "name": "Shaohan Huang",
          "email": ""
        },
        {
          "forename": "Zihan",
          "surname": "Zhang",
          "name": "Zihan Zhang",
          "email": ""
        },
        {
          "forename": "Haizhen",
          "surname": "Huang",
          "name": "Haizhen Huang",
          "email": ""
        },
        {
          "forename": "Furu",
          "surname": "Wei",
          "name": "Furu Wei",
          "email": ""
        },
        {
          "forename": "Weiwei",
          "surname": "Deng",
          "name": "Weiwei Deng",
          "email": ""
        }
      ],
      "doi": "arXiv:2309.13308",
      "venue": "Feng Sun, and Qi Zhang. 2023. Calibrating llm-based evaluator",
      "date": "2023"
    },
    {
      "index": "b147",
      "title": "Feng Sun, and Qi Zhang. 2024. HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition",
      "author": [
        {
          "forename": "Yuxuan",
          "surname": "Liu",
          "name": "Yuxuan Liu",
          "email": ""
        },
        {
          "forename": "Tianchi",
          "surname": "Yang",
          "name": "Tianchi Yang",
          "email": ""
        },
        {
          "forename": "Shaohan",
          "surname": "Huang",
          "name": "Shaohan Huang",
          "email": ""
        },
        {
          "forename": "Zihan",
          "surname": "Zhang",
          "name": "Zihan Zhang",
          "email": ""
        },
        {
          "forename": "Haizhen",
          "surname": "Huang",
          "name": "Haizhen Huang",
          "email": ""
        },
        {
          "forename": "Furu",
          "surname": "Wei",
          "name": "Furu Wei",
          "email": ""
        },
        {
          "forename": "Weiwei",
          "surname": "Deng",
          "name": "Weiwei Deng",
          "email": ""
        }
      ],
      "doi": "arXiv:2402.15754",
      "venue": "Feng Sun, and Qi Zhang. 2024. HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition",
      "date": "2024"
    },
    {
      "index": "b148",
      "title": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style",
      "author": [
        {
          "forename": "Yantao",
          "surname": "Liu",
          "name": "Yantao Liu",
          "email": ""
        },
        {
          "forename": "Zijun",
          "surname": "Yao",
          "name": "Zijun Yao",
          "email": ""
        },
        {
          "forename": "Rui",
          "surname": "Min",
          "name": "Rui Min",
          "email": ""
        },
        {
          "forename": "Yixin",
          "surname": "Cao",
          "name": "Yixin Cao",
          "email": ""
        },
        {
          "forename": "Lei",
          "surname": "Hou",
          "name": "Lei Hou",
          "email": ""
        },
        {
          "forename": "Juanzi",
          "surname": "Li",
          "name": "Juanzi Li",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.16184",
      "venue": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style",
      "date": "2024"
    },
    {
      "index": "b149",
      "title": "Aligning with human judgement: The role of pairwise preference in large language model evaluators",
      "author": [
        {
          "forename": "Yinhong",
          "surname": "Liu",
          "name": "Yinhong Liu",
          "email": ""
        },
        {
          "forename": "Han",
          "surname": "Zhou",
          "name": "Han Zhou",
          "email": ""
        },
        {
          "forename": "Zhijiang",
          "surname": "Guo",
          "name": "Zhijiang Guo",
          "email": ""
        },
        {
          "forename": "Ehsan",
          "surname": "Shareghi",
          "name": "Ehsan Shareghi",
          "email": ""
        },
        {
          "forename": "Ivan",
          "surname": "Vulić",
          "name": "Ivan Vulić",
          "email": ""
        },
        {
          "forename": "Anna",
          "surname": "Korhonen",
          "name": "Anna Korhonen",
          "email": ""
        },
        {
          "forename": "Nigel",
          "surname": "Collier",
          "name": "Nigel Collier",
          "email": ""
        }
      ],
      "doi": "arXiv:2403.16950",
      "venue": "Aligning with human judgement: The role of pairwise preference in large language model evaluators",
      "date": "2024"
    },
    {
      "index": "b150",
      "title": "Yassir Fathullah, and Mark Gales. 2024. Efficient LLM Comparative Assessment: a Product of Experts Framework for Pairwise Comparisons",
      "author": [
        {
          "forename": "Adian",
          "surname": "Liusie",
          "name": "Adian Liusie",
          "email": ""
        }
      ],
      "doi": "arXiv:2405.05894",
      "venue": "Yassir Fathullah, and Mark Gales. 2024. Efficient LLM Comparative Assessment: a Product of Experts Framework for Pairwise Comparisons",
      "date": "2024"
    },
    {
      "index": "b151",
      "title": "JUDING THE JUDGES: ASYSTEMATIC INVESTIGATION OF POSITION BIAS IN PAIRWISE COMPARATIVE AS. Under review as a conference paper at ICLR 2025",
      "author": [],
      "doi": "",
      "venue": "JUDING THE JUDGES: ASYSTEMATIC INVESTIGATION OF POSITION BIAS IN PAIRWISE COMPARATIVE AS. Under review as a conference paper at ICLR 2025",
      "date": "2025"
    },
    {
      "index": "b152",
      "title": "An empirical study of catastrophic forgetting in large language models during continual fine-tuning",
      "author": [
        {
          "forename": "Yun",
          "surname": "Luo",
          "name": "Yun Luo",
          "email": ""
        },
        {
          "forename": "Zhen",
          "surname": "Yang",
          "name": "Zhen Yang",
          "email": ""
        },
        {
          "forename": "Fandong",
          "surname": "Meng",
          "name": "Fandong Meng",
          "email": ""
        },
        {
          "forename": "Yafu",
          "surname": "Li",
          "name": "Yafu Li",
          "email": ""
        },
        {
          "forename": "Jie",
          "surname": "Zhou",
          "name": "Jie Zhou",
          "email": ""
        },
        {
          "forename": "Yue",
          "surname": "Zhang",
          "name": "Yue Zhang",
          "email": ""
        }
      ],
      "doi": "arXiv:2308.08747",
      "venue": "An empirical study of catastrophic forgetting in large language models during continual fine-tuning",
      "date": "2023"
    },
    {
      "index": "b153",
      "title": "VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation",
      "author": [
        {
          "forename": "Ziyang",
          "surname": "Luo",
          "name": "Ziyang Luo",
          "email": ""
        },
        {
          "forename": "Haoning",
          "surname": "Wu",
          "name": "Haoning Wu",
          "email": ""
        },
        {
          "forename": "Dongxu",
          "surname": "Li",
          "name": "Dongxu Li",
          "email": ""
        },
        {
          "forename": "Jing",
          "surname": "Ma",
          "name": "Jing Ma",
          "email": ""
        },
        {
          "forename": "Mohan",
          "surname": "Kankanhalli",
          "name": "Mohan Kankanhalli",
          "email": ""
        },
        {
          "forename": "Junnan",
          "surname": "Li",
          "name": "Junnan Li",
          "email": ""
        }
      ],
      "doi": "arXiv:2411.13281",
      "venue": "VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation",
      "date": "2024"
    },
    {
      "index": "b154",
      "title": "Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval",
      "author": [
        {
          "forename": "Shengjie",
          "surname": "Ma",
          "name": "Shengjie Ma",
          "email": ""
        },
        {
          "forename": "Chong",
          "surname": "Chen",
          "name": "Chong Chen",
          "email": ""
        },
        {
          "forename": "Qi",
          "surname": "Chu",
          "name": "Qi Chu",
          "email": ""
        },
        {
          "forename": "Jiaxin",
          "surname": "Mao",
          "name": "Jiaxin Mao",
          "email": ""
        }
      ],
      "doi": "arXiv:2403.18405",
      "venue": "Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval",
      "date": "2024"
    },
    {
      "index": "b155",
      "title": "Self-refine: Iterative refinement with self-feedback",
      "author": [
        {
          "forename": "Aman",
          "surname": "Madaan",
          "name": "Aman Madaan",
          "email": ""
        },
        {
          "forename": "Niket",
          "surname": "Tandon",
          "name": "Niket Tandon",
          "email": ""
        },
        {
          "forename": "Prakhar",
          "surname": "Gupta",
          "name": "Prakhar Gupta",
          "email": ""
        },
        {
          "forename": "Skyler",
          "surname": "Hallinan",
          "name": "Skyler Hallinan",
          "email": ""
        },
        {
          "forename": "Luyu",
          "surname": "Gao",
          "name": "Luyu Gao",
          "email": ""
        },
        {
          "forename": "Sarah",
          "surname": "Wiegreffe",
          "name": "Sarah Wiegreffe",
          "email": ""
        },
        {
          "forename": "Uri",
          "surname": "Alon",
          "name": "Uri Alon",
          "email": ""
        },
        {
          "forename": "Nouha",
          "surname": "Dziri",
          "name": "Nouha Dziri",
          "email": ""
        },
        {
          "forename": "Shrimai",
          "surname": "Prabhumoye",
          "name": "Shrimai Prabhumoye",
          "email": ""
        },
        {
          "forename": "Yiming",
          "surname": "Yang",
          "name": "Yiming Yang",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Advances in Neural Information Processing Systems",
      "date": "2024"
    },
    {
      "index": "b156",
      "title": "USR: An unsupervised and reference free evaluation metric for dialog generation",
      "author": [
        {
          "forename": "Shikib",
          "surname": "Mehri",
          "name": "Shikib Mehri",
          "email": ""
        },
        {
          "forename": "Maxine",
          "surname": "Eskenazi",
          "name": "Maxine Eskenazi",
          "email": ""
        }
      ],
      "doi": "arXiv:2005.00456",
      "venue": "USR: An unsupervised and reference free evaluation metric for dialog generation",
      "date": "2020"
    },
    {
      "index": "b157",
      "title": "Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs",
      "author": [
        {
          "forename": "John",
          "surname": "Mendonça",
          "name": "John Mendonça",
          "email": ""
        },
        {
          "forename": "Isabel",
          "surname": "Trancoso",
          "name": "Isabel Trancoso",
          "email": ""
        },
        {
          "forename": "Alon",
          "surname": "Lavie",
          "name": "Alon Lavie",
          "email": ""
        }
      ],
      "doi": "arXiv:2408.10902",
      "venue": "Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs",
      "date": "2024"
    },
    {
      "index": "b158",
      "title": "Evaluating the Performance of Large Language Models via Debates",
      "author": [
        {
          "forename": "Behrad",
          "surname": "Moniri",
          "name": "Behrad Moniri",
          "email": ""
        },
        {
          "forename": "Hamed",
          "surname": "Hassani",
          "name": "Hamed Hassani",
          "email": ""
        },
        {
          "forename": "Edgar",
          "surname": "Dobriban",
          "name": "Edgar Dobriban",
          "email": ""
        }
      ],
      "doi": "arXiv:2406.11044",
      "venue": "Evaluating the Performance of Large Language Models via Debates",
      "date": "2024"
    },
    {
      "index": "b159",
      "title": "A corpus and cloze evaluation for deeper understanding of commonsense stories",
      "author": [
        {
          "forename": "Nasrin",
          "surname": "Mostafazadeh",
          "name": "Nasrin Mostafazadeh",
          "email": ""
        },
        {
          "forename": "Nathanael",
          "surname": "Chambers",
          "name": "Nathanael Chambers",
          "email": ""
        },
        {
          "forename": "Xiaodong",
          "surname": "He",
          "name": "Xiaodong He",
          "email": ""
        },
        {
          "forename": "Devi",
          "surname": "Parikh",
          "name": "Devi Parikh",
          "email": ""
        },
        {
          "forename": "Dhruv",
          "surname": "Batra",
          "name": "Dhruv Batra",
          "email": ""
        },
        {
          "forename": "Lucy",
          "surname": "Vanderwende",
          "name": "Lucy Vanderwende",
          "email": ""
        },
        {
          "forename": "Pushmeet",
          "surname": "Kohli",
          "name": "Pushmeet Kohli",
          "email": ""
        },
        {
          "forename": "James",
          "surname": "Allen",
          "name": "James Allen",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "date": "2016"
    },
    {
      "index": "b160",
      "title": "Creative Beam Search: LLM-as-a-Judge for Improving Response Generation",
      "author": [
        {
          "forename": "Mirco",
          "surname": "Musolesi",
          "name": "Mirco Musolesi",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Creative Beam Search: LLM-as-a-Judge for Improving Response Generation",
      "date": "2024"
    },
    {
      "index": "b161",
      "title": "Sondos Mahmoud Bsharat, and Zhiqiang Shen. 2024. Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena",
      "author": [
        {
          "forename": "Aidar",
          "surname": "Myrzakhan",
          "name": "Aidar Myrzakhan",
          "email": ""
        }
      ],
      "doi": "arXiv:2406.07545",
      "venue": "Sondos Mahmoud Bsharat, and Zhiqiang Shen. 2024. Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena",
      "date": "2024"
    },
    {
      "index": "b162",
      "title": "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
      "author": [
        {
          "forename": "Shashi",
          "surname": "Narayan",
          "name": "Shashi Narayan",
          "email": ""
        },
        {
          "forename": "B.",
          "surname": "Shay",
          "name": "B. Shay",
          "email": ""
        },
        {
          "forename": "Mirella",
          "surname": "Cohen",
          "name": "Mirella Cohen",
          "email": ""
        }
      ],
      "doi": "arXiv:1808.08745",
      "venue": "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
      "date": "2018"
    },
    {
      "index": "b163",
      "title": "JurEE not Judges: safeguarding llm interactions with small",
      "author": [
        {
          "forename": "Dom",
          "surname": "Nasrabadi",
          "name": "Dom Nasrabadi",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.08442",
      "venue": "JurEE not Judges: safeguarding llm interactions with small",
      "date": "2024"
    },
    {
      "index": "b164",
      "title": "Principles of artificial intelligence",
      "author": [
        {
          "forename": "J.",
          "surname": "Nils",
          "name": "J. Nils",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Principles of artificial intelligence",
      "date": "2014"
    },
    {
      "index": "b165",
      "title": "PiCO: Peer Review in LLMs based on the Consistency Optimization",
      "author": [
        {
          "forename": "Kun-Peng",
          "surname": "Ning",
          "name": "Kun-Peng Ning",
          "email": ""
        },
        {
          "forename": "Shuo",
          "surname": "Yang",
          "name": "Shuo Yang",
          "email": ""
        },
        {
          "forename": "Yuyang",
          "surname": "Liu",
          "name": "Yuyang Liu",
          "email": ""
        },
        {
          "forename": "Jia-Yu",
          "surname": "Yao",
          "name": "Jia-Yu Yao",
          "email": ""
        },
        {
          "forename": "Zhenhui",
          "surname": "Liu",
          "name": "Zhenhui Liu",
          "email": ""
        },
        {
          "forename": "Yu",
          "surname": "Wang",
          "name": "Yu Wang",
          "email": ""
        },
        {
          "forename": "Ming",
          "surname": "Pang",
          "name": "Ming Pang",
          "email": ""
        },
        {
          "forename": "Li",
          "surname": "Yuan",
          "name": "Li Yuan",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "PiCO: Peer Review in LLMs based on the Consistency Optimization",
      "date": "2024"
    },
    {
      "index": "b166",
      "title": "JudgeRank: Leveraging Large Language Models for Reasoning-Intensive Reranking",
      "author": [
        {
          "forename": "Tong",
          "surname": "Niu",
          "name": "Tong Niu",
          "email": ""
        },
        {
          "forename": "Shafiq",
          "surname": "Joty",
          "name": "Shafiq Joty",
          "email": ""
        },
        {
          "forename": "Ye",
          "surname": "Liu",
          "name": "Ye Liu",
          "email": ""
        },
        {
          "forename": "Caiming",
          "surname": "Xiong",
          "name": "Caiming Xiong",
          "email": ""
        },
        {
          "forename": "Yingbo",
          "surname": "Zhou",
          "name": "Yingbo Zhou",
          "email": ""
        },
        {
          "forename": "Semih",
          "surname": "Yavuz",
          "name": "Semih Yavuz",
          "email": ""
        }
      ],
      "doi": "arXiv:2411.00142",
      "venue": "JudgeRank: Leveraging Large Language Models for Reasoning-Intensive Reranking",
      "date": "2024"
    },
    {
      "index": "b167",
      "title": "BioPlanner: automatic evaluation of LLMs on protocol planning in biology",
      "author": [
        {
          "forename": "Aleksandar",
          "surname": "Odhran O'donoghue",
          "name": "Aleksandar Odhran O'donoghue",
          "email": ""
        },
        {
          "forename": "John",
          "surname": "Shtedritski",
          "name": "John Shtedritski",
          "email": ""
        },
        {
          "forename": "Ralph",
          "surname": "Ginger",
          "name": "Ralph Ginger",
          "email": ""
        },
        {
          "forename": "Ali Essa ",
          "surname": "Abboud",
          "name": "Ali Essa  Abboud",
          "email": ""
        },
        {
          "forename": "Justin",
          "surname": "Ghareeb",
          "name": "Justin Ghareeb",
          "email": ""
        },
        {
          "forename": "Samuel G",
          "surname": "Booth",
          "name": "Samuel G Booth",
          "email": ""
        }
      ],
      "doi": "arXiv:2310.10632",
      "venue": "BioPlanner: automatic evaluation of LLMs on protocol planning in biology",
      "date": "2023"
    },
    {
      "index": "b168",
      "title": "Hanieh Deilamsalehy, and Nedim Lipka. 2024. A multi-llm debiasing framework",
      "author": [
        {
          "forename": "M.",
          "surname": "Deonna",
          "name": "M. Deonna",
          "email": ""
        },
        {
          "forename": "Ryan A.",
          "surname": "Owens",
          "name": "Ryan A. Owens",
          "email": ""
        },
        {
          "forename": "Sungchul",
          "surname": "Rossi",
          "name": "Sungchul Rossi",
          "email": ""
        },
        {
          "forename": "Tong",
          "surname": "Kim",
          "name": "Tong Kim",
          "email": ""
        },
        {
          "forename": "Franck",
          "surname": "Yu",
          "name": "Franck Yu",
          "email": ""
        },
        {
          "forename": "Xiang",
          "surname": "Dernoncourt",
          "name": "Xiang Dernoncourt",
          "email": ""
        },
        {
          "forename": "Ruiyi",
          "surname": "Chen",
          "name": "Ruiyi Chen",
          "email": ""
        },
        {
          "forename": "Jiuxiang",
          "surname": "Zhang",
          "name": "Jiuxiang Zhang",
          "email": ""
        }
      ],
      "doi": "arXiv:2409.13884",
      "venue": "Hanieh Deilamsalehy, and Nedim Lipka. 2024. A multi-llm debiasing framework",
      "date": "2024"
    },
    {
      "index": "b169",
      "title": "Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics",
      "author": [
        {
          "forename": "Artidoro",
          "surname": "Pagnoni",
          "name": "Artidoro Pagnoni",
          "email": ""
        },
        {
          "forename": "Vidhisha",
          "surname": "Balachandran",
          "name": "Vidhisha Balachandran",
          "email": ""
        },
        {
          "forename": "Yulia",
          "surname": "Tsvetkov",
          "name": "Yulia Tsvetkov",
          "email": ""
        }
      ],
      "doi": "arXiv:2104.13346",
      "venue": "Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics",
      "date": "2021"
    },
    {
      "index": "b170",
      "title": "Human-Centered Design Recommendations for LLM-as-a-judge",
      "author": [
        {
          "forename": "Qian",
          "surname": "Pan",
          "name": "Qian Pan",
          "email": ""
        },
        {
          "forename": "Zahra",
          "surname": "Ashktorab",
          "name": "Zahra Ashktorab",
          "email": ""
        },
        {
          "forename": "Michael",
          "surname": "Desmond",
          "name": "Michael Desmond",
          "email": ""
        },
        {
          "forename": "Martin Santillan ",
          "surname": "Cooper",
          "name": "Martin Santillan  Cooper",
          "email": ""
        },
        {
          "forename": "James",
          "surname": "Johnson",
          "name": "James Johnson",
          "email": ""
        },
        {
          "forename": "Rahul",
          "surname": "Nair",
          "name": "Rahul Nair",
          "email": ""
        },
        {
          "forename": "Elizabeth",
          "surname": "Daly",
          "name": "Elizabeth Daly",
          "email": ""
        },
        {
          "forename": "Werner",
          "surname": "Geyer",
          "name": "Werner Geyer",
          "email": ""
        }
      ],
      "doi": "arXiv:2407.03479",
      "venue": "Human-Centered Design Recommendations for LLM-as-a-judge",
      "date": "2024"
    },
    {
      "index": "b171",
      "title": "Unifying large language models and knowledge graphs: A roadmap",
      "author": [
        {
          "forename": "Linhao",
          "surname": "Shirui Pan",
          "name": "Linhao Shirui Pan",
          "email": ""
        },
        {
          "forename": "Yufei",
          "surname": "Luo",
          "name": "Yufei Luo",
          "email": ""
        },
        {
          "forename": "Chen",
          "surname": "Wang",
          "name": "Chen Wang",
          "email": ""
        },
        {
          "forename": "Jiapu",
          "surname": "Chen",
          "name": "Jiapu Chen",
          "email": ""
        },
        {
          "forename": "Xindong",
          "surname": "Wang",
          "name": "Xindong Wang",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "date": "2024"
    },
    {
      "index": "b172",
      "title": "Llm evaluators recognize and favor their own generations",
      "author": [
        {
          "forename": "Arjun",
          "surname": "Panickssery",
          "name": "Arjun Panickssery",
          "email": ""
        },
        {
          "forename": "R.",
          "surname": "Samuel",
          "name": "R. Samuel",
          "email": ""
        },
        {
          "forename": "Shi",
          "surname": "Bowman",
          "name": "Shi Bowman",
          "email": ""
        }
      ],
      "doi": "arXiv:2404.13076",
      "venue": "Llm evaluators recognize and favor their own generations",
      "date": "2024"
    },
    {
      "index": "b173",
      "title": "Bleu: a method for automatic evaluation of machine translation",
      "author": [
        {
          "forename": "Kishore",
          "surname": "Papineni",
          "name": "Kishore Papineni",
          "email": ""
        },
        {
          "forename": "Salim",
          "surname": "Roukos",
          "name": "Salim Roukos",
          "email": ""
        },
        {
          "forename": "Todd",
          "surname": "Ward",
          "name": "Todd Ward",
          "email": ""
        },
        {
          "forename": "Wei-Jing",
          "surname": "Zhu",
          "name": "Wei-Jing Zhu",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics",
      "date": "2002"
    },
    {
      "index": "b174",
      "title": "Offsetbias: Leveraging debiased data for tuning evaluators",
      "author": [
        {
          "forename": "Junsoo",
          "surname": "Park",
          "name": "Junsoo Park",
          "email": ""
        },
        {
          "forename": "Seungyeon",
          "surname": "Jwa",
          "name": "Seungyeon Jwa",
          "email": ""
        },
        {
          "forename": "Meiying",
          "surname": "Ren",
          "name": "Meiying Ren",
          "email": ""
        },
        {
          "forename": "Daeyoung",
          "surname": "Kim",
          "name": "Daeyoung Kim",
          "email": ""
        },
        {
          "forename": "Sanghyuk",
          "surname": "Choi",
          "name": "Sanghyuk Choi",
          "email": ""
        }
      ],
      "doi": "arXiv:2407.06551",
      "venue": "Offsetbias: Leveraging debiased data for tuning evaluators",
      "date": "2024"
    },
    {
      "index": "b175",
      "title": "AIME: AI System Optimization via Multiple LLM Evaluators",
      "author": [
        {
          "forename": "Bhrij",
          "surname": "Patel",
          "name": "Bhrij Patel",
          "email": ""
        },
        {
          "forename": "Souradip",
          "surname": "Chakraborty",
          "name": "Souradip Chakraborty",
          "email": ""
        },
        {
          "forename": "A.",
          "surname": "Wesley",
          "name": "A. Wesley",
          "email": ""
        },
        {
          "forename": "Mengdi",
          "surname": "Suttle",
          "name": "Mengdi Suttle",
          "email": ""
        },
        {
          "forename": "Amrit Singh ",
          "surname": "Wang",
          "name": "Amrit Singh  Wang",
          "email": ""
        },
        {
          "forename": "Dinesh",
          "surname": "Bedi",
          "name": "Dinesh Bedi",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.03131",
      "venue": "AIME: AI System Optimization via Multiple LLM Evaluators",
      "date": "2024"
    },
    {
      "index": "b176",
      "title": "Refiner: Reasoning feedback on intermediate representations",
      "author": [
        {
          "forename": "Debjit",
          "surname": "Paul",
          "name": "Debjit Paul",
          "email": ""
        },
        {
          "forename": "Mete",
          "surname": "Ismayilzada",
          "name": "Mete Ismayilzada",
          "email": ""
        },
        {
          "forename": "Maxime",
          "surname": "Peyrard",
          "name": "Maxime Peyrard",
          "email": ""
        },
        {
          "forename": "Beatriz",
          "surname": "Borges",
          "name": "Beatriz Borges",
          "email": ""
        },
        {
          "forename": "Antoine",
          "surname": "Bosselut",
          "name": "Antoine Bosselut",
          "email": ""
        },
        {
          "forename": "Robert",
          "surname": "West",
          "name": "Robert West",
          "email": ""
        },
        {
          "forename": "Boi",
          "surname": "Faltings",
          "name": "Boi Faltings",
          "email": ""
        }
      ],
      "doi": "arXiv:2304.01904",
      "venue": "Refiner: Reasoning feedback on intermediate representations",
      "date": "2023"
    },
    {
      "index": "b177",
      "title": "Ignore previous prompt: Attack techniques for language models",
      "author": [
        {
          "forename": "Fábio",
          "surname": "Perez",
          "name": "Fábio Perez",
          "email": ""
        },
        {
          "forename": "Ian",
          "surname": "Ribeiro",
          "name": "Ian Ribeiro",
          "email": ""
        }
      ],
      "doi": "arXiv:2211.09527",
      "venue": "Ignore previous prompt: Attack techniques for language models",
      "date": "2022"
    },
    {
      "index": "b178",
      "title": "Large language models sensitivity to the order of options in multiple-choice questions",
      "author": [
        {
          "forename": "Pouya",
          "surname": "Pezeshkpour",
          "name": "Pouya Pezeshkpour",
          "email": ""
        },
        {
          "forename": "Estevam",
          "surname": "Hruschka",
          "name": "Estevam Hruschka",
          "email": ""
        }
      ],
      "doi": "arXiv:2308.11483",
      "venue": "Large language models sensitivity to the order of options in multiple-choice questions",
      "date": "2023"
    },
    {
      "index": "b179",
      "title": "Bias patterns in the application of LLMs for clinical decision support: A comprehensive study",
      "author": [
        {
          "forename": "Raphael",
          "surname": "Poulain",
          "name": "Raphael Poulain",
          "email": ""
        },
        {
          "forename": "Hamed",
          "surname": "Fayyaz",
          "name": "Hamed Fayyaz",
          "email": ""
        },
        {
          "forename": "Rahmatollah",
          "surname": "Beheshti",
          "name": "Rahmatollah Beheshti",
          "email": ""
        }
      ],
      "doi": "arXiv:2404.15149",
      "venue": "Bias patterns in the application of LLMs for clinical decision support: A comprehensive study",
      "date": "2024"
    },
    {
      "index": "b180",
      "title": "Large language models are effective text rankers with pairwise ranking prompting",
      "author": [
        {
          "forename": "Zhen",
          "surname": "Qin",
          "name": "Zhen Qin",
          "email": ""
        },
        {
          "forename": "Rolf",
          "surname": "Jagerman",
          "name": "Rolf Jagerman",
          "email": ""
        },
        {
          "forename": "Kai",
          "surname": "Hui",
          "name": "Kai Hui",
          "email": ""
        },
        {
          "forename": "Honglei",
          "surname": "Zhuang",
          "name": "Honglei Zhuang",
          "email": ""
        },
        {
          "forename": "Junru",
          "surname": "Wu",
          "name": "Junru Wu",
          "email": ""
        },
        {
          "forename": "Le",
          "surname": "Yan",
          "name": "Le Yan",
          "email": ""
        },
        {
          "forename": "Jiaming",
          "surname": "Shen",
          "name": "Jiaming Shen",
          "email": ""
        },
        {
          "forename": "Tianqi",
          "surname": "Liu",
          "name": "Tianqi Liu",
          "email": ""
        },
        {
          "forename": "Jialu",
          "surname": "Liu",
          "name": "Jialu Liu",
          "email": ""
        },
        {
          "forename": "Donald",
          "surname": "Metzler",
          "name": "Donald Metzler",
          "email": ""
        }
      ],
      "doi": "arXiv:2306.17563",
      "venue": "Large language models are effective text rankers with pairwise ranking prompting",
      "date": "2023"
    },
    {
      "index": "b181",
      "title": "Direct preference optimization: Your language model is secretly a reward model",
      "author": [
        {
          "forename": "Rafael",
          "surname": "Rafailov",
          "name": "Rafael Rafailov",
          "email": ""
        },
        {
          "forename": "Archit",
          "surname": "Sharma",
          "name": "Archit Sharma",
          "email": ""
        },
        {
          "forename": "Eric",
          "surname": "Mitchell",
          "name": "Eric Mitchell",
          "email": ""
        },
        {
          "forename": "D.",
          "surname": "Christopher",
          "name": "D. Christopher",
          "email": ""
        },
        {
          "forename": "Stefano",
          "surname": "Manning",
          "name": "Stefano Manning",
          "email": ""
        },
        {
          "forename": "Chelsea",
          "surname": "Ermon",
          "name": "Chelsea Ermon",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Advances in Neural Information Processing Systems",
      "date": "2024"
    },
    {
      "index": "b182",
      "title": "Center-of-inattention: Position biases in decision-making",
      "author": [
        {
          "forename": "Priya",
          "surname": "Raghubir",
          "name": "Priya Raghubir",
          "email": ""
        },
        {
          "forename": "Ana",
          "surname": "Valenzuela",
          "name": "Ana Valenzuela",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Organizational Behavior and Human Decision Processes",
      "date": "2006"
    },
    {
      "index": "b183",
      "title": "LLMJudge: LLMs for Relevance Judgments",
      "author": [
        {
          "forename": "A.",
          "surname": "Hossein",
          "name": "A. Hossein",
          "email": ""
        },
        {
          "forename": "Emine",
          "surname": "Rahmani",
          "name": "Emine Rahmani",
          "email": ""
        },
        {
          "forename": "Nick",
          "surname": "Yilmaz",
          "name": "Nick Yilmaz",
          "email": ""
        },
        {
          "forename": "Bhaskar",
          "surname": "Craswell",
          "name": "Bhaskar Craswell",
          "email": ""
        },
        {
          "forename": "Paul",
          "surname": "Mitra",
          "name": "Paul Mitra",
          "email": ""
        },
        {
          "forename": "L.A.",
          "surname": "Charles",
          "name": "L.A. Charles",
          "email": ""
        },
        {
          "forename": "Mohammad",
          "surname": "Clarke",
          "name": "Mohammad Clarke",
          "email": ""
        },
        {
          "forename": "Clemencia",
          "surname": "Aliannejadi",
          "name": "Clemencia Aliannejadi",
          "email": ""
        },
        {
          "forename": "Guglielmo",
          "surname": "Siro",
          "name": "Guglielmo Siro",
          "email": ""
        }
      ],
      "doi": "arXiv:2408.08896",
      "venue": "LLMJudge: LLMs for Relevance Judgments",
      "date": "2024"
    },
    {
      "index": "b184",
      "title": "Adian Liusie, and Mark Gales. 2024. Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment",
      "author": [
        {
          "forename": "Raina",
          "surname": "Vyas",
          "name": "Raina Vyas",
          "email": ""
        }
      ],
      "doi": "arXiv:2402.14016",
      "venue": "Adian Liusie, and Mark Gales. 2024. Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment",
      "date": "2024"
    },
    {
      "index": "b185",
      "title": "Constructing domain-specific evaluation sets for llm-as-a-judge",
      "author": [
        {
          "forename": "Ravi",
          "surname": "Raju",
          "name": "Ravi Raju",
          "email": ""
        },
        {
          "forename": "Swayambhoo",
          "surname": "Jain",
          "name": "Swayambhoo Jain",
          "email": ""
        },
        {
          "forename": "Bo",
          "surname": "Li",
          "name": "Bo Li",
          "email": ""
        },
        {
          "forename": "Jonathan",
          "surname": "Li",
          "name": "Jonathan Li",
          "email": ""
        },
        {
          "forename": "Urmish",
          "surname": "Thakker",
          "name": "Urmish Thakker",
          "email": ""
        }
      ],
      "doi": "arXiv:2408.08808",
      "venue": "Constructing domain-specific evaluation sets for llm-as-a-judge",
      "date": "2024"
    },
    {
      "index": "b186",
      "title": "Self-evaluation improves selective generation in large language models",
      "author": [
        {
          "forename": "Jie",
          "surname": "Ren",
          "name": "Jie Ren",
          "email": ""
        },
        {
          "forename": "Yao",
          "surname": "Zhao",
          "name": "Yao Zhao",
          "email": ""
        },
        {
          "forename": "Tu",
          "surname": "Vu",
          "name": "Tu Vu",
          "email": ""
        },
        {
          "forename": "J.",
          "surname": "Peter",
          "name": "J. Peter",
          "email": ""
        },
        {
          "forename": "Balaji",
          "surname": "Liu",
          "name": "Balaji Liu",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings on. PMLR",
      "date": "2023"
    },
    {
      "index": "b187",
      "title": "Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA",
      "author": [
        {
          "forename": "Cheol",
          "surname": "Ryu",
          "name": "Cheol Ryu",
          "email": ""
        },
        {
          "forename": "Seolhwa",
          "surname": "Lee",
          "name": "Seolhwa Lee",
          "email": ""
        },
        {
          "forename": "Subeen",
          "surname": "Pang",
          "name": "Subeen Pang",
          "email": ""
        },
        {
          "forename": "Chanyeol",
          "surname": "Choi",
          "name": "Chanyeol Choi",
          "email": ""
        },
        {
          "forename": "Hojun",
          "surname": "Choi",
          "name": "Hojun Choi",
          "email": ""
        },
        {
          "forename": "Myeonggee",
          "surname": "Min",
          "name": "Myeonggee Min",
          "email": ""
        },
        {
          "forename": "Jy-Yong",
          "surname": "Sohn",
          "name": "Jy-Yong Sohn",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the Natural Legal Language Processing Workshop 2023",
      "date": "2023"
    },
    {
      "index": "b188",
      "title": "Ares: An automated evaluation framework for retrieval-augmented generation systems",
      "author": [
        {
          "forename": "Jon",
          "surname": "Saad-Falcon",
          "name": "Jon Saad-Falcon",
          "email": ""
        },
        {
          "forename": "Omar",
          "surname": "Khattab",
          "name": "Omar Khattab",
          "email": ""
        },
        {
          "forename": "Christopher",
          "surname": "Potts",
          "name": "Christopher Potts",
          "email": ""
        },
        {
          "forename": "Matei",
          "surname": "Zaharia",
          "name": "Matei Zaharia",
          "email": ""
        }
      ],
      "doi": "arXiv:2311.09476",
      "venue": "Ares: An automated evaluation framework for retrieval-augmented generation systems",
      "date": "2023"
    },
    {
      "index": "b189",
      "title": "Samrat Mondal, and Aman Chadha. 2024. A systematic survey of prompt engineering in large language models",
      "author": [
        {
          "forename": "Pranab",
          "surname": "Sahoo",
          "name": "Pranab Sahoo",
          "email": ""
        },
        {
          "forename": "Ayush Kumar ",
          "surname": "Singh",
          "name": "Ayush Kumar  Singh",
          "email": ""
        },
        {
          "forename": "Sriparna",
          "surname": "Saha",
          "name": "Sriparna Saha",
          "email": ""
        },
        {
          "forename": "Vinija",
          "surname": "Jain",
          "name": "Vinija Jain",
          "email": ""
        }
      ],
      "doi": "arXiv:2402.07927",
      "venue": "Samrat Mondal, and Aman Chadha. 2024. A systematic survey of prompt engineering in large language models",
      "date": "2024"
    },
    {
      "index": "b190",
      "title": "Spearman's rank correlation coefficient",
      "author": [
        {
          "forename": "Philip",
          "surname": "Sedgwick",
          "name": "Philip Sedgwick",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Bmj",
      "date": "2014"
    },
    {
      "index": "b191",
      "title": "Estimates of the regression coefficient based on Kendall's tau",
      "author": [
        {
          "forename": "Pranab",
          "surname": "Kumar Sen",
          "name": "Pranab Kumar Sen",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Journal of the American statistical association",
      "date": "1968"
    },
    {
      "index": "b192",
      "title": "Who validates the validators? aligning llm-assisted evaluation of llm outputs with human preferences",
      "author": [
        {
          "forename": "Shreya",
          "surname": "Shankar",
          "name": "Shreya Shankar",
          "email": ""
        },
        {
          "forename": "Björn",
          "surname": "Zamfirescu-Pereira",
          "name": "Björn Zamfirescu-Pereira",
          "email": ""
        },
        {
          "forename": "Aditya",
          "surname": "Hartmann",
          "name": "Aditya Hartmann",
          "email": ""
        },
        {
          "forename": "Ian",
          "surname": "Parameswaran",
          "name": "Ian Parameswaran",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the 37th",
      "date": "2024-12"
    },
    {
      "index": "b195",
      "title": "Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
      "author": [
        {
          "forename": "Xinyue",
          "surname": "Shen",
          "name": "Xinyue Shen",
          "email": ""
        },
        {
          "forename": "Zeyuan",
          "surname": "Chen",
          "name": "Zeyuan Chen",
          "email": ""
        },
        {
          "forename": "Michael",
          "surname": "Backes",
          "name": "Michael Backes",
          "email": ""
        },
        {
          "forename": "Yun",
          "surname": "Shen",
          "name": "Yun Shen",
          "email": ""
        },
        {
          "forename": "Yang",
          "surname": "Zhang",
          "name": "Yang Zhang",
          "email": ""
        }
      ],
      "doi": "arXiv:2308.03825",
      "venue": "Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
      "date": "2023"
    },
    {
      "index": "b196",
      "title": "Opinsummeval: Revisiting automated evaluation for opinion summarization",
      "author": [
        {
          "forename": "Yuchen",
          "surname": "Shen",
          "name": "Yuchen Shen",
          "email": ""
        },
        {
          "forename": "Xiaojun",
          "surname": "Wan",
          "name": "Xiaojun Wan",
          "email": ""
        }
      ],
      "doi": "arXiv:2310.18122",
      "venue": "Opinsummeval: Revisiting automated evaluation for opinion summarization",
      "date": "2023"
    },
    {
      "index": "b197",
      "title": "Large language models can be easily distracted by irrelevant context",
      "author": [
        {
          "forename": "Freda",
          "surname": "Shi",
          "name": "Freda Shi",
          "email": ""
        },
        {
          "forename": "Xinyun",
          "surname": "Chen",
          "name": "Xinyun Chen",
          "email": ""
        },
        {
          "forename": "Kanishka",
          "surname": "Misra",
          "name": "Kanishka Misra",
          "email": ""
        },
        {
          "forename": "Nathan",
          "surname": "Scales",
          "name": "Nathan Scales",
          "email": ""
        },
        {
          "forename": "David",
          "surname": "Dohan",
          "name": "David Dohan",
          "email": ""
        },
        {
          "forename": "Ed H.",
          "surname": "Chi",
          "name": "Ed H. Chi",
          "email": ""
        },
        {
          "forename": "Nathanael",
          "surname": "Schärli",
          "name": "Nathanael Schärli",
          "email": ""
        },
        {
          "forename": "Denny",
          "surname": "Zhou",
          "name": "Denny Zhou",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "International Conference on Machine Learning. PMLR",
      "date": "2023"
    },
    {
      "index": "b198",
      "title": "Optimizationbased Prompt Injection Attack to LLM-as-a-Judge",
      "author": [
        {
          "forename": "Jiawen",
          "surname": "Shi",
          "name": "Jiawen Shi",
          "email": ""
        },
        {
          "forename": "Zenghui",
          "surname": "Yuan",
          "name": "Zenghui Yuan",
          "email": ""
        },
        {
          "forename": "Yinuo",
          "surname": "Liu",
          "name": "Yinuo Liu",
          "email": ""
        },
        {
          "forename": "Yue",
          "surname": "Huang",
          "name": "Yue Huang",
          "email": ""
        },
        {
          "forename": "Pan",
          "surname": "Zhou",
          "name": "Pan Zhou",
          "email": ""
        },
        {
          "forename": "Lichao",
          "surname": "Sun",
          "name": "Lichao Sun",
          "email": ""
        },
        {
          "forename": "Neil Zhenqiang",
          "surname": "Gong",
          "name": "Neil Zhenqiang Gong",
          "email": ""
        }
      ],
      "doi": "arXiv:2403.17710",
      "venue": "Optimizationbased Prompt Injection Attack to LLM-as-a-Judge",
      "date": "2024"
    },
    {
      "index": "b199",
      "title": "Judging the judges: A systematic investigation of position bias in pairwise comparative assessments by llms",
      "author": [
        {
          "forename": "Lin",
          "surname": "Shi",
          "name": "Lin Shi",
          "email": ""
        },
        {
          "forename": "Chiyu",
          "surname": "Ma",
          "name": "Chiyu Ma",
          "email": ""
        },
        {
          "forename": "Wenhua",
          "surname": "Liang",
          "name": "Wenhua Liang",
          "email": ""
        },
        {
          "forename": "Weicheng",
          "surname": "Ma",
          "name": "Weicheng Ma",
          "email": ""
        },
        {
          "forename": "Soroush",
          "surname": "Vosoughi",
          "name": "Soroush Vosoughi",
          "email": ""
        }
      ],
      "doi": "arXiv:2406.07791",
      "venue": "Judging the judges: A systematic investigation of position bias in pairwise comparative assessments by llms",
      "date": "2024"
    },
    {
      "index": "b200",
      "title": "Fusion-Eval: Integrating Assistant Evaluators with LLMs",
      "author": [
        {
          "forename": "Lei",
          "surname": "Shu",
          "name": "Lei Shu",
          "email": ""
        },
        {
          "forename": "Nevan",
          "surname": "Wichers",
          "name": "Nevan Wichers",
          "email": ""
        },
        {
          "forename": "Liangchen",
          "surname": "Luo",
          "name": "Liangchen Luo",
          "email": ""
        },
        {
          "forename": "Yun",
          "surname": "Zhu",
          "name": "Yun Zhu",
          "email": ""
        },
        {
          "forename": "Yinxiao",
          "surname": "Liu",
          "name": "Yinxiao Liu",
          "email": ""
        },
        {
          "forename": "Jindong",
          "surname": "Chen",
          "name": "Jindong Chen",
          "email": ""
        },
        {
          "forename": "Lei",
          "surname": "Meng",
          "name": "Lei Meng",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track",
      "date": "2024"
    },
    {
      "index": "b201",
      "title": "Beyond human data: Scaling self-training for problem-solving with language models",
      "author": [
        {
          "forename": "Avi",
          "surname": "Singh",
          "name": "Avi Singh",
          "email": ""
        },
        {
          "forename": "John D.",
          "surname": "Co-Reyes",
          "name": "John D. Co-Reyes",
          "email": ""
        },
        {
          "forename": "Rishabh",
          "surname": "Agarwal",
          "name": "Rishabh Agarwal",
          "email": ""
        },
        {
          "forename": "Ankesh",
          "surname": "Anand",
          "name": "Ankesh Anand",
          "email": ""
        },
        {
          "forename": "Piyush",
          "surname": "Patil",
          "name": "Piyush Patil",
          "email": ""
        },
        {
          "forename": "Xavier",
          "surname": "Garcia",
          "name": "Xavier Garcia",
          "email": ""
        },
        {
          "forename": "J.",
          "surname": "Peter",
          "name": "J. Peter",
          "email": ""
        },
        {
          "forename": "James",
          "surname": "Liu",
          "name": "James Liu",
          "email": ""
        },
        {
          "forename": "Jaehoon",
          "surname": "Harrison",
          "name": "Jaehoon Harrison",
          "email": ""
        },
        {
          "forename": "Kelvin",
          "surname": "Lee",
          "name": "Kelvin Lee",
          "email": ""
        }
      ],
      "doi": "arXiv:2312.06585",
      "venue": "Beyond human data: Scaling self-training for problem-solving with language models",
      "date": "2023"
    },
    {
      "index": "b202",
      "title": "2024. Don't Use LLMs to Make Relevance Judgments",
      "author": [
        {
          "forename": "Ian",
          "surname": "Soboroff",
          "name": "Ian Soboroff",
          "email": ""
        }
      ],
      "doi": "arXiv:2409.15133",
      "venue": "2024. Don't Use LLMs to Make Relevance Judgments",
      "date": "2024"
    },
    {
      "index": "b203",
      "title": "KRX Bench: Automating Financial Benchmark Creation via Large Language Models",
      "author": [
        {
          "forename": "Guijin",
          "surname": "Son",
          "name": "Guijin Son",
          "email": ""
        },
        {
          "forename": "Hyunjun",
          "surname": "Jeon",
          "name": "Hyunjun Jeon",
          "email": ""
        },
        {
          "forename": "Chami",
          "surname": "Hwang",
          "name": "Chami Hwang",
          "email": ""
        },
        {
          "forename": "Hanearl",
          "surname": "Jung",
          "name": "Hanearl Jung",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the Joint Workshop of the 7th Financial Technology and Natural Language Processing, the 5th Knowledge Discovery from Unstructured Data in Financial Services, and the 4th Workshop on Economics and Natural Language Processing@ LREC-COLING 2024",
      "date": "2024"
    },
    {
      "index": "b204",
      "title": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models",
      "author": [
        {
          "forename": "Guijin",
          "surname": "Son",
          "name": "Guijin Son",
          "email": ""
        },
        {
          "forename": "Dongkeun",
          "surname": "Yoon",
          "name": "Dongkeun Yoon",
          "email": ""
        },
        {
          "forename": "Juyoung",
          "surname": "Suk",
          "name": "Juyoung Suk",
          "email": ""
        },
        {
          "forename": "Javier",
          "surname": "Aula-Blasco",
          "name": "Javier Aula-Blasco",
          "email": ""
        },
        {
          "forename": "Mano",
          "surname": "Aslan",
          "name": "Mano Aslan",
          "email": ""
        },
        {
          "forename": "Jaume",
          "surname": "Islam",
          "name": "Jaume Islam",
          "email": ""
        },
        {
          "forename": "Lucía",
          "surname": "Prats-Cristià",
          "name": "Lucía Prats-Cristià",
          "email": ""
        },
        {
          "forename": "Seungone",
          "surname": "Tormo-Bañuelos",
          "name": "Seungone Tormo-Bañuelos",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.17578",
      "venue": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models",
      "date": "2024"
    },
    {
      "index": "b205",
      "title": "FineSurE: Fine-grained summarization evaluation using LLMs",
      "author": [
        {
          "forename": "Hwanjun",
          "surname": "Song",
          "name": "Hwanjun Song",
          "email": ""
        },
        {
          "forename": "Hang",
          "surname": "Su",
          "name": "Hang Su",
          "email": ""
        },
        {
          "forename": "Igor",
          "surname": "Shalyminov",
          "name": "Igor Shalyminov",
          "email": ""
        },
        {
          "forename": "Jason",
          "surname": "Cai",
          "name": "Jason Cai",
          "email": ""
        },
        {
          "forename": "Saab",
          "surname": "Mansour",
          "name": "Saab Mansour",
          "email": ""
        }
      ],
      "doi": "arXiv:2407.00908",
      "venue": "FineSurE: Fine-grained summarization evaluation using LLMs",
      "date": "2024"
    },
    {
      "index": "b206",
      "title": "Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More",
      "author": [
        {
          "forename": "Mingyang",
          "surname": "Song",
          "name": "Mingyang Song",
          "email": ""
        },
        {
          "forename": "Mao",
          "surname": "Zheng",
          "name": "Mao Zheng",
          "email": ""
        },
        {
          "forename": "Xuan",
          "surname": "Luo",
          "name": "Xuan Luo",
          "email": ""
        }
      ],
      "doi": "arXiv:2406.11629",
      "venue": "Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More",
      "date": "2024"
    },
    {
      "index": "b207",
      "title": "Automated Essay Scoring and Revising Based on Open-Source Large Language Models",
      "author": [
        {
          "forename": "Yishen",
          "surname": "Song",
          "name": "Yishen Song",
          "email": ""
        },
        {
          "forename": "Qianta",
          "surname": "Zhu",
          "name": "Qianta Zhu",
          "email": ""
        },
        {
          "forename": "Huaibo",
          "surname": "Wang",
          "name": "Huaibo Wang",
          "email": ""
        },
        {
          "forename": "Qinhua",
          "surname": "Zheng",
          "name": "Qinhua Zheng",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "IEEE Transactions on Learning Technologies",
      "date": "2024"
    },
    {
      "index": "b208",
      "title": "Evaluation metrics in the era of GPT-4: reliably evaluating large language models on sequence to sequence tasks",
      "author": [
        {
          "forename": "Andrea",
          "surname": "Sottana",
          "name": "Andrea Sottana",
          "email": ""
        },
        {
          "forename": "Bin",
          "surname": "Liang",
          "name": "Bin Liang",
          "email": ""
        },
        {
          "forename": "Kai",
          "surname": "Zou",
          "name": "Kai Zou",
          "email": ""
        },
        {
          "forename": "Zheng",
          "surname": "Yuan",
          "name": "Zheng Yuan",
          "email": ""
        }
      ],
      "doi": "arXiv:2310.13800",
      "venue": "Evaluation metrics in the era of GPT-4: reliably evaluating large language models on sequence to sequence tasks",
      "date": "2023"
    },
    {
      "index": "b209",
      "title": "From calculation to adjudication: Examining llm judges on mathematical reasoning tasks",
      "author": [
        {
          "forename": "Andreas",
          "surname": "Stephan",
          "name": "Andreas Stephan",
          "email": ""
        },
        {
          "forename": "Dawei",
          "surname": "Zhu",
          "name": "Dawei Zhu",
          "email": ""
        },
        {
          "forename": "Matthias",
          "surname": "Aßenmacher",
          "name": "Matthias Aßenmacher",
          "email": ""
        },
        {
          "forename": "Xiaoyu",
          "surname": "Shen",
          "name": "Xiaoyu Shen",
          "email": ""
        },
        {
          "forename": "Benjamin",
          "surname": "Roth",
          "name": "Benjamin Roth",
          "email": ""
        }
      ],
      "doi": "arXiv:2409.04168",
      "venue": "From calculation to adjudication: Examining llm judges on mathematical reasoning tasks",
      "date": "2024"
    },
    {
      "index": "b210",
      "title": "Large language models are inconsistent and biased evaluators",
      "author": [
        {
          "forename": "Rickard",
          "surname": "Stureborg",
          "name": "Rickard Stureborg",
          "email": ""
        },
        {
          "forename": "Dimitris",
          "surname": "Alikaniotis",
          "name": "Dimitris Alikaniotis",
          "email": ""
        },
        {
          "forename": "Yoshi",
          "surname": "Suhara",
          "name": "Yoshi Suhara",
          "email": ""
        }
      ],
      "doi": "arXiv:2405.01724",
      "venue": "Large language models are inconsistent and biased evaluators",
      "date": "2024"
    },
    {
      "index": "b211",
      "title": "Fast Best-of-N Decoding via Speculative Rejection",
      "author": [
        {
          "forename": "Hanshi",
          "surname": "Sun",
          "name": "Hanshi Sun",
          "email": ""
        },
        {
          "forename": "Momin",
          "surname": "Haider",
          "name": "Momin Haider",
          "email": ""
        },
        {
          "forename": "Ruiqi",
          "surname": "Zhang",
          "name": "Ruiqi Zhang",
          "email": ""
        },
        {
          "forename": "Huitao",
          "surname": "Yang",
          "name": "Huitao Yang",
          "email": ""
        },
        {
          "forename": "Jiahao",
          "surname": "Qiu",
          "name": "Jiahao Qiu",
          "email": ""
        },
        {
          "forename": "Ming",
          "surname": "Yin",
          "name": "Ming Yin",
          "email": ""
        },
        {
          "forename": "Mengdi",
          "surname": "Wang",
          "name": "Mengdi Wang",
          "email": ""
        },
        {
          "forename": "Peter",
          "surname": "Bartlett",
          "name": "Peter Bartlett",
          "email": ""
        },
        {
          "forename": "Andrea",
          "surname": "Zanette",
          "name": "Andrea Zanette",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.20290",
      "venue": "Fast Best-of-N Decoding via Speculative Rejection",
      "date": "2024"
    },
    {
      "index": "b212",
      "title": "2020. Natural backdoor attack on text data",
      "author": [],
      "doi": "arXiv:2006.16176",
      "venue": "2020. Natural backdoor attack on text data",
      "date": "2020"
    },
    {
      "index": "b213",
      "title": "Adv-bert: Bert is not robust on misspellings! generating nature adversarial samples on bert",
      "author": [
        {
          "forename": "Lichao",
          "surname": "Sun",
          "name": "Lichao Sun",
          "email": ""
        },
        {
          "forename": "Kazuma",
          "surname": "Hashimoto",
          "name": "Kazuma Hashimoto",
          "email": ""
        },
        {
          "forename": "Wenpeng",
          "surname": "Yin",
          "name": "Wenpeng Yin",
          "email": ""
        },
        {
          "forename": "Akari",
          "surname": "Asai",
          "name": "Akari Asai",
          "email": ""
        },
        {
          "forename": "Jia",
          "surname": "Li",
          "name": "Jia Li",
          "email": ""
        },
        {
          "forename": "Philip",
          "surname": "Yu",
          "name": "Philip Yu",
          "email": ""
        },
        {
          "forename": "Caiming",
          "surname": "Xiong",
          "name": "Caiming Xiong",
          "email": ""
        }
      ],
      "doi": "arXiv:2003.04985",
      "venue": "Adv-bert: Bert is not robust on misspellings! generating nature adversarial samples on bert",
      "date": "2020"
    },
    {
      "index": "b214",
      "title": "Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks",
      "author": [
        {
          "forename": "Annalisa",
          "surname": "Szymanski",
          "name": "Annalisa Szymanski",
          "email": ""
        },
        {
          "forename": "Noah",
          "surname": "Ziems",
          "name": "Noah Ziems",
          "email": ""
        },
        {
          "forename": "A.",
          "surname": "Heather",
          "name": "A. Heather",
          "email": ""
        },
        {
          "forename": "Toby Jia-Jun",
          "surname": "Eicher-Miller",
          "name": "Toby Jia-Jun Eicher-Miller",
          "email": ""
        },
        {
          "forename": "Meng",
          "surname": "Li",
          "name": "Meng Li",
          "email": ""
        },
        {
          "forename": "Ronald A.",
          "surname": "Jiang",
          "name": "Ronald A. Jiang",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.20266",
      "venue": "Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks",
      "date": "2024"
    },
    {
      "index": "b215",
      "title": "Raluca Ada Popa, and Ion Stoica. 2024. JudgeBench: A Benchmark for Evaluating LLM-based Judges",
      "author": [
        {
          "forename": "Sijun",
          "surname": "Tan",
          "name": "Sijun Tan",
          "email": ""
        },
        {
          "forename": "Siyuan",
          "surname": "Zhuang",
          "name": "Siyuan Zhuang",
          "email": ""
        },
        {
          "forename": "Kyle",
          "surname": "Montgomery",
          "name": "Kyle Montgomery",
          "email": ""
        },
        {
          "forename": "Y.",
          "surname": "William",
          "name": "Y. William",
          "email": ""
        },
        {
          "forename": "Alejandro",
          "surname": "Tang",
          "name": "Alejandro Tang",
          "email": ""
        },
        {
          "forename": "Chenguang",
          "surname": "Cuadron",
          "name": "Chenguang Cuadron",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.12784",
      "venue": "Raluca Ada Popa, and Ion Stoica. 2024. JudgeBench: A Benchmark for Evaluating LLM-based Judges",
      "date": "2024"
    },
    {
      "index": "b216",
      "title": "AI can help humans find common ground in democratic deliberation",
      "author": [
        {
          "forename": "Michael Henry ",
          "surname": "Tessler",
          "name": "Michael Henry  Tessler",
          "email": ""
        },
        {
          "forename": "A.",
          "surname": "Michiel",
          "name": "A. Michiel",
          "email": ""
        },
        {
          "forename": "Daniel",
          "surname": "Bakker",
          "name": "Daniel Bakker",
          "email": ""
        },
        {
          "forename": "Hannah",
          "surname": "Jarrett",
          "name": "Hannah Jarrett",
          "email": ""
        },
        {
          "forename": "J.",
          "surname": "Martin",
          "name": "J. Martin",
          "email": ""
        },
        {
          "forename": "Raphael",
          "surname": "Chadwick",
          "name": "Raphael Chadwick",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Science",
      "date": "2024"
    },
    {
      "index": "b217",
      "title": "Venkat Srinik Ramayapally, Sankaran Vaidyanathan, and Dieuwke Hupkes. 2024. Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges",
      "author": [
        {
          "forename": "Aman",
          "surname": "Singh Thakur",
          "name": "Aman Singh Thakur",
          "email": ""
        },
        {
          "forename": "Kartik",
          "surname": "Choudhary",
          "name": "Kartik Choudhary",
          "email": ""
        }
      ],
      "doi": "arXiv:2406.12624",
      "venue": "Venkat Srinik Ramayapally, Sankaran Vaidyanathan, and Dieuwke Hupkes. 2024. Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges",
      "date": "2024"
    },
    {
      "index": "b218",
      "title": "A comprehensive survey of hallucination mitigation techniques in large language models",
      "author": [
        {
          "forename": "Vinija",
          "surname": "Zaman",
          "name": "Vinija Zaman",
          "email": ""
        },
        {
          "forename": "Anku",
          "surname": "Jain",
          "name": "Anku Jain",
          "email": ""
        },
        {
          "forename": "Vipula",
          "surname": "Rani",
          "name": "Vipula Rani",
          "email": ""
        },
        {
          "forename": "Aman",
          "surname": "Rawte",
          "name": "Aman Rawte",
          "email": ""
        },
        {
          "forename": "Amitava",
          "surname": "Chadha",
          "name": "Amitava Chadha",
          "email": ""
        }
      ],
      "doi": "arXiv:2401.01313",
      "venue": "A comprehensive survey of hallucination mitigation techniques in large language models",
      "date": "2024"
    },
    {
      "index": "b219",
      "title": "Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning",
      "author": [
        {
          "forename": "Petter",
          "surname": "Törnberg",
          "name": "Petter Törnberg",
          "email": ""
        }
      ],
      "doi": "arXiv:2304.06588",
      "venue": "Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning",
      "date": "2023"
    },
    {
      "index": "b220",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "author": [
        {
          "forename": "Hugo",
          "surname": "Touvron",
          "name": "Hugo Touvron",
          "email": ""
        },
        {
          "forename": "Louis",
          "surname": "Martin",
          "name": "Louis Martin",
          "email": ""
        },
        {
          "forename": "Kevin",
          "surname": "Stone",
          "name": "Kevin Stone",
          "email": ""
        },
        {
          "forename": "Peter",
          "surname": "Albert",
          "name": "Peter Albert",
          "email": ""
        },
        {
          "forename": "Amjad",
          "surname": "Almahairi",
          "name": "Amjad Almahairi",
          "email": ""
        },
        {
          "forename": "Yasmine",
          "surname": "Babaei",
          "name": "Yasmine Babaei",
          "email": ""
        },
        {
          "forename": "Nikolay",
          "surname": "Bashlykov",
          "name": "Nikolay Bashlykov",
          "email": ""
        },
        {
          "forename": "Soumya",
          "surname": "Batra",
          "name": "Soumya Batra",
          "email": ""
        },
        {
          "forename": "Prajjwal",
          "surname": "Bhargava",
          "name": "Prajjwal Bhargava",
          "email": ""
        },
        {
          "forename": "Shruti",
          "surname": "Bhosale",
          "name": "Shruti Bhosale",
          "email": ""
        }
      ],
      "doi": "arXiv:2307.09288",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "date": "2023"
    },
    {
      "index": "b221",
      "title": "Appworld: A controllable world of apps and people for benchmarking interactive coding agents",
      "author": [
        {
          "forename": "Harsh",
          "surname": "Trivedi",
          "name": "Harsh Trivedi",
          "email": ""
        },
        {
          "forename": "Tushar",
          "surname": "Khot",
          "name": "Tushar Khot",
          "email": ""
        },
        {
          "forename": "Mareike",
          "surname": "Hartmann",
          "name": "Mareike Hartmann",
          "email": ""
        },
        {
          "forename": "Ruskin",
          "surname": "Manku",
          "name": "Ruskin Manku",
          "email": ""
        },
        {
          "forename": "Vinty",
          "surname": "Dong",
          "name": "Vinty Dong",
          "email": ""
        },
        {
          "forename": "Edward",
          "surname": "Li",
          "name": "Edward Li",
          "email": ""
        },
        {
          "forename": "Shashank",
          "surname": "Gupta",
          "name": "Shashank Gupta",
          "email": ""
        },
        {
          "forename": "Ashish",
          "surname": "Sabharwal",
          "name": "Ashish Sabharwal",
          "email": ""
        },
        {
          "forename": "Niranjan",
          "surname": "Balasubramanian",
          "name": "Niranjan Balasubramanian",
          "email": ""
        }
      ],
      "doi": "arXiv:2407.18901",
      "venue": "Appworld: A controllable world of apps and people for benchmarking interactive coding agents",
      "date": "2024"
    },
    {
      "index": "b222",
      "title": "Self-rationalization improves LLM as a fine-grained judge",
      "author": [
        {
          "forename": "Prapti",
          "surname": "Trivedi",
          "name": "Prapti Trivedi",
          "email": ""
        },
        {
          "forename": "Aditya",
          "surname": "Gulati",
          "name": "Aditya Gulati",
          "email": ""
        },
        {
          "forename": "Oliver",
          "surname": "Molenschot",
          "name": "Oliver Molenschot",
          "email": ""
        },
        {
          "forename": "Rajkumar",
          "surname": "Meghana Arakkal Rajeev",
          "name": "Rajkumar Meghana Arakkal Rajeev",
          "email": ""
        },
        {
          "forename": "Keith",
          "surname": "Ramamurthy",
          "name": "Keith Ramamurthy",
          "email": ""
        },
        {
          "forename": "Tanveesh",
          "surname": "Stevens",
          "name": "Tanveesh Stevens",
          "email": ""
        },
        {
          "forename": "Jahnavi",
          "surname": "Singh Chaudhery",
          "name": "Jahnavi Singh Chaudhery",
          "email": ""
        },
        {
          "forename": "James",
          "surname": "Jambholkar",
          "name": "James Jambholkar",
          "email": ""
        },
        {
          "forename": "Nazneen",
          "surname": "Zou",
          "name": "Nazneen Zou",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.05495",
      "venue": "Self-rationalization improves LLM as a fine-grained judge",
      "date": "2024"
    },
    {
      "index": "b224",
      "title": "Computing machinery and intelligence",
      "author": [
        {
          "forename": "M.",
          "surname": "Alan",
          "name": "M. Alan",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Computing machinery and intelligence",
      "date": "2009"
    },
    {
      "index": "b225",
      "title": "LLMs cannot find reasoning errors",
      "author": [
        {
          "forename": "Gladys",
          "surname": "Tyen",
          "name": "Gladys Tyen",
          "email": ""
        },
        {
          "forename": "Hassan",
          "surname": "Mansoor",
          "name": "Hassan Mansoor",
          "email": ""
        },
        {
          "forename": "Peter",
          "surname": "Chen",
          "name": "Peter Chen",
          "email": ""
        },
        {
          "forename": "Tony",
          "surname": "Mak",
          "name": "Tony Mak",
          "email": ""
        },
        {
          "forename": "Victor",
          "surname": "Cărbune",
          "name": "Victor Cărbune",
          "email": ""
        }
      ],
      "doi": "arXiv:2311.08516",
      "venue": "LLMs cannot find reasoning errors",
      "date": "2023"
    },
    {
      "index": "b226",
      "title": "Can large language models really improve by self-critiquing their own plans?",
      "author": [
        {
          "forename": "Karthik",
          "surname": "Valmeekam",
          "name": "Karthik Valmeekam",
          "email": ""
        },
        {
          "forename": "Matthew",
          "surname": "Marquez",
          "name": "Matthew Marquez",
          "email": ""
        },
        {
          "forename": "Subbarao",
          "surname": "Kambhampati",
          "name": "Subbarao Kambhampati",
          "email": ""
        }
      ],
      "doi": "arXiv:2310.08118",
      "venue": "Can large language models really improve by self-critiquing their own plans?",
      "date": "2023"
    },
    {
      "index": "b227",
      "title": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models",
      "author": [
        {
          "forename": "Pat",
          "surname": "Verga",
          "name": "Pat Verga",
          "email": ""
        },
        {
          "forename": "Sebastian",
          "surname": "Hofstatter",
          "name": "Sebastian Hofstatter",
          "email": ""
        },
        {
          "forename": "Sophia",
          "surname": "Althammer",
          "name": "Sophia Althammer",
          "email": ""
        },
        {
          "forename": "Yixuan",
          "surname": "Su",
          "name": "Yixuan Su",
          "email": ""
        },
        {
          "forename": "Aleksandra",
          "surname": "Piktus",
          "name": "Aleksandra Piktus",
          "email": ""
        },
        {
          "forename": "Arkady",
          "surname": "Arkhangorodsky",
          "name": "Arkady Arkhangorodsky",
          "email": ""
        },
        {
          "forename": "Minjie",
          "surname": "Xu",
          "name": "Minjie Xu",
          "email": ""
        },
        {
          "forename": "Naomi",
          "surname": "White",
          "name": "Naomi White",
          "email": ""
        },
        {
          "forename": "Patrick",
          "surname": "Lewis",
          "name": "Patrick Lewis",
          "email": ""
        }
      ],
      "doi": "arXiv:2404.18796",
      "venue": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models",
      "date": "2024"
    },
    {
      "index": "b228",
      "title": "Foundational autoraters: Taming large language models for better automatic evaluation",
      "author": [
        {
          "forename": "Tu",
          "surname": "Vu",
          "name": "Tu Vu",
          "email": ""
        },
        {
          "forename": "Kalpesh",
          "surname": "Krishna",
          "name": "Kalpesh Krishna",
          "email": ""
        },
        {
          "forename": "Salaheddin",
          "surname": "Alzubi",
          "name": "Salaheddin Alzubi",
          "email": ""
        },
        {
          "forename": "Chris",
          "surname": "Tar",
          "name": "Chris Tar",
          "email": ""
        },
        {
          "forename": "Manaal",
          "surname": "Faruqui",
          "name": "Manaal Faruqui",
          "email": ""
        },
        {
          "forename": "Yun-Hsuan",
          "surname": "Sung",
          "name": "Yun-Hsuan Sung",
          "email": ""
        }
      ],
      "doi": "arXiv:2407.10817",
      "venue": "Foundational autoraters: Taming large language models for better automatic evaluation",
      "date": "2024"
    },
    {
      "index": "b229",
      "title": "Halu-j: Critique-based hallucination judge",
      "author": [
        {
          "forename": "Binjie",
          "surname": "Wang",
          "name": "Binjie Wang",
          "email": ""
        },
        {
          "forename": "Steffi",
          "surname": "Chern",
          "name": "Steffi Chern",
          "email": ""
        },
        {
          "forename": "Ethan",
          "surname": "Chern",
          "name": "Ethan Chern",
          "email": ""
        },
        {
          "forename": "Pengfei",
          "surname": "Liu",
          "name": "Pengfei Liu",
          "email": ""
        }
      ],
      "doi": "arXiv:2407.12943",
      "venue": "Halu-j: Critique-based hallucination judge",
      "date": "2024"
    },
    {
      "index": "b230",
      "title": "Automated Genre-Aware Article Scoring and Feedback Using Large Language Models",
      "author": [
        {
          "forename": "Chihang",
          "surname": "Wang",
          "name": "Chihang Wang",
          "email": ""
        },
        {
          "forename": "Yuxin",
          "surname": "Dong",
          "name": "Yuxin Dong",
          "email": ""
        },
        {
          "forename": "Zhenhong",
          "surname": "Zhang",
          "name": "Zhenhong Zhang",
          "email": ""
        },
        {
          "forename": "Ruotong",
          "surname": "Wang",
          "name": "Ruotong Wang",
          "email": ""
        },
        {
          "forename": "Shuo",
          "surname": "Wang",
          "name": "Shuo Wang",
          "email": ""
        },
        {
          "forename": "Jiajing",
          "surname": "Chen",
          "name": "Jiajing Chen",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.14165",
      "venue": "Automated Genre-Aware Article Scoring and Feedback Using Large Language Models",
      "date": "2024"
    },
    {
      "index": "b231",
      "title": "Learning Evaluation Models from Large Language Models for Sequence Generation",
      "author": [
        {
          "forename": "Chenglong",
          "surname": "Wang",
          "name": "Chenglong Wang",
          "email": ""
        },
        {
          "forename": "Hang",
          "surname": "Zhou",
          "name": "Hang Zhou",
          "email": ""
        },
        {
          "forename": "Kaiyan",
          "surname": "Chang",
          "name": "Kaiyan Chang",
          "email": ""
        },
        {
          "forename": "Tongran",
          "surname": "Liu",
          "name": "Tongran Liu",
          "email": ""
        },
        {
          "forename": "Chunliang",
          "surname": "Zhang",
          "name": "Chunliang Zhang",
          "email": ""
        },
        {
          "forename": "Quan",
          "surname": "Du",
          "name": "Quan Du",
          "email": ""
        },
        {
          "forename": "Tong",
          "surname": "Xiao",
          "name": "Tong Xiao",
          "email": ""
        },
        {
          "forename": "Jingbo",
          "surname": "Zhu",
          "name": "Jingbo Zhu",
          "email": ""
        }
      ],
      "doi": "arXiv:2308.04386",
      "venue": "Learning Evaluation Models from Large Language Models for Sequence Generation",
      "date": "2023"
    },
    {
      "index": "b232",
      "title": "Tianyu Liu, and Zhifang Sui. 2023. Large language models are not fair evaluators",
      "author": [
        {
          "forename": "Peiyi",
          "surname": "Wang",
          "name": "Peiyi Wang",
          "email": ""
        },
        {
          "forename": "Lei",
          "surname": "Li",
          "name": "Lei Li",
          "email": ""
        },
        {
          "forename": "Liang",
          "surname": "Chen",
          "name": "Liang Chen",
          "email": ""
        },
        {
          "forename": "Zefan",
          "surname": "Cai",
          "name": "Zefan Cai",
          "email": ""
        },
        {
          "forename": "Dawei",
          "surname": "Zhu",
          "name": "Dawei Zhu",
          "email": ""
        },
        {
          "forename": "Binghuai",
          "surname": "Lin",
          "name": "Binghuai Lin",
          "email": ""
        },
        {
          "forename": "Yunbo",
          "surname": "Cao",
          "name": "Yunbo Cao",
          "email": ""
        },
        {
          "forename": "Qi",
          "surname": "Liu",
          "name": "Qi Liu",
          "email": ""
        }
      ],
      "doi": "arXiv:2305.17926",
      "venue": "Tianyu Liu, and Zhifang Sui. 2023. Large language models are not fair evaluators",
      "date": "2023"
    },
    {
      "index": "b234",
      "title": "Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2023. Shepherd: A critic for language model generation",
      "author": [
        {
          "forename": "Tianlu",
          "surname": "Wang",
          "name": "Tianlu Wang",
          "email": ""
        },
        {
          "forename": "Ping",
          "surname": "Yu",
          "name": "Ping Yu",
          "email": ""
        },
        {
          "forename": "Xiaoqing Ellen ",
          "surname": "Tan",
          "name": "Xiaoqing Ellen  Tan",
          "email": ""
        },
        {
          "forename": "O'",
          "surname": "Sean",
          "name": "O' Sean",
          "email": ""
        },
        {
          "forename": "Ramakanth",
          "surname": "Brien",
          "name": "Ramakanth Brien",
          "email": ""
        },
        {
          "forename": "Jane",
          "surname": "Pasunuru",
          "name": "Jane Pasunuru",
          "email": ""
        },
        {
          "forename": "Olga",
          "surname": "Dwivedi-Yu",
          "name": "Olga Dwivedi-Yu",
          "email": ""
        },
        {
          "forename": "Luke",
          "surname": "Golovneva",
          "name": "Luke Golovneva",
          "email": ""
        }
      ],
      "doi": "arXiv:2308.04592",
      "venue": "Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2023. Shepherd: A critic for language model generation",
      "date": "2023"
    },
    {
      "index": "b235",
      "title": "Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic Evaluation Framework for LLMs",
      "author": [
        {
          "forename": "Wanying",
          "surname": "Wang",
          "name": "Wanying Wang",
          "email": ""
        },
        {
          "forename": "Zeyu",
          "surname": "Ma",
          "name": "Zeyu Ma",
          "email": ""
        },
        {
          "forename": "Pengfei",
          "surname": "Liu",
          "name": "Pengfei Liu",
          "email": ""
        },
        {
          "forename": "Mingang",
          "surname": "Chen",
          "name": "Mingang Chen",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.11507",
      "venue": "Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic Evaluation Framework for LLMs",
      "date": "2024"
    },
    {
      "index": "b236",
      "title": "Position bias estimation for unbiased learning to rank in personal search",
      "author": [
        {
          "forename": "Xuanhui",
          "surname": "Wang",
          "name": "Xuanhui Wang",
          "email": ""
        },
        {
          "forename": "Nadav",
          "surname": "Golbandi",
          "name": "Nadav Golbandi",
          "email": ""
        },
        {
          "forename": "Michael",
          "surname": "Bendersky",
          "name": "Michael Bendersky",
          "email": ""
        },
        {
          "forename": "Donald",
          "surname": "Metzler",
          "name": "Donald Metzler",
          "email": ""
        },
        {
          "forename": "Marc",
          "surname": "Najork",
          "name": "Marc Najork",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the eleventh ACM international conference on web search and data mining",
      "date": "2018"
    },
    {
      "index": "b237",
      "title": "Self-consistency improves chain of thought reasoning in language models",
      "author": [
        {
          "forename": "Xuezhi",
          "surname": "Wang",
          "name": "Xuezhi Wang",
          "email": ""
        },
        {
          "forename": "Jason",
          "surname": "Wei",
          "name": "Jason Wei",
          "email": ""
        },
        {
          "forename": "Dale",
          "surname": "Schuurmans",
          "name": "Dale Schuurmans",
          "email": ""
        },
        {
          "forename": "Quoc",
          "surname": "Le",
          "name": "Quoc Le",
          "email": ""
        },
        {
          "forename": "Ed",
          "surname": "Chi",
          "name": "Ed Chi",
          "email": ""
        },
        {
          "forename": "Sharan",
          "surname": "Narang",
          "name": "Sharan Narang",
          "email": ""
        },
        {
          "forename": "Aakanksha",
          "surname": "Chowdhery",
          "name": "Aakanksha Chowdhery",
          "email": ""
        },
        {
          "forename": "Denny",
          "surname": "Zhou",
          "name": "Denny Zhou",
          "email": ""
        }
      ],
      "doi": "arXiv:2203.11171",
      "venue": "Self-consistency improves chain of thought reasoning in language models",
      "date": "2022"
    },
    {
      "index": "b238",
      "title": "Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization",
      "author": [
        {
          "forename": "Yidong",
          "surname": "Wang",
          "name": "Yidong Wang",
          "email": ""
        },
        {
          "forename": "Zhuohao",
          "surname": "Yu",
          "name": "Zhuohao Yu",
          "email": ""
        },
        {
          "forename": "Zhengran",
          "surname": "Zeng",
          "name": "Zhengran Zeng",
          "email": ""
        },
        {
          "forename": "Linyi",
          "surname": "Yang",
          "name": "Linyi Yang",
          "email": ""
        },
        {
          "forename": "Cunxiang",
          "surname": "Wang",
          "name": "Cunxiang Wang",
          "email": ""
        },
        {
          "forename": "Hao",
          "surname": "Chen",
          "name": "Hao Chen",
          "email": ""
        },
        {
          "forename": "Chaoya",
          "surname": "Jiang",
          "name": "Chaoya Jiang",
          "email": ""
        },
        {
          "forename": "Rui",
          "surname": "Xie",
          "name": "Rui Xie",
          "email": ""
        },
        {
          "forename": "Jindong",
          "surname": "Wang",
          "name": "Jindong Wang",
          "email": ""
        },
        {
          "forename": "Xing",
          "surname": "Xie",
          "name": "Xing Xie",
          "email": ""
        }
      ],
      "doi": "arXiv:2306.05087",
      "venue": "Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization",
      "date": "2023"
    },
    {
      "index": "b239",
      "title": "HelpSteer2-Preference: Complementing Ratings with Preferences",
      "author": [
        {
          "forename": "Zhilin",
          "surname": "Wang",
          "name": "Zhilin Wang",
          "email": ""
        },
        {
          "forename": "Alexander",
          "surname": "Bukharin",
          "name": "Alexander Bukharin",
          "email": ""
        },
        {
          "forename": "Olivier",
          "surname": "Delalleau",
          "name": "Olivier Delalleau",
          "email": ""
        },
        {
          "forename": "Daniel",
          "surname": "Egert",
          "name": "Daniel Egert",
          "email": ""
        },
        {
          "forename": "Gerald",
          "surname": "Shen",
          "name": "Gerald Shen",
          "email": ""
        },
        {
          "forename": "Jiaqi",
          "surname": "Zeng",
          "name": "Jiaqi Zeng",
          "email": ""
        },
        {
          "forename": "Oleksii",
          "surname": "Kuchaiev",
          "name": "Oleksii Kuchaiev",
          "email": ""
        },
        {
          "forename": "Yi",
          "surname": "Dong",
          "name": "Yi Dong",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.01257",
      "venue": "HelpSteer2-Preference: Complementing Ratings with Preferences",
      "date": "2024"
    },
    {
      "index": "b240",
      "title": "Helpsteer: Multi-attribute helpfulness dataset for steerlm",
      "author": [
        {
          "forename": "Zhilin",
          "surname": "Wang",
          "name": "Zhilin Wang",
          "email": ""
        },
        {
          "forename": "Yi",
          "surname": "Dong",
          "name": "Yi Dong",
          "email": ""
        },
        {
          "forename": "Jiaqi",
          "surname": "Zeng",
          "name": "Jiaqi Zeng",
          "email": ""
        },
        {
          "forename": "Virginia",
          "surname": "Adams",
          "name": "Virginia Adams",
          "email": ""
        },
        {
          "forename": "Daniel",
          "surname": "Sreedhar",
          "name": "Daniel Sreedhar",
          "email": ""
        },
        {
          "forename": "Olivier",
          "surname": "Egert",
          "name": "Olivier Egert",
          "email": ""
        },
        {
          "forename": "Jane Polak ",
          "surname": "Delalleau",
          "name": "Jane Polak  Delalleau",
          "email": ""
        },
        {
          "forename": "Neel",
          "surname": "Scowcroft",
          "name": "Neel Scowcroft",
          "email": ""
        },
        {
          "forename": "Aidan",
          "surname": "Kant",
          "name": "Aidan Kant",
          "email": ""
        }
      ],
      "doi": "arXiv:2311.09528",
      "venue": "Helpsteer: Multi-attribute helpfulness dataset for steerlm",
      "date": "2023"
    },
    {
      "index": "b241",
      "title": "Cream: Consistency regularized self-rewarding language models",
      "author": [
        {
          "forename": "Zhaoyang",
          "surname": "Wang",
          "name": "Zhaoyang Wang",
          "email": ""
        },
        {
          "forename": "Weilei",
          "surname": "He",
          "name": "Weilei He",
          "email": ""
        },
        {
          "forename": "Zhiyuan",
          "surname": "Liang",
          "name": "Zhiyuan Liang",
          "email": ""
        },
        {
          "forename": "Xuchao",
          "surname": "Zhang",
          "name": "Xuchao Zhang",
          "email": ""
        },
        {
          "forename": "Chetan",
          "surname": "Bansal",
          "name": "Chetan Bansal",
          "email": ""
        },
        {
          "forename": "Ying",
          "surname": "Wei",
          "name": "Ying Wei",
          "email": ""
        },
        {
          "forename": "Weitong",
          "surname": "Zhang",
          "name": "Weitong Zhang",
          "email": ""
        },
        {
          "forename": "Huaxiu",
          "surname": "Yao",
          "name": "Huaxiu Yao",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.12735",
      "venue": "Cream: Consistency regularized self-rewarding language models",
      "date": "2024"
    },
    {
      "index": "b242",
      "title": "Five ways to look at Cohen's kappa",
      "author": [
        {
          "forename": "J.",
          "surname": "Matthijs",
          "name": "J. Matthijs",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Journal of Psychology & Psychotherapy",
      "date": "2015"
    },
    {
      "index": "b243",
      "title": "PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data",
      "author": [
        {
          "forename": "Ishaan",
          "surname": "Watts",
          "name": "Ishaan Watts",
          "email": ""
        },
        {
          "forename": "Varun",
          "surname": "Gumma",
          "name": "Varun Gumma",
          "email": ""
        },
        {
          "forename": "Aditya",
          "surname": "Yadavalli",
          "name": "Aditya Yadavalli",
          "email": ""
        },
        {
          "forename": "Vivek",
          "surname": "Seshadri",
          "name": "Vivek Seshadri",
          "email": ""
        },
        {
          "forename": "Manohar",
          "surname": "Swaminathan",
          "name": "Manohar Swaminathan",
          "email": ""
        },
        {
          "forename": "Sunayana",
          "surname": "Sitaram",
          "name": "Sunayana Sitaram",
          "email": ""
        }
      ],
      "doi": "arXiv:2406.15053",
      "venue": "PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data",
      "date": "2024"
    },
    {
      "index": "b244",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "author": [
        {
          "forename": "Jason",
          "surname": "Wei",
          "name": "Jason Wei",
          "email": ""
        },
        {
          "forename": "Xuezhi",
          "surname": "Wang",
          "name": "Xuezhi Wang",
          "email": ""
        },
        {
          "forename": "Dale",
          "surname": "Schuurmans",
          "name": "Dale Schuurmans",
          "email": ""
        },
        {
          "forename": "Maarten",
          "surname": "Bosma",
          "name": "Maarten Bosma",
          "email": ""
        },
        {
          "forename": "Fei",
          "surname": "Xia",
          "name": "Fei Xia",
          "email": ""
        },
        {
          "forename": "Ed",
          "surname": "Chi",
          "name": "Ed Chi",
          "email": ""
        },
        {
          "forename": "V.",
          "surname": "Quoc",
          "name": "V. Quoc",
          "email": ""
        },
        {
          "forename": "Denny",
          "surname": "Le",
          "name": "Denny Le",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Advances in neural information processing systems",
      "date": "2022"
    },
    {
      "index": "b245",
      "title": "CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences",
      "author": [
        {
          "forename": "Martin",
          "surname": "Weyssow",
          "name": "Martin Weyssow",
          "email": ""
        },
        {
          "forename": "Aton",
          "surname": "Kamanda",
          "name": "Aton Kamanda",
          "email": ""
        },
        {
          "forename": "Houari",
          "surname": "Sahraoui",
          "name": "Houari Sahraoui",
          "email": ""
        }
      ],
      "doi": "arXiv:2403.09032",
      "venue": "CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences",
      "date": "2024"
    },
    {
      "index": "b246",
      "title": "Continual learning for large language models: A survey",
      "author": [
        {
          "forename": "Tongtong",
          "surname": "Wu",
          "name": "Tongtong Wu",
          "email": ""
        },
        {
          "forename": "Linhao",
          "surname": "Luo",
          "name": "Linhao Luo",
          "email": ""
        },
        {
          "forename": "Yuan-Fang",
          "surname": "Li",
          "name": "Yuan-Fang Li",
          "email": ""
        },
        {
          "forename": "Shirui",
          "surname": "Pan",
          "name": "Shirui Pan",
          "email": ""
        },
        {
          "forename": "Thuy-Trang",
          "surname": "Vu",
          "name": "Thuy-Trang Vu",
          "email": ""
        },
        {
          "forename": "Gholamreza",
          "surname": "Haffari",
          "name": "Gholamreza Haffari",
          "email": ""
        }
      ],
      "doi": "arXiv:2402.01364",
      "venue": "Continual learning for large language models: A survey",
      "date": "2024"
    },
    {
      "index": "b247",
      "title": "Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge",
      "author": [
        {
          "forename": "Tianhao",
          "surname": "Wu",
          "name": "Tianhao Wu",
          "email": ""
        },
        {
          "forename": "Weizhe",
          "surname": "Yuan",
          "name": "Weizhe Yuan",
          "email": ""
        },
        {
          "forename": "Olga",
          "surname": "Golovneva",
          "name": "Olga Golovneva",
          "email": ""
        },
        {
          "forename": "Jing",
          "surname": "Xu",
          "name": "Jing Xu",
          "email": ""
        },
        {
          "forename": "Yuandong",
          "surname": "Tian",
          "name": "Yuandong Tian",
          "email": ""
        },
        {
          "forename": "Jiantao",
          "surname": "Jiao",
          "name": "Jiantao Jiao",
          "email": ""
        },
        {
          "forename": "Jason",
          "surname": "Weston",
          "name": "Jason Weston",
          "email": ""
        },
        {
          "forename": "Sainbayar",
          "surname": "Sukhbaatar",
          "name": "Sainbayar Sukhbaatar",
          "email": ""
        }
      ],
      "doi": "arXiv:2407.19594",
      "venue": "Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge",
      "date": "2024"
    },
    {
      "index": "b248",
      "title": "Evaluating Mathematical Reasoning Beyond Accuracy",
      "author": [
        {
          "forename": "Shijie",
          "surname": "Xia",
          "name": "Shijie Xia",
          "email": ""
        },
        {
          "forename": "Xuefeng",
          "surname": "Li",
          "name": "Xuefeng Li",
          "email": ""
        },
        {
          "forename": "Yixin",
          "surname": "Liu",
          "name": "Yixin Liu",
          "email": ""
        },
        {
          "forename": "Tongshuang",
          "surname": "Wu",
          "name": "Tongshuang Wu",
          "email": ""
        },
        {
          "forename": "Pengfei",
          "surname": "Liu",
          "name": "Pengfei Liu",
          "email": ""
        }
      ],
      "doi": "arXiv:2404.05692",
      "venue": "Evaluating Mathematical Reasoning Beyond Accuracy",
      "date": "2024"
    },
    {
      "index": "b249",
      "title": "Language Models can Evaluate Themselves via Probability Discrepancy",
      "author": [
        {
          "forename": "Tingyu",
          "surname": "Xia",
          "name": "Tingyu Xia",
          "email": ""
        },
        {
          "forename": "Bowen",
          "surname": "Yu",
          "name": "Bowen Yu",
          "email": ""
        },
        {
          "forename": "Yuan",
          "surname": "Wu",
          "name": "Yuan Wu",
          "email": ""
        },
        {
          "forename": "Yi",
          "surname": "Chang",
          "name": "Yi Chang",
          "email": ""
        },
        {
          "forename": "Chang",
          "surname": "Zhou",
          "name": "Chang Zhou",
          "email": ""
        }
      ],
      "doi": "arXiv:2405.10516",
      "venue": "Language Models can Evaluate Themselves via Probability Discrepancy",
      "date": "2024"
    },
    {
      "index": "b250",
      "title": "Pixiu: A large language model, instruction data and evaluation benchmark for finance",
      "author": [
        {
          "forename": "Qianqian",
          "surname": "Xie",
          "name": "Qianqian Xie",
          "email": ""
        },
        {
          "forename": "Weiguang",
          "surname": "Han",
          "name": "Weiguang Han",
          "email": ""
        },
        {
          "forename": "Xiao",
          "surname": "Zhang",
          "name": "Xiao Zhang",
          "email": ""
        },
        {
          "forename": "Yanzhao",
          "surname": "Lai",
          "name": "Yanzhao Lai",
          "email": ""
        },
        {
          "forename": "Min",
          "surname": "Peng",
          "name": "Min Peng",
          "email": ""
        },
        {
          "forename": "Alejandro",
          "surname": "Lopez-Lira",
          "name": "Alejandro Lopez-Lira",
          "email": ""
        },
        {
          "forename": "Jimin",
          "surname": "Huang",
          "name": "Jimin Huang",
          "email": ""
        }
      ],
      "doi": "arXiv:2306.05443",
      "venue": "Pixiu: A large language model, instruction data and evaluation benchmark for finance",
      "date": "2023"
    },
    {
      "index": "b251",
      "title": "Sorry-bench: Systematically evaluating large language model safety refusal behaviors",
      "author": [
        {
          "forename": "Tinghao",
          "surname": "Xie",
          "name": "Tinghao Xie",
          "email": ""
        },
        {
          "forename": "Xiangyu",
          "surname": "Qi",
          "name": "Xiangyu Qi",
          "email": ""
        },
        {
          "forename": "Yi",
          "surname": "Zeng",
          "name": "Yi Zeng",
          "email": ""
        },
        {
          "forename": "Yangsibo",
          "surname": "Huang",
          "name": "Yangsibo Huang",
          "email": ""
        },
        {
          "forename": "Udari Madhushani ",
          "surname": "Sehwag",
          "name": "Udari Madhushani  Sehwag",
          "email": ""
        },
        {
          "forename": "Kaixuan",
          "surname": "Huang",
          "name": "Kaixuan Huang",
          "email": ""
        },
        {
          "forename": "Luxi",
          "surname": "He",
          "name": "Luxi He",
          "email": ""
        },
        {
          "forename": "Boyi",
          "surname": "Wei",
          "name": "Boyi Wei",
          "email": ""
        },
        {
          "forename": "Dacheng",
          "surname": "Li",
          "name": "Dacheng Li",
          "email": ""
        },
        {
          "forename": "Ying",
          "surname": "Sheng",
          "name": "Ying Sheng",
          "email": ""
        }
      ],
      "doi": "arXiv:2406.14598",
      "venue": "Sorry-bench: Systematically evaluating large language model safety refusal behaviors",
      "date": "2024"
    },
    {
      "index": "b252",
      "title": "Selfevaluation guided beam search for reasoning",
      "author": [
        {
          "forename": "Yuxi",
          "surname": "Xie",
          "name": "Yuxi Xie",
          "email": ""
        },
        {
          "forename": "Kenji",
          "surname": "Kawaguchi",
          "name": "Kenji Kawaguchi",
          "email": ""
        },
        {
          "forename": "Yiran",
          "surname": "Zhao",
          "name": "Yiran Zhao",
          "email": ""
        },
        {
          "forename": "James Xu ",
          "surname": "Zhao",
          "name": "James Xu  Zhao",
          "email": ""
        },
        {
          "forename": "Min-Yen",
          "surname": "Kan",
          "name": "Min-Yen Kan",
          "email": ""
        },
        {
          "forename": "Junxian",
          "surname": "He",
          "name": "Junxian He",
          "email": ""
        },
        {
          "forename": "Michael",
          "surname": "Xie",
          "name": "Michael Xie",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Advances in Neural Information Processing Systems",
      "date": "2024"
    },
    {
      "index": "b253",
      "title": "DOCLENS: Multi-aspect fine-grained evaluation for medical text generation",
      "author": [
        {
          "forename": "Yiqing",
          "surname": "Xie",
          "name": "Yiqing Xie",
          "email": ""
        },
        {
          "forename": "Sheng",
          "surname": "Zhang",
          "name": "Sheng Zhang",
          "email": ""
        },
        {
          "forename": "Hao",
          "surname": "Cheng",
          "name": "Hao Cheng",
          "email": ""
        },
        {
          "forename": "Pengfei",
          "surname": "Liu",
          "name": "Pengfei Liu",
          "email": ""
        },
        {
          "forename": "Zelalem",
          "surname": "Gero",
          "name": "Zelalem Gero",
          "email": ""
        },
        {
          "forename": "Cliff",
          "surname": "Wong",
          "name": "Cliff Wong",
          "email": ""
        },
        {
          "forename": "Tristan",
          "surname": "Naumann",
          "name": "Tristan Naumann",
          "email": ""
        },
        {
          "forename": "Hoifung",
          "surname": "Poon",
          "name": "Hoifung Poon",
          "email": ""
        },
        {
          "forename": "Carolyn",
          "surname": "Rose",
          "name": "Carolyn Rose",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics",
      "date": "2024"
    },
    {
      "index": "b254",
      "title": "Improving Model Factuality with Fine-grained Critique-based Evaluator",
      "author": [
        {
          "forename": "Yiqing",
          "surname": "Xie",
          "name": "Yiqing Xie",
          "email": ""
        },
        {
          "forename": "Wenxuan",
          "surname": "Zhou",
          "name": "Wenxuan Zhou",
          "email": ""
        },
        {
          "forename": "Pradyot",
          "surname": "Prakash",
          "name": "Pradyot Prakash",
          "email": ""
        },
        {
          "forename": "Di",
          "surname": "Jin",
          "name": "Di Jin",
          "email": ""
        },
        {
          "forename": "Yuning",
          "surname": "Mao",
          "name": "Yuning Mao",
          "email": ""
        },
        {
          "forename": "Quintin",
          "surname": "Fettes",
          "name": "Quintin Fettes",
          "email": ""
        },
        {
          "forename": "Arya",
          "surname": "Talebzadeh",
          "name": "Arya Talebzadeh",
          "email": ""
        },
        {
          "forename": "Sinong",
          "surname": "Wang",
          "name": "Sinong Wang",
          "email": ""
        },
        {
          "forename": "Han",
          "surname": "Fang",
          "name": "Han Fang",
          "email": ""
        },
        {
          "forename": "Carolyn",
          "surname": "Rose",
          "name": "Carolyn Rose",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.18359",
      "venue": "Improving Model Factuality with Fine-grained Critique-based Evaluator",
      "date": "2024"
    },
    {
      "index": "b255",
      "title": "Llava-critic: Learning to evaluate multimodal models",
      "author": [
        {
          "forename": "Tianyi",
          "surname": "Xiong",
          "name": "Tianyi Xiong",
          "email": ""
        },
        {
          "forename": "Xiyao",
          "surname": "Wang",
          "name": "Xiyao Wang",
          "email": ""
        },
        {
          "forename": "Dong",
          "surname": "Guo",
          "name": "Dong Guo",
          "email": ""
        },
        {
          "forename": "Qinghao",
          "surname": "Ye",
          "name": "Qinghao Ye",
          "email": ""
        },
        {
          "forename": "Haoqi",
          "surname": "Fan",
          "name": "Haoqi Fan",
          "email": ""
        },
        {
          "forename": "Quanquan",
          "surname": "Gu",
          "name": "Quanquan Gu",
          "email": ""
        },
        {
          "forename": "Heng",
          "surname": "Huang",
          "name": "Heng Huang",
          "email": ""
        },
        {
          "forename": "Chunyuan",
          "surname": "Li",
          "name": "Chunyuan Li",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.02712",
      "venue": "Llava-critic: Learning to evaluate multimodal models",
      "date": "2024"
    },
    {
      "index": "b256",
      "title": "Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions",
      "author": [
        {
          "forename": "Can",
          "surname": "Xu",
          "name": "Can Xu",
          "email": ""
        },
        {
          "forename": "Qingfeng",
          "surname": "Sun",
          "name": "Qingfeng Sun",
          "email": ""
        },
        {
          "forename": "Kai",
          "surname": "Zheng",
          "name": "Kai Zheng",
          "email": ""
        },
        {
          "forename": "Xiubo",
          "surname": "Geng",
          "name": "Xiubo Geng",
          "email": ""
        },
        {
          "forename": "Pu",
          "surname": "Zhao",
          "name": "Pu Zhao",
          "email": ""
        },
        {
          "forename": "Jiazhan",
          "surname": "Feng",
          "name": "Jiazhan Feng",
          "email": ""
        }
      ],
      "doi": "arXiv:2304.12244",
      "venue": "Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions",
      "date": "2023"
    },
    {
      "index": "b257",
      "title": "Cvalues: Measuring the values of chinese large language models from safety to responsibility",
      "author": [
        {
          "forename": "Guohai",
          "surname": "Xu",
          "name": "Guohai Xu",
          "email": ""
        },
        {
          "forename": "Jiayi",
          "surname": "Liu",
          "name": "Jiayi Liu",
          "email": ""
        },
        {
          "forename": "Ming",
          "surname": "Yan",
          "name": "Ming Yan",
          "email": ""
        },
        {
          "forename": "Haotian",
          "surname": "Xu",
          "name": "Haotian Xu",
          "email": ""
        },
        {
          "forename": "Jinghui",
          "surname": "Si",
          "name": "Jinghui Si",
          "email": ""
        },
        {
          "forename": "Zhuoran",
          "surname": "Zhou",
          "name": "Zhuoran Zhou",
          "email": ""
        },
        {
          "forename": "Peng",
          "surname": "Yi",
          "name": "Peng Yi",
          "email": ""
        },
        {
          "forename": "Xing",
          "surname": "Gao",
          "name": "Xing Gao",
          "email": ""
        },
        {
          "forename": "Jitao",
          "surname": "Sang",
          "name": "Jitao Sang",
          "email": ""
        },
        {
          "forename": "Rong",
          "surname": "Zhang",
          "name": "Rong Zhang",
          "email": ""
        }
      ],
      "doi": "arXiv:2307.09705",
      "venue": "Cvalues: Measuring the values of chinese large language models from safety to responsibility",
      "date": "2023"
    },
    {
      "index": "b258",
      "title": "Large Language Models Are Active Critics in NLG Evaluation",
      "author": [
        {
          "forename": "Shuying",
          "surname": "Xu",
          "name": "Shuying Xu",
          "email": ""
        },
        {
          "forename": "Junjie",
          "surname": "Hu",
          "name": "Junjie Hu",
          "email": ""
        },
        {
          "forename": "Ming",
          "surname": "Jiang",
          "name": "Ming Jiang",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.10724",
      "venue": "Large Language Models Are Active Critics in NLG Evaluation",
      "date": "2024"
    },
    {
      "index": "b259",
      "title": "The perfect blend: Redefining RLHF with mixture of judges",
      "author": [
        {
          "forename": "Tengyu",
          "surname": "Xu",
          "name": "Tengyu Xu",
          "email": ""
        },
        {
          "forename": "Eryk",
          "surname": "Helenowski",
          "name": "Eryk Helenowski",
          "email": ""
        },
        {
          "forename": "Abinav",
          "surname": "Karthik",
          "name": "Abinav Karthik",
          "email": ""
        },
        {
          "forename": "Di",
          "surname": "Sankararaman",
          "name": "Di Sankararaman",
          "email": ""
        },
        {
          "forename": "Kaiyan",
          "surname": "Jin",
          "name": "Kaiyan Jin",
          "email": ""
        },
        {
          "forename": "Eric",
          "surname": "Peng",
          "name": "Eric Peng",
          "email": ""
        },
        {
          "forename": "Shaoliang",
          "surname": "Han",
          "name": "Shaoliang Han",
          "email": ""
        },
        {
          "forename": "Chen",
          "surname": "Nie",
          "name": "Chen Nie",
          "email": ""
        },
        {
          "forename": "Hejia",
          "surname": "Zhu",
          "name": "Hejia Zhu",
          "email": ""
        },
        {
          "forename": "Wenxuan",
          "surname": "Zhang",
          "name": "Wenxuan Zhang",
          "email": ""
        }
      ],
      "doi": "arXiv:2409.20370",
      "venue": "The perfect blend: Redefining RLHF with mixture of judges",
      "date": "2024"
    },
    {
      "index": "b260",
      "title": "STRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback",
      "author": [
        {
          "forename": "Wenda",
          "surname": "Xu",
          "name": "Wenda Xu",
          "email": ""
        },
        {
          "forename": "Danqing",
          "surname": "Wang",
          "name": "Danqing Wang",
          "email": ""
        },
        {
          "forename": "Liangming",
          "surname": "Pan",
          "name": "Liangming Pan",
          "email": ""
        },
        {
          "forename": "Zhenqiao",
          "surname": "Song",
          "name": "Zhenqiao Song",
          "email": ""
        },
        {
          "forename": "Markus",
          "surname": "Freitag",
          "name": "Markus Freitag",
          "email": ""
        },
        {
          "forename": "William Yang ",
          "surname": "Wang",
          "name": "William Yang  Wang",
          "email": ""
        },
        {
          "forename": "Lei",
          "surname": "Li",
          "name": "Lei Li",
          "email": ""
        }
      ],
      "doi": "arXiv:2305.14282",
      "venue": "STRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback",
      "date": "2023"
    },
    {
      "index": "b261",
      "title": "Pride and prejudice: LLM amplifies self-bias in self-refinement",
      "author": [
        {
          "forename": "Wenda",
          "surname": "Xu",
          "name": "Wenda Xu",
          "email": ""
        },
        {
          "forename": "Guanglei",
          "surname": "Zhu",
          "name": "Guanglei Zhu",
          "email": ""
        },
        {
          "forename": "Xuandong",
          "surname": "Zhao",
          "name": "Xuandong Zhao",
          "email": ""
        },
        {
          "forename": "Liangming",
          "surname": "Pan",
          "name": "Liangming Pan",
          "email": ""
        },
        {
          "forename": "Lei",
          "surname": "Li",
          "name": "Lei Li",
          "email": ""
        },
        {
          "forename": "William",
          "surname": "Wang",
          "name": "William Wang",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics",
      "date": "2024"
    },
    {
      "index": "b262",
      "title": "An LLM can Fool Itself: A Prompt-Based Adversarial Attack",
      "author": [
        {
          "forename": "Xilie",
          "surname": "Xu",
          "name": "Xilie Xu",
          "email": ""
        },
        {
          "forename": "Keyi",
          "surname": "Kong",
          "name": "Keyi Kong",
          "email": ""
        },
        {
          "forename": "Ning",
          "surname": "Liu",
          "name": "Ning Liu",
          "email": ""
        },
        {
          "forename": "Lizhen",
          "surname": "Cui",
          "name": "Lizhen Cui",
          "email": ""
        },
        {
          "forename": "Di",
          "surname": "Wang",
          "name": "Di Wang",
          "email": ""
        },
        {
          "forename": "Jingfeng",
          "surname": "Zhang",
          "name": "Jingfeng Zhang",
          "email": ""
        },
        {
          "forename": "Mohan",
          "surname": "Kankanhalli",
          "name": "Mohan Kankanhalli",
          "email": ""
        }
      ],
      "doi": "arXiv:2310.13345",
      "venue": "An LLM can Fool Itself: A Prompt-Based Adversarial Attack",
      "date": "2023"
    },
    {
      "index": "b263",
      "title": "Towards reasoning in large language models via multi-agent peer review collaboration",
      "author": [
        {
          "forename": "Zhenran",
          "surname": "Xu",
          "name": "Zhenran Xu",
          "email": ""
        },
        {
          "forename": "Senbao",
          "surname": "Shi",
          "name": "Senbao Shi",
          "email": ""
        },
        {
          "forename": "Baotian",
          "surname": "Hu",
          "name": "Baotian Hu",
          "email": ""
        },
        {
          "forename": "Jindi",
          "surname": "Yu",
          "name": "Jindi Yu",
          "email": ""
        },
        {
          "forename": "Dongfang",
          "surname": "Li",
          "name": "Dongfang Li",
          "email": ""
        },
        {
          "forename": "Min",
          "surname": "Zhang",
          "name": "Min Zhang",
          "email": ""
        },
        {
          "forename": "Yuxiang",
          "surname": "Wu",
          "name": "Yuxiang Wu",
          "email": ""
        }
      ],
      "doi": "arXiv:2311.08152",
      "venue": "Towards reasoning in large language models via multi-agent peer review collaboration",
      "date": "2023"
    },
    {
      "index": "b264",
      "title": "Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing",
      "author": [
        {
          "forename": "Le",
          "surname": "Yan",
          "name": "Le Yan",
          "email": ""
        },
        {
          "forename": "Zhen",
          "surname": "Qin",
          "name": "Zhen Qin",
          "email": ""
        },
        {
          "forename": "Honglei",
          "surname": "Zhuang",
          "name": "Honglei Zhuang",
          "email": ""
        },
        {
          "forename": "Rolf",
          "surname": "Jagerman",
          "name": "Rolf Jagerman",
          "email": ""
        },
        {
          "forename": "Xuanhui",
          "surname": "Wang",
          "name": "Xuanhui Wang",
          "email": ""
        },
        {
          "forename": "Michael",
          "surname": "Bendersky",
          "name": "Michael Bendersky",
          "email": ""
        },
        {
          "forename": "Harrie",
          "surname": "Oosterhuis",
          "name": "Harrie Oosterhuis",
          "email": ""
        }
      ],
      "doi": "arXiv:2404.11791",
      "venue": "Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing",
      "date": "2024"
    },
    {
      "index": "b265",
      "title": "Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing",
      "author": [
        {
          "forename": "Le",
          "surname": "Yan",
          "name": "Le Yan",
          "email": ""
        },
        {
          "forename": "Zhen",
          "surname": "Qin",
          "name": "Zhen Qin",
          "email": ""
        },
        {
          "forename": "Honglei",
          "surname": "Zhuang",
          "name": "Honglei Zhuang",
          "email": ""
        },
        {
          "forename": "Rolf",
          "surname": "Jagerman",
          "name": "Rolf Jagerman",
          "email": ""
        },
        {
          "forename": "Xuanhui",
          "surname": "Wang",
          "name": "Xuanhui Wang",
          "email": ""
        },
        {
          "forename": "Michael",
          "surname": "Bendersky",
          "name": "Michael Bendersky",
          "email": ""
        },
        {
          "forename": "Harrie",
          "surname": "Oosterhuis",
          "name": "Harrie Oosterhuis",
          "email": ""
        }
      ],
      "doi": "10.18653/v1/2024.emnlp-main.25",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
      "date": "2024"
    },
    {
      "index": "b266",
      "title": "Mitigating biases for instruction-following language models via bias neurons elimination",
      "author": [
        {
          "forename": "Nakyeong",
          "surname": "Yang",
          "name": "Nakyeong Yang",
          "email": ""
        },
        {
          "forename": "Taegwan",
          "surname": "Kang",
          "name": "Taegwan Kang",
          "email": ""
        },
        {
          "forename": "Stanley Jungkyu ",
          "surname": "Choi",
          "name": "Stanley Jungkyu  Choi",
          "email": ""
        },
        {
          "forename": "Honglak",
          "surname": "Lee",
          "name": "Honglak Lee",
          "email": ""
        },
        {
          "forename": "Kyomin",
          "surname": "Jung",
          "name": "Kyomin Jung",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics",
      "date": "2024"
    },
    {
      "index": "b267",
      "title": "Tree of thoughts: Deliberate problem solving with large language models",
      "author": [
        {
          "forename": "Shunyu",
          "surname": "Yao",
          "name": "Shunyu Yao",
          "email": ""
        },
        {
          "forename": "Dian",
          "surname": "Yu",
          "name": "Dian Yu",
          "email": ""
        },
        {
          "forename": "Jeffrey",
          "surname": "Zhao",
          "name": "Jeffrey Zhao",
          "email": ""
        },
        {
          "forename": "Izhak",
          "surname": "Shafran",
          "name": "Izhak Shafran",
          "email": ""
        },
        {
          "forename": "Tom",
          "surname": "Griffiths",
          "name": "Tom Griffiths",
          "email": ""
        },
        {
          "forename": "Yuan",
          "surname": "Cao",
          "name": "Yuan Cao",
          "email": ""
        },
        {
          "forename": "Karthik",
          "surname": "Narasimhan",
          "name": "Karthik Narasimhan",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Advances in Neural Information Processing Systems",
      "date": "2024"
    },
    {
      "index": "b268",
      "title": "Self-Judge: Selective Instruction Following with Alignment Self-Evaluation",
      "author": [
        {
          "forename": "Hai",
          "surname": "Ye",
          "name": "Hai Ye",
          "email": ""
        },
        {
          "forename": "Hwee Tou",
          "surname": "Ng",
          "name": "Hwee Tou Ng",
          "email": ""
        }
      ],
      "doi": "arXiv:2409.00935",
      "venue": "Self-Judge: Selective Instruction Following with Alignment Self-Evaluation",
      "date": "2024"
    },
    {
      "index": "b269",
      "title": "Justice or prejudice? quantifying biases in llm-as-a-judge",
      "author": [
        {
          "forename": "Jiayi",
          "surname": "Ye",
          "name": "Jiayi Ye",
          "email": ""
        },
        {
          "forename": "Yanbo",
          "surname": "Wang",
          "name": "Yanbo Wang",
          "email": ""
        },
        {
          "forename": "Yue",
          "surname": "Huang",
          "name": "Yue Huang",
          "email": ""
        },
        {
          "forename": "Dongping",
          "surname": "Chen",
          "name": "Dongping Chen",
          "email": ""
        },
        {
          "forename": "Qihui",
          "surname": "Zhang",
          "name": "Qihui Zhang",
          "email": ""
        },
        {
          "forename": "Nuno",
          "surname": "Moniz",
          "name": "Nuno Moniz",
          "email": ""
        },
        {
          "forename": "Tian",
          "surname": "Gao",
          "name": "Tian Gao",
          "email": ""
        },
        {
          "forename": "Werner",
          "surname": "Geyer",
          "name": "Werner Geyer",
          "email": ""
        },
        {
          "forename": "Chao",
          "surname": "Huang",
          "name": "Chao Huang",
          "email": ""
        },
        {
          "forename": "Pin-Yu",
          "surname": "Chen",
          "name": "Pin-Yu Chen",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.02736",
      "venue": "Justice or prejudice? quantifying biases in llm-as-a-judge",
      "date": "2024"
    },
    {
      "index": "b270",
      "title": "Selfee: Iterative self-revising llm empowered by self-feedback generation",
      "author": [
        {
          "forename": "Seonghyeon",
          "surname": "Ye",
          "name": "Seonghyeon Ye",
          "email": ""
        },
        {
          "forename": "Yongrae",
          "surname": "Jo",
          "name": "Yongrae Jo",
          "email": ""
        },
        {
          "forename": "Doyoung",
          "surname": "Kim",
          "name": "Doyoung Kim",
          "email": ""
        },
        {
          "forename": "Sungdong",
          "surname": "Kim",
          "name": "Sungdong Kim",
          "email": ""
        },
        {
          "forename": "Hyeonbin",
          "surname": "Hwang",
          "name": "Hyeonbin Hwang",
          "email": ""
        },
        {
          "forename": "Minjoon",
          "surname": "Seo",
          "name": "Minjoon Seo",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Blog post",
      "date": "2023"
    },
    {
      "index": "b271",
      "title": "Flask: Fine-grained language model evaluation based on alignment skill sets",
      "author": [
        {
          "forename": "Seonghyeon",
          "surname": "Ye",
          "name": "Seonghyeon Ye",
          "email": ""
        },
        {
          "forename": "Doyoung",
          "surname": "Kim",
          "name": "Doyoung Kim",
          "email": ""
        },
        {
          "forename": "Sungdong",
          "surname": "Kim",
          "name": "Sungdong Kim",
          "email": ""
        },
        {
          "forename": "Hyeonbin",
          "surname": "Hwang",
          "name": "Hyeonbin Hwang",
          "email": ""
        },
        {
          "forename": "Seungone",
          "surname": "Kim",
          "name": "Seungone Kim",
          "email": ""
        },
        {
          "forename": "Yongrae",
          "surname": "Jo",
          "name": "Yongrae Jo",
          "email": ""
        },
        {
          "forename": "James",
          "surname": "Thorne",
          "name": "James Thorne",
          "email": ""
        },
        {
          "forename": "Juho",
          "surname": "Kim",
          "name": "Juho Kim",
          "email": ""
        },
        {
          "forename": "Minjoon",
          "surname": "Seo",
          "name": "Minjoon Seo",
          "email": ""
        }
      ],
      "doi": "arXiv:2307.10928",
      "venue": "Flask: Fine-grained language model evaluation based on alignment skill sets",
      "date": "2023"
    },
    {
      "index": "b272",
      "title": "Beyond Scalar Reward Model: Learning Generative Judge from Preference Data",
      "author": [
        {
          "forename": "Ziyi",
          "surname": "Ye",
          "name": "Ziyi Ye",
          "email": ""
        },
        {
          "forename": "Xiangsheng",
          "surname": "Li",
          "name": "Xiangsheng Li",
          "email": ""
        },
        {
          "forename": "Qiuchi",
          "surname": "Li",
          "name": "Qiuchi Li",
          "email": ""
        },
        {
          "forename": "Qingyao",
          "surname": "Ai",
          "name": "Qingyao Ai",
          "email": ""
        },
        {
          "forename": "Yujia",
          "surname": "Zhou",
          "name": "Yujia Zhou",
          "email": ""
        },
        {
          "forename": "Wei",
          "surname": "Shen",
          "name": "Wei Shen",
          "email": ""
        },
        {
          "forename": "Dong",
          "surname": "Yan",
          "name": "Dong Yan",
          "email": ""
        },
        {
          "forename": "Yiqun",
          "surname": "Liu",
          "name": "Yiqun Liu",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.03742",
      "venue": "Beyond Scalar Reward Model: Learning Generative Judge from Preference Data",
      "date": "2024"
    },
    {
      "index": "b273",
      "title": "ProtocoLLM: Automatic Evaluation Framework of LLMs on Domain-Specific Scientific Protocol Formulation Tasks",
      "author": [
        {
          "forename": "Seungjun",
          "surname": "Yi",
          "name": "Seungjun Yi",
          "email": ""
        },
        {
          "forename": "Jaeyoung",
          "surname": "Lim",
          "name": "Jaeyoung Lim",
          "email": ""
        },
        {
          "forename": "Juyong",
          "surname": "Yoon",
          "name": "Juyong Yoon",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.04601",
      "venue": "ProtocoLLM: Automatic Evaluation Framework of LLMs on Domain-Specific Scientific Protocol Formulation Tasks",
      "date": "2024"
    },
    {
      "index": "b274",
      "title": "Overview of the Tenth Dialog System Technology Challenge: DSTC10",
      "author": [
        {
          "forename": "Koichiro",
          "surname": "Yoshino",
          "name": "Koichiro Yoshino",
          "email": ""
        },
        {
          "forename": "Yun-Nung",
          "surname": "Chen",
          "name": "Yun-Nung Chen",
          "email": ""
        },
        {
          "forename": "Paul",
          "surname": "Crook",
          "name": "Paul Crook",
          "email": ""
        },
        {
          "forename": "Satwik",
          "surname": "Kottur",
          "name": "Satwik Kottur",
          "email": ""
        },
        {
          "forename": "Jinchao",
          "surname": "Li",
          "name": "Jinchao Li",
          "email": ""
        },
        {
          "forename": "Behnam",
          "surname": "Hedayatnia",
          "name": "Behnam Hedayatnia",
          "email": ""
        },
        {
          "forename": "Seungwhan",
          "surname": "Moon",
          "name": "Seungwhan Moon",
          "email": ""
        },
        {
          "forename": "Zhengcong",
          "surname": "Fei",
          "name": "Zhengcong Fei",
          "email": ""
        },
        {
          "forename": "Zekang",
          "surname": "Li",
          "name": "Zekang Li",
          "email": ""
        },
        {
          "forename": "Jinchao",
          "surname": "Zhang",
          "name": "Jinchao Zhang",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
      "date": "2023"
    },
    {
      "index": "b275",
      "title": "Kieval: A knowledge-grounded interactive evaluation framework for large language models",
      "author": [
        {
          "forename": "Zhuohao",
          "surname": "Yu",
          "name": "Zhuohao Yu",
          "email": ""
        },
        {
          "forename": "Chang",
          "surname": "Gao",
          "name": "Chang Gao",
          "email": ""
        },
        {
          "forename": "Wenjin",
          "surname": "Yao",
          "name": "Wenjin Yao",
          "email": ""
        },
        {
          "forename": "Yidong",
          "surname": "Wang",
          "name": "Yidong Wang",
          "email": ""
        },
        {
          "forename": "Wei",
          "surname": "Ye",
          "name": "Wei Ye",
          "email": ""
        },
        {
          "forename": "Jindong",
          "surname": "Wang",
          "name": "Jindong Wang",
          "email": ""
        },
        {
          "forename": "Xing",
          "surname": "Xie",
          "name": "Xing Xie",
          "email": ""
        },
        {
          "forename": "Yue",
          "surname": "Zhang",
          "name": "Yue Zhang",
          "email": ""
        },
        {
          "forename": "Shikun",
          "surname": "Zhang",
          "name": "Shikun Zhang",
          "email": ""
        }
      ],
      "doi": "arXiv:2402.15043",
      "venue": "Kieval: A knowledge-grounded interactive evaluation framework for large language models",
      "date": "2024"
    },
    {
      "index": "b276",
      "title": "Self-rewarding language models",
      "author": [
        {
          "forename": "Weizhe",
          "surname": "Yuan",
          "name": "Weizhe Yuan",
          "email": ""
        },
        {
          "forename": "Richard Yuanzhe ",
          "surname": "Pang",
          "name": "Richard Yuanzhe  Pang",
          "email": ""
        },
        {
          "forename": "Kyunghyun",
          "surname": "Cho",
          "name": "Kyunghyun Cho",
          "email": ""
        },
        {
          "forename": "Sainbayar",
          "surname": "Sukhbaatar",
          "name": "Sainbayar Sukhbaatar",
          "email": ""
        },
        {
          "forename": "Jing",
          "surname": "Xu",
          "name": "Jing Xu",
          "email": ""
        },
        {
          "forename": "Jason",
          "surname": "Weston",
          "name": "Jason Weston",
          "email": ""
        }
      ],
      "doi": "arXiv:2401.10020",
      "venue": "Self-rewarding language models",
      "date": "2024"
    },
    {
      "index": "b277",
      "title": "Disc-lawllm: Fine-tuning large language models for intelligent legal services",
      "author": [
        {
          "forename": "Shengbin",
          "surname": "Yue",
          "name": "Shengbin Yue",
          "email": ""
        },
        {
          "forename": "Wei",
          "surname": "Chen",
          "name": "Wei Chen",
          "email": ""
        },
        {
          "forename": "Siyuan",
          "surname": "Wang",
          "name": "Siyuan Wang",
          "email": ""
        },
        {
          "forename": "Bingxuan",
          "surname": "Li",
          "name": "Bingxuan Li",
          "email": ""
        },
        {
          "forename": "Chenchen",
          "surname": "Shen",
          "name": "Chenchen Shen",
          "email": ""
        },
        {
          "forename": "Shujun",
          "surname": "Liu",
          "name": "Shujun Liu",
          "email": ""
        },
        {
          "forename": "Yuxuan",
          "surname": "Zhou",
          "name": "Yuxuan Zhou",
          "email": ""
        },
        {
          "forename": "Yao",
          "surname": "Xiao",
          "name": "Yao Xiao",
          "email": ""
        },
        {
          "forename": "Song",
          "surname": "Yun",
          "name": "Song Yun",
          "email": ""
        },
        {
          "forename": "Xuanjing",
          "surname": "Huang",
          "name": "Xuanjing Huang",
          "email": ""
        }
      ],
      "doi": "arXiv:2309.11325",
      "venue": "Disc-lawllm: Fine-tuning large language models for intelligent legal services",
      "date": "2023"
    },
    {
      "index": "b278",
      "title": "Automatic evaluation of attribution by large language models",
      "author": [
        {
          "forename": "Xiang",
          "surname": "Yue",
          "name": "Xiang Yue",
          "email": ""
        },
        {
          "forename": "Boshi",
          "surname": "Wang",
          "name": "Boshi Wang",
          "email": ""
        },
        {
          "forename": "Ziru",
          "surname": "Chen",
          "name": "Ziru Chen",
          "email": ""
        },
        {
          "forename": "Kai",
          "surname": "Zhang",
          "name": "Kai Zhang",
          "email": ""
        },
        {
          "forename": "Yu",
          "surname": "Su",
          "name": "Yu Su",
          "email": ""
        },
        {
          "forename": "Huan",
          "surname": "Sun",
          "name": "Huan Sun",
          "email": ""
        }
      ],
      "doi": "arXiv:2305.06311",
      "venue": "Automatic evaluation of attribution by large language models",
      "date": "2023"
    },
    {
      "index": "b279",
      "title": "STaR: Self-taught reasoner bootstrapping reasoning with reasoning",
      "author": [
        {
          "forename": "Eric",
          "surname": "Zelikman",
          "name": "Eric Zelikman",
          "email": ""
        },
        {
          "forename": "Jesse",
          "surname": "Wu",
          "name": "Jesse Wu",
          "email": ""
        },
        {
          "forename": "Noah D",
          "surname": "Mu",
          "name": "Noah D Mu",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proc. the 36th International Conference on Neural Information Processing Systems",
      "date": "2024"
    },
    {
      "index": "b280",
      "title": "Automatic Instruction Evolving for Large Language Models",
      "author": [
        {
          "forename": "Weihao",
          "surname": "Zeng",
          "name": "Weihao Zeng",
          "email": ""
        },
        {
          "forename": "Can",
          "surname": "Xu",
          "name": "Can Xu",
          "email": ""
        },
        {
          "forename": "Yingxiu",
          "surname": "Zhao",
          "name": "Yingxiu Zhao",
          "email": ""
        },
        {
          "forename": "Jian-Guang",
          "surname": "Lou",
          "name": "Jian-Guang Lou",
          "email": ""
        },
        {
          "forename": "Weizhu",
          "surname": "Chen",
          "name": "Weizhu Chen",
          "email": ""
        }
      ],
      "doi": "arXiv:2406.00770",
      "venue": "Automatic Instruction Evolving for Large Language Models",
      "date": "2024"
    },
    {
      "index": "b281",
      "title": "Automatic evaluation and moderation of open-domain dialogue systems",
      "author": [
        {
          "forename": "Chen",
          "surname": "Zhang",
          "name": "Chen Zhang",
          "email": ""
        },
        {
          "forename": "João",
          "surname": "Sedoc",
          "name": "João Sedoc",
          "email": ""
        },
        {
          "forename": "Luis",
          "surname": "Fernando",
          "name": "Luis Fernando",
          "email": ""
        },
        {
          "forename": "D'",
          "surname": "Haro",
          "name": "D' Haro",
          "email": ""
        },
        {
          "forename": "Rafael",
          "surname": "Banchs",
          "name": "Rafael Banchs",
          "email": ""
        },
        {
          "forename": "Alexander",
          "surname": "Rudnicky",
          "name": "Alexander Rudnicky",
          "email": ""
        }
      ],
      "doi": "arXiv:2111.02110",
      "venue": "Automatic evaluation and moderation of open-domain dialogue systems",
      "date": "2021"
    },
    {
      "index": "b282",
      "title": "TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot",
      "author": [
        {
          "forename": "Kaiqi",
          "surname": "Zhang",
          "name": "Kaiqi Zhang",
          "email": ""
        },
        {
          "forename": "Shuai",
          "surname": "Yuan",
          "name": "Shuai Yuan",
          "email": ""
        },
        {
          "forename": "Honghan",
          "surname": "Zhao",
          "name": "Honghan Zhao",
          "email": ""
        }
      ],
      "doi": "arXiv:2407.10999",
      "venue": "TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot",
      "date": "2024"
    },
    {
      "index": "b283",
      "title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References",
      "author": [
        {
          "forename": "Qiyuan",
          "surname": "Zhang",
          "name": "Qiyuan Zhang",
          "email": ""
        },
        {
          "forename": "Yufei",
          "surname": "Wang",
          "name": "Yufei Wang",
          "email": ""
        },
        {
          "forename": "Tiezheng",
          "surname": "Yu",
          "name": "Tiezheng Yu",
          "email": ""
        },
        {
          "forename": "Yuxin",
          "surname": "Jiang",
          "name": "Yuxin Jiang",
          "email": ""
        },
        {
          "forename": "Chuhan",
          "surname": "Wu",
          "name": "Chuhan Wu",
          "email": ""
        },
        {
          "forename": "Liangyou",
          "surname": "Li",
          "name": "Liangyou Li",
          "email": ""
        },
        {
          "forename": "Yasheng",
          "surname": "Wang",
          "name": "Yasheng Wang",
          "email": ""
        },
        {
          "forename": "Xin",
          "surname": "Jiang",
          "name": "Xin Jiang",
          "email": ""
        },
        {
          "forename": "Lifeng",
          "surname": "Shang",
          "name": "Lifeng Shang",
          "email": ""
        },
        {
          "forename": "Ruiming",
          "surname": "Tang",
          "name": "Ruiming Tang",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.05193",
      "venue": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References",
      "date": "2024"
    },
    {
      "index": "b284",
      "title": "Llmaaa: Making large language models as active annotators",
      "author": [
        {
          "forename": "Ruoyu",
          "surname": "Zhang",
          "name": "Ruoyu Zhang",
          "email": ""
        },
        {
          "forename": "Yanzeng",
          "surname": "Li",
          "name": "Yanzeng Li",
          "email": ""
        },
        {
          "forename": "Yongliang",
          "surname": "Ma",
          "name": "Yongliang Ma",
          "email": ""
        },
        {
          "forename": "Ming",
          "surname": "Zhou",
          "name": "Ming Zhou",
          "email": ""
        },
        {
          "forename": "Lei",
          "surname": "Zou",
          "name": "Lei Zou",
          "email": ""
        }
      ],
      "doi": "arXiv:2310.19596",
      "venue": "Llmaaa: Making large language models as active annotators",
      "date": "2023"
    },
    {
      "index": "b285",
      "title": "Personalizing dialogue agents: I have a dog",
      "author": [
        {
          "forename": "Saizheng",
          "surname": "Zhang",
          "name": "Saizheng Zhang",
          "email": ""
        }
      ],
      "doi": "arXiv:1801.07243",
      "venue": "Personalizing dialogue agents: I have a dog",
      "date": "2018"
    },
    {
      "index": "b286",
      "title": "Large language models as evaluators for recommendation explanations",
      "author": [
        {
          "forename": "Xiaoyu",
          "surname": "Zhang",
          "name": "Xiaoyu Zhang",
          "email": ""
        },
        {
          "forename": "Yishan",
          "surname": "Li",
          "name": "Yishan Li",
          "email": ""
        },
        {
          "forename": "Jiayin",
          "surname": "Wang",
          "name": "Jiayin Wang",
          "email": ""
        },
        {
          "forename": "Bowen",
          "surname": "Sun",
          "name": "Bowen Sun",
          "email": ""
        },
        {
          "forename": "Weizhi",
          "surname": "Ma",
          "name": "Weizhi Ma",
          "email": ""
        },
        {
          "forename": "Peijie",
          "surname": "Sun",
          "name": "Peijie Sun",
          "email": ""
        },
        {
          "forename": "Min",
          "surname": "Zhang",
          "name": "Min Zhang",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the 18th ACM Conference on Recommender Systems",
      "date": "2024-12"
    },
    {
      "index": "b287",
      "title": "Wider and deeper llm networks are fairer llm evaluators",
      "author": [
        {
          "forename": "Xinghua",
          "surname": "Zhang",
          "name": "Xinghua Zhang",
          "email": ""
        },
        {
          "forename": "Bowen",
          "surname": "Yu",
          "name": "Bowen Yu",
          "email": ""
        },
        {
          "forename": "Haiyang",
          "surname": "Yu",
          "name": "Haiyang Yu",
          "email": ""
        },
        {
          "forename": "Yangyu",
          "surname": "Lv",
          "name": "Yangyu Lv",
          "email": ""
        },
        {
          "forename": "Tingwen",
          "surname": "Liu",
          "name": "Tingwen Liu",
          "email": ""
        },
        {
          "forename": "Fei",
          "surname": "Huang",
          "name": "Fei Huang",
          "email": ""
        },
        {
          "forename": "Hongbo",
          "surname": "Xu",
          "name": "Hongbo Xu",
          "email": ""
        },
        {
          "forename": "Yongbin",
          "surname": "Li",
          "name": "Yongbin Li",
          "email": ""
        }
      ],
      "doi": "arXiv:2308.01862",
      "venue": "Wider and deeper llm networks are fairer llm evaluators",
      "date": "2023"
    },
    {
      "index": "b288",
      "title": "Auto Arena of LLMs",
      "author": [
        {
          "forename": "Ruochen",
          "surname": "Zhao",
          "name": "Ruochen Zhao",
          "email": ""
        },
        {
          "forename": "Wenxuan",
          "surname": "Zhang",
          "name": "Wenxuan Zhang",
          "email": ""
        },
        {
          "forename": "Yew Ken ",
          "surname": "Chia",
          "name": "Yew Ken  Chia",
          "email": ""
        },
        {
          "forename": "Deli",
          "surname": "Zhao",
          "name": "Deli Zhao",
          "email": ""
        },
        {
          "forename": "Lidong",
          "surname": "Bing",
          "name": "Lidong Bing",
          "email": ""
        }
      ],
      "doi": "arXiv:2405.20267",
      "venue": "Automating LLM Evaluations with Agent Peer-battles and Committee Discussions",
      "date": "2024"
    },
    {
      "index": "b290",
      "title": "Measuring the inconsistency of large language models in preferential ranking",
      "author": [
        {
          "forename": "Xiutian",
          "surname": "Zhao",
          "name": "Xiutian Zhao",
          "email": ""
        },
        {
          "forename": "Ke",
          "surname": "Wang",
          "name": "Ke Wang",
          "email": ""
        },
        {
          "forename": "Wei",
          "surname": "Peng",
          "name": "Wei Peng",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.08851",
      "venue": "Measuring the inconsistency of large language models in preferential ranking",
      "date": "2024"
    },
    {
      "index": "b291",
      "title": "Ruifang He, and Yuexian Hou. 2023. Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models",
      "author": [
        {
          "forename": "Yachao",
          "surname": "Zhao",
          "name": "Yachao Zhao",
          "email": ""
        },
        {
          "forename": "Bo",
          "surname": "Wang",
          "name": "Bo Wang",
          "email": ""
        },
        {
          "forename": "Dongming",
          "surname": "Zhao",
          "name": "Dongming Zhao",
          "email": ""
        },
        {
          "forename": "Kun",
          "surname": "Huang",
          "name": "Kun Huang",
          "email": ""
        },
        {
          "forename": "Yan",
          "surname": "Wang",
          "name": "Yan Wang",
          "email": ""
        }
      ],
      "doi": "arXiv:2308.12578",
      "venue": "Ruifang He, and Yuexian Hou. 2023. Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models",
      "date": "2023"
    },
    {
      "index": "b292",
      "title": "Calibrate before use: Improving few-shot performance of language models",
      "author": [
        {
          "forename": "Zihao",
          "surname": "Zhao",
          "name": "Zihao Zhao",
          "email": ""
        },
        {
          "forename": "Eric",
          "surname": "Wallace",
          "name": "Eric Wallace",
          "email": ""
        },
        {
          "forename": "Shi",
          "surname": "Feng",
          "name": "Shi Feng",
          "email": ""
        },
        {
          "forename": "Dan",
          "surname": "Klein",
          "name": "Dan Klein",
          "email": ""
        },
        {
          "forename": "Sameer",
          "surname": "Singh",
          "name": "Sameer Singh",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "International conference on machine learning. PMLR",
      "date": "2021"
    },
    {
      "index": "b293",
      "title": "Large language models are not robust multiple choice selectors",
      "author": [
        {
          "forename": "Chujie",
          "surname": "Zheng",
          "name": "Chujie Zheng",
          "email": ""
        },
        {
          "forename": "Hao",
          "surname": "Zhou",
          "name": "Hao Zhou",
          "email": ""
        },
        {
          "forename": "Fandong",
          "surname": "Meng",
          "name": "Fandong Meng",
          "email": ""
        },
        {
          "forename": "Jie",
          "surname": "Zhou",
          "name": "Jie Zhou",
          "email": ""
        },
        {
          "forename": "Minlie",
          "surname": "Huang",
          "name": "Minlie Huang",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "The Twelfth International Conference on Learning Representations",
      "date": "2023"
    },
    {
      "index": "b294",
      "title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
      "author": [
        {
          "forename": "Lianmin",
          "surname": "Zheng",
          "name": "Lianmin Zheng",
          "email": ""
        },
        {
          "forename": "Wei-Lin",
          "surname": "Chiang",
          "name": "Wei-Lin Chiang",
          "email": ""
        },
        {
          "forename": "Ying",
          "surname": "Sheng",
          "name": "Ying Sheng",
          "email": ""
        },
        {
          "forename": "Siyuan",
          "surname": "Zhuang",
          "name": "Siyuan Zhuang",
          "email": ""
        },
        {
          "forename": "Zhanghao",
          "surname": "Wu",
          "name": "Zhanghao Wu",
          "email": ""
        },
        {
          "forename": "Yonghao",
          "surname": "Zhuang",
          "name": "Yonghao Zhuang",
          "email": ""
        },
        {
          "forename": "Zi",
          "surname": "Lin",
          "name": "Zi Lin",
          "email": ""
        },
        {
          "forename": "Zhuohan",
          "surname": "Li",
          "name": "Zhuohan Li",
          "email": ""
        },
        {
          "forename": "Dacheng",
          "surname": "Li",
          "name": "Dacheng Li",
          "email": ""
        },
        {
          "forename": "Eric",
          "surname": "Xing",
          "name": "Eric Xing",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Advances in Neural Information Processing Systems",
      "date": "2023"
    },
    {
      "index": "b295",
      "title": "Cheating automatic llm benchmarks: Null models achieve high win rates",
      "author": [
        {
          "forename": "Xiaosen",
          "surname": "Zheng",
          "name": "Xiaosen Zheng",
          "email": ""
        },
        {
          "forename": "Tianyu",
          "surname": "Pang",
          "name": "Tianyu Pang",
          "email": ""
        },
        {
          "forename": "Chao",
          "surname": "Du",
          "name": "Chao Du",
          "email": ""
        },
        {
          "forename": "Qian",
          "surname": "Liu",
          "name": "Qian Liu",
          "email": ""
        },
        {
          "forename": "Jing",
          "surname": "Jiang",
          "name": "Jing Jiang",
          "email": ""
        },
        {
          "forename": "Min",
          "surname": "Lin",
          "name": "Min Lin",
          "email": ""
        }
      ],
      "doi": "arXiv:2410.07137",
      "venue": "Cheating automatic llm benchmarks: Null models achieve high win rates",
      "date": "2024"
    },
    {
      "index": "b296",
      "title": "Mitigating the Bias of Large Language Model Evaluation",
      "author": [
        {
          "forename": "Hongli",
          "surname": "Zhou",
          "name": "Hongli Zhou",
          "email": ""
        },
        {
          "forename": "Hui",
          "surname": "Huang",
          "name": "Hui Huang",
          "email": ""
        },
        {
          "forename": "Yunfei",
          "surname": "Long",
          "name": "Yunfei Long",
          "email": ""
        },
        {
          "forename": "Bing",
          "surname": "Xu",
          "name": "Bing Xu",
          "email": ""
        },
        {
          "forename": "Conghui",
          "surname": "Zhu",
          "name": "Conghui Zhu",
          "email": ""
        },
        {
          "forename": "Hailong",
          "surname": "Cao",
          "name": "Hailong Cao",
          "email": ""
        },
        {
          "forename": "Muyun",
          "surname": "Yang",
          "name": "Muyun Yang",
          "email": ""
        },
        {
          "forename": "Tiejun",
          "surname": "Zhao",
          "name": "Tiejun Zhao",
          "email": ""
        }
      ],
      "doi": "arXiv:2409.16788",
      "venue": "Mitigating the Bias of Large Language Model Evaluation",
      "date": "2024"
    },
    {
      "index": "b297",
      "title": "Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments",
      "author": [
        {
          "forename": "Han",
          "surname": "Zhou",
          "name": "Han Zhou",
          "email": ""
        },
        {
          "forename": "Xingchen",
          "surname": "Wan",
          "name": "Xingchen Wan",
          "email": ""
        },
        {
          "forename": "Yinhong",
          "surname": "Liu",
          "name": "Yinhong Liu",
          "email": ""
        },
        {
          "forename": "Nigel",
          "surname": "Collier",
          "name": "Nigel Collier",
          "email": ""
        },
        {
          "forename": "Ivan",
          "surname": "Vulić",
          "name": "Ivan Vulić",
          "email": ""
        },
        {
          "forename": "Anna",
          "surname": "Korhonen",
          "name": "Anna Korhonen",
          "email": ""
        }
      ],
      "doi": "arXiv:2406.11370",
      "venue": "Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments",
      "date": "2024"
    },
    {
      "index": "b298",
      "title": "Batch calibration: Rethinking calibration for in-context learning and prompt engineering",
      "author": [
        {
          "forename": "Han",
          "surname": "Zhou",
          "name": "Han Zhou",
          "email": ""
        },
        {
          "forename": "Xingchen",
          "surname": "Wan",
          "name": "Xingchen Wan",
          "email": ""
        },
        {
          "forename": "Lev",
          "surname": "Proleev",
          "name": "Lev Proleev",
          "email": ""
        },
        {
          "forename": "Diana",
          "surname": "Mincu",
          "name": "Diana Mincu",
          "email": ""
        },
        {
          "forename": "Jilin",
          "surname": "Chen",
          "name": "Jilin Chen",
          "email": ""
        },
        {
          "forename": "Katherine",
          "surname": "Heller",
          "name": "Katherine Heller",
          "email": ""
        },
        {
          "forename": "Subhrajit",
          "surname": "Roy",
          "name": "Subhrajit Roy",
          "email": ""
        }
      ],
      "doi": "arXiv:2309.17249",
      "venue": "Batch calibration: Rethinking calibration for in-context learning and prompt engineering",
      "date": "2023"
    },
    {
      "index": "b299",
      "title": "Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks",
      "author": [
        {
          "forename": "Ruiyang",
          "surname": "Zhou",
          "name": "Ruiyang Zhou",
          "email": ""
        },
        {
          "forename": "Lu",
          "surname": "Chen",
          "name": "Lu Chen",
          "email": ""
        },
        {
          "forename": "Kai",
          "surname": "Yu",
          "name": "Kai Yu",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024",
      "date": "2024"
    },
    {
      "index": "b300",
      "title": "Sotopia: Interactive evaluation for social intelligence in language agents",
      "author": [
        {
          "forename": "Xuhui",
          "surname": "Zhou",
          "name": "Xuhui Zhou",
          "email": ""
        },
        {
          "forename": "Hao",
          "surname": "Zhu",
          "name": "Hao Zhu",
          "email": ""
        },
        {
          "forename": "Leena",
          "surname": "Mathur",
          "name": "Leena Mathur",
          "email": ""
        },
        {
          "forename": "Ruohong",
          "surname": "Zhang",
          "name": "Ruohong Zhang",
          "email": ""
        },
        {
          "forename": "Haofei",
          "surname": "Yu",
          "name": "Haofei Yu",
          "email": ""
        },
        {
          "forename": "Zhengyang",
          "surname": "Qi",
          "name": "Zhengyang Qi",
          "email": ""
        },
        {
          "forename": "Louis-Philippe",
          "surname": "Morency",
          "name": "Louis-Philippe Morency",
          "email": ""
        },
        {
          "forename": "Yonatan",
          "surname": "Bisk",
          "name": "Yonatan Bisk",
          "email": ""
        },
        {
          "forename": "Daniel",
          "surname": "Fried",
          "name": "Daniel Fried",
          "email": ""
        },
        {
          "forename": "Graham",
          "surname": "Neubig",
          "name": "Graham Neubig",
          "email": ""
        }
      ],
      "doi": "arXiv:2310.11667",
      "venue": "Sotopia: Interactive evaluation for social intelligence in language agents",
      "date": "2023"
    },
    {
      "index": "b301",
      "title": "Calibrated self-rewarding vision language models",
      "author": [
        {
          "forename": "Yiyang",
          "surname": "Zhou",
          "name": "Yiyang Zhou",
          "email": ""
        },
        {
          "forename": "Zhiyuan",
          "surname": "Fan",
          "name": "Zhiyuan Fan",
          "email": ""
        },
        {
          "forename": "Dongjie",
          "surname": "Cheng",
          "name": "Dongjie Cheng",
          "email": ""
        },
        {
          "forename": "Sihan",
          "surname": "Yang",
          "name": "Sihan Yang",
          "email": ""
        },
        {
          "forename": "Zhaorun",
          "surname": "Chen",
          "name": "Zhaorun Chen",
          "email": ""
        },
        {
          "forename": "Chenhang",
          "surname": "Cui",
          "name": "Chenhang Cui",
          "email": ""
        },
        {
          "forename": "Xiyao",
          "surname": "Wang",
          "name": "Xiyao Wang",
          "email": ""
        },
        {
          "forename": "Yun",
          "surname": "Li",
          "name": "Yun Li",
          "email": ""
        },
        {
          "forename": "Linjun",
          "surname": "Zhang",
          "name": "Linjun Zhang",
          "email": ""
        },
        {
          "forename": "Huaxiu",
          "surname": "Yao",
          "name": "Huaxiu Yao",
          "email": ""
        }
      ],
      "doi": "arXiv:2405.14622",
      "venue": "Calibrated self-rewarding vision language models",
      "date": "2024"
    },
    {
      "index": "b302",
      "title": "Are Large Language Models Rational Investors? arXiv preprint",
      "author": [
        {
          "forename": "Yuhang",
          "surname": "Zhou",
          "name": "Yuhang Zhou",
          "email": ""
        },
        {
          "forename": "Yuchen",
          "surname": "Ni",
          "name": "Yuchen Ni",
          "email": ""
        },
        {
          "forename": "Xiang",
          "surname": "Liu",
          "name": "Xiang Liu",
          "email": ""
        },
        {
          "forename": "Jian",
          "surname": "Zhang",
          "name": "Jian Zhang",
          "email": ""
        },
        {
          "forename": "Sen",
          "surname": "Liu",
          "name": "Sen Liu",
          "email": ""
        },
        {
          "forename": "Guangnan",
          "surname": "Ye",
          "name": "Guangnan Ye",
          "email": ""
        },
        {
          "forename": "Hongfeng",
          "surname": "Chai",
          "name": "Hongfeng Chai",
          "email": ""
        }
      ],
      "doi": "arXiv:2402.12713",
      "venue": "Are Large Language Models Rational Investors? arXiv preprint",
      "date": "2024"
    },
    {
      "index": "b303",
      "title": "Judgelm: Fine-tuned large language models are scalable judges",
      "author": [
        {
          "forename": "Lianghui",
          "surname": "Zhu",
          "name": "Lianghui Zhu",
          "email": ""
        },
        {
          "forename": "Xinggang",
          "surname": "Wang",
          "name": "Xinggang Wang",
          "email": ""
        },
        {
          "forename": "Xinlong",
          "surname": "Wang",
          "name": "Xinlong Wang",
          "email": ""
        }
      ],
      "doi": "arXiv:2310.17631",
      "venue": "Judgelm: Fine-tuned large language models are scalable judges",
      "date": "2023"
    },
    {
      "index": "b304",
      "title": "A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models",
      "author": [
        {
          "forename": "Shengyao",
          "surname": "Zhuang",
          "name": "Shengyao Zhuang",
          "email": ""
        },
        {
          "forename": "Honglei",
          "surname": "Zhuang",
          "name": "Honglei Zhuang",
          "email": ""
        },
        {
          "forename": "Bevan",
          "surname": "Koopman",
          "name": "Bevan Koopman",
          "email": ""
        },
        {
          "forename": "Guido",
          "surname": "Zuccon",
          "name": "Guido Zuccon",
          "email": ""
        }
      ],
      "doi": "10.1145/3626772.3657813",
      "venue": "Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "date": "2024"
    },
    {
      "index": "b306",
      "title": "ICE-Score: Instructing Large Language Models to Evaluate Code",
      "author": [
        {
          "forename": "Zhuo",
          "surname": "Terry Yue",
          "name": "Zhuo Terry Yue",
          "email": ""
        }
      ],
      "doi": "arXiv:2304.14317",
      "venue": "ICE-Score: Instructing Large Language Models to Evaluate Code",
      "date": "2023"
    },
    {
      "index": "b307",
      "title": "Universal and transferable adversarial attacks on aligned language models",
      "author": [
        {
          "forename": "Andy",
          "surname": "Zou",
          "name": "Andy Zou",
          "email": ""
        },
        {
          "forename": "Zifan",
          "surname": "Wang",
          "name": "Zifan Wang",
          "email": ""
        },
        {
          "forename": "Nicholas",
          "surname": "Carlini",
          "name": "Nicholas Carlini",
          "email": ""
        },
        {
          "forename": "Milad",
          "surname": "Nasr",
          "name": "Milad Nasr",
          "email": ""
        },
        {
          "forename": "Zico",
          "surname": "Kolter",
          "name": "Zico Kolter",
          "email": ""
        },
        {
          "forename": "Matt",
          "surname": "Fredrikson",
          "name": "Matt Fredrikson",
          "email": ""
        }
      ],
      "doi": "arXiv:2307.15043",
      "venue": "Universal and transferable adversarial attacks on aligned language models",
      "date": "2023"
    }
  ]
}