{
  "title": "Beyond Factual Accuracy: Evaluating Coverage of Diverse Factual Information in Long-form Text Generation",
  "publication": {
    "publisher": {},
    "date": ""
  },
  "author": [
    {
      "forename": "Chris",
      "surname": "Samarinas",
      "name": "Chris Samarinas",
      "email": "csamarinas@cs.umass.edu"
    },
    {
      "forename": "Alexander",
      "surname": "Krubner",
      "name": "Alexander Krubner",
      "email": ""
    },
    {
      "forename": "Alireza",
      "surname": "Salemi",
      "name": "Alireza Salemi",
      "email": "asalemi@cs.umass.edu"
    },
    {
      "forename": "Youngwoo",
      "surname": "Kim",
      "name": "Youngwoo Kim",
      "email": "youngwookim@cs.umass.edu"
    },
    {
      "forename": "Hamed",
      "surname": "Zamani",
      "name": "Hamed Zamani",
      "email": "zamani@cs.umass.edu"
    }
  ],
  "abstract": [
    [
      "This paper presents ICAT, an evaluation framework for measuring coverage of diverse factual information in long-form text generation. ICAT breaks down a long output text into a list of atomic claims and not only verifies each claim through retrieval from a (reliable) knowledge source, but also computes the alignment between the atomic factual claims and various aspects expected to be presented in the output. We study three implementations of the ICAT framework, each with a different assumption on the availability of aspects and alignment method. By adopting data from the diversification task in the TREC Web Track and the ClueWeb corpus, we evaluate the ICAT framework. We demonstrate strong correlation with human judgments and provide comprehensive evaluation across multiple state-ofthe-art LLMs. Our framework further offers interpretable and fine-grained analysis of diversity and coverage. Its modular design allows for easy adaptation to different domains and datasets, making it a valuable tool for evaluating the qualitative aspects of long-form responses produced by LLMs. * Equal contribution. † Work done while visiting the Center for Intelligent Information Retrieval at UMass Amherst."
    ]
  ],
  "body": [
    {
      "section": {
        "index": "1",
        "name": "Introduction"
      },
      "p": [
        {
          "text": "The rapid advancement of Large Language Models (LLMs) has revolutionized long-form text generation, enabling increasingly sophisticated applications from report writing to complex question answering. However, this progress has highlighted a critical challenge: how do we effectively evaluate not just the factual accuracy of generated content, but also its completeness and coverage of diverse perspectives? While recent work has made significant progress in developing comprehensive evaluation frameworks for LLMs (Li et al., 2024), existing metrics often focus on isolated aspects like factuality or response quality (Min et al., 2023), failing to capture the multi-dimensional nature of high-quality long-form text.",
          "quote": [
            {
              "text": "(Li et al., 2024)",
              "target": "#b15",
              "type": "bibr",
              "context": " for LLMs ",
              "index": 515
            },
            {
              "text": "(Min et al., 2023)",
              "target": "#b22",
              "type": "bibr",
              "context": "e quality ",
              "index": 619
            }
          ]
        },
        {
          "text": "The evaluation of long-form text generation presents unique challenges that go beyond traditional metrics (Samarinas et al., 2024). Lexical overlap metrics like BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) are fundamentally limited by their reliance on surface-level text similarity, making them inadequate for evaluating semantically equivalent but lexically different expressions. While more recent approaches like BERTScore (Zhang et al., 2019) and G-Eval (Liu et al., 2023) attempt to address this through semantic similarity, they still face fundamental limitations when applied to longform content. These metrics cannot effectively verify factual accuracy or assess whether the response comprehensively covers all relevant aspects of a topic. The vast space of possible acceptable outputs makes it impractical to create comprehensive reference texts, leading to the development of reference-free evaluation methodologies like Prism (Agrawal et al., 2021; Thompson and Post, 2020). However, these approaches often struggle to effectively identify hallucinations and biases.",
          "quote": [
            {
              "text": "(Samarinas et al., 2024)",
              "target": "#b27",
              "type": "bibr",
              "context": "l metrics ",
              "index": 106
            },
            {
              "text": "(Papineni et al., 2002)",
              "target": "#b24",
              "type": "bibr",
              "context": "like BLEU ",
              "index": 166
            },
            {
              "text": "(Lin, 2004)",
              "target": "#b17",
              "type": "bibr",
              "context": "2), ROUGE ",
              "index": 197
            },
            {
              "text": "(Banerjee and Lavie, 2005)",
              "target": "#b1",
              "type": "bibr",
              "context": "nd METEOR ",
              "index": 221
            },
            {
              "text": "(Zhang et al., 2019)",
              "target": "#b37",
              "type": "bibr",
              "context": "BERTScore ",
              "index": 469
            },
            {
              "text": "(Liu et al., 2023)",
              "target": "",
              "type": "bibr",
              "context": "nd G-Eval ",
              "index": 501
            },
            {
              "text": "Thompson and Post, 2020)",
              "target": "#b31",
              "type": "bibr",
              "context": "l., 2021; ",
              "index": 1003
            },
            {
              "text": "[(Agrawaletal.,2021]",
              "type": "bibr",
              "index": 980,
              "context": "ike Prism ",
              "target": "bNaN"
            },
            {
              "text": "[]",
              "type": "bibr",
              "index": 980,
              "context": "ike Prism ",
              "target": "bNaN"
            }
          ]
        },
        {
          "text": "Recent approaches like FActScore (Min et al., 2023) and VERISCORE (Song et al., 2024) have addressed the factuality challenge by evaluating atomic claims against reliable sources, but factual accuracy alone is insufficient. A unified metric that considers both factuality and coverage is crucial not only for evaluation but also for optimizing LLM performance. Such a metric could serve as a reward function in reinforcement learning to simultaneously improve both the factual accuracy and comprehensive coverage of LLM outputs.",
          "quote": [
            {
              "text": "(Min et al., 2023)",
              "target": "#b22",
              "type": "bibr",
              "context": "FActScore ",
              "index": 33
            },
            {
              "text": "(Song et al., 2024)",
              "target": "#b29",
              "type": "bibr",
              "context": "VERISCORE ",
              "index": 66
            }
          ]
        },
        {
          "text": "Consider a user asking \"What are the health effects of coffee consumption?\" While an LLM response might present entirely factual claims about coffee's benefits, such as its role in improving alert-arXiv:2501.03545v3 [cs.CL] 17 Feb 2025 ness and potential protective effects against certain diseases, failing to address known health risks (like anxiety or sleep disruption) would present an incomplete and potentially misleading picture. This illustrates a critical gap in current evaluation approaches: the need to assess not just the accuracy of individual claims, but also the comprehensive coverage of diverse relevant information. This is particularly crucial for applications like medical information systems, policy analysis, text summarization, or educational content where balanced, complete information is essential for informed decision making.",
          "quote": [
            {
              "text": "[cs.CL]",
              "target": "",
              "type": "bibr",
              "context": "1.03545v3 ",
              "index": 216
            }
          ]
        },
        {
          "text": "This paper introduces ICAT, 1 a novel evaluation framework that addresses this gap by measuring both factual accuracy and coverage of diverse factual information in long-form text generation. Our key contributions include: 1) a modular evaluation framework that decomposes long-form text into atomic claims and evaluates both their factual accuracy and their alignment with expected aspects, 2) three implementations with varying degrees of automation, suitable for different evaluation scenarios, and 3) a comprehensive evaluation of various LLMs and demonstrating strong correlation with human judgments in a user study.",
          "quote": []
        },
        {
          "text": "ICAT first breaks down the generated long text into atomic claims. Through retrieval from a (reliable) corpus C or the Web, ICAT verifies each atomic claim to ensure its factuality. To measure completeness and coverage of diverse facts, ICAT requires a set of diverse aspects to compute an alignment between each atomic factual claim in the LLM response and the set of diverse aspects. We study three implementations of the ICAT framework as follows: ICAT-M assumes that a groundtruth set of diverse claims are obtained manually and is available to the evaluation framework. It also assumes that the groundtruth relevance annotation for each document in the corpus C to each aspect is provided. Using this information, the retrieval model can identify which aspect is being covered by each atomic factual claim in the LLM response. ICAT-S similarly assumes that a groundtruth set of diverse claims are obtained manually, however no aspect-level relevance judgment is available. Therefore, it uses an LLM to conduct pseudo-labeling and performing alignment between the atomic factual claims and the set of aspects. On the other hand, ICAT-A assumes that the aspect set is not 1 source code: https://github.com/algoprog/ICAT available, so it first uses an LLM to automatically generate diverse aspects of the input and then conduct pseudo-labeling for alignment, as is done in the second variant.",
          "quote": []
        },
        {
          "text": "In our experiments, we use ClueWeb (The Lemur Project, 2009) as the retrieval corpus. We solely focus on the English documents of the ClueWeb collection. In addition to custom open-source retrieval from ClueWeb, we explore web-based grounding using the Brave Search API. For experiments, we rely on the input queries from the TREC Web Track (Clarke et al., 2009 . The argument for this decision is based on the fact that TREC Web Track queries have also been used for search result diversification. This means that the queries include up to 7 aspects and documents are provided with aspect-level relevance annotations. Our experiments show that there is relatively strong corelation of ICAT with human judgments, showcasing the utility of this framework for evaluating coverage of diverse factual information in LLM responses without human input.(Clarke et al., , 2010 ",
          "quote": [
            {
              "text": "(Clarke et al., 2009",
              "target": "#b2",
              "type": "bibr",
              "context": "Web Track ",
              "index": 341
            },
            {
              "text": "(Clarke et al., , 2010",
              "target": "#b3",
              "type": "bibr",
              "context": "al., 2009 ",
              "index": 362
            },
            {
              "text": "(Clarke et al., , 2011",
              "target": "",
              "type": "bibr"
            },
            {
              "text": "(Clarke et al., , 2012))",
              "target": "#b5",
              "type": "bibr"
            }
          ]
        },
        {
          "text": "By offering a modular and adaptable framework, ICAT enables researchers to tailor the evaluation process to specific needs, making it a valuable tool for assessing the qualitative aspects of long-form responses produced by LLMs. The decomposition of LLM outputs into atomic claims and their alignment with specific topics makes the evaluation process highly interpretable -evaluators can trace exactly which claims support which topics and identify gaps in coverage. This granular analysis capability, combined with the framework's ability to evaluate both factual accuracy and topic coverage, provides a more comprehensive assessment compared to existing metrics that only measure one of these aspects.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "2",
        "name": "Related Work"
      },
      "p": [
        {
          "text": "Text Generation Evaluation Traditional approaches to evaluating generated text have primarily focused on n-gram overlap metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005). While these metrics are effective for assessing local coherence and fluency, they fail to capture higher-level aspects such as topic coverage and diversity. Recent work has introduced more sophisticated metrics like BERTScore (Zhang et al., 2019), BLEURT (Sellam et al., 2020), and unified multi-dimensional evaluators (Zhong et al., 2022) which leverage pre- trained LLMs for more nuanced evaluation.",
          "quote": [
            {
              "text": "(Papineni et al., 2002)",
              "target": "#b24",
              "type": "bibr",
              "context": "h as BLEU ",
              "index": 141
            },
            {
              "text": "(Lin, 2004)",
              "target": "#b17",
              "type": "bibr",
              "context": "2), ROUGE ",
              "index": 172
            },
            {
              "text": "(Banerjee and Lavie, 2005)",
              "target": "#b1",
              "type": "bibr",
              "context": "nd METEOR ",
              "index": 196
            },
            {
              "text": "(Zhang et al., 2019)",
              "target": "#b37",
              "type": "bibr",
              "context": "BERTScore ",
              "index": 450
            },
            {
              "text": "(Sellam et al., 2020)",
              "target": "#b28",
              "type": "bibr",
              "context": "), BLEURT ",
              "index": 479
            },
            {
              "text": "(Zhong et al., 2022)",
              "target": "#b38",
              "type": "bibr",
              "context": "valuators ",
              "index": 543
            }
          ]
        },
        {
          "text": "Topic Coverage and Diversity Research on evaluating topic coverage has roots in information retrieval, where metrics like α-nDCG (Clarke et al., 2008) and S-Recall were used to assess the topical diversity of search results. The concept of diversity in evaluating generated text can encompass various interpretations, including lexical diversity (analyzing the variety of words used) and topical diversity (assessing the range of topics covered). In the context of text generation, recent work has explored various approaches to measuring lexical diversity, including term overlap self-similarity such as Self-BLEU (Zhu et al., 2018) and the proportion of distinct unigrams and bigrams in generated responses (Li et al., 2016). However, research on evaluation of topical diversity in LLMs is currently limited.",
          "quote": [
            {
              "text": "(Clarke et al., 2008)",
              "target": "#b6",
              "type": "bibr",
              "context": "ke α-nDCG ",
              "index": 129
            },
            {
              "text": "(Zhu et al., 2018)",
              "target": "#b39",
              "type": "bibr",
              "context": "Self-BLEU ",
              "index": 615
            },
            {
              "text": "(Li et al., 2016)",
              "target": "#b16",
              "type": "bibr",
              "context": "responses ",
              "index": 709
            }
          ]
        },
        {
          "text": "LLM Evaluation Frameworks Several frameworks have been proposed for evaluating different aspects of LLM performance, including factuality (Min et al., 2023; Song et al., 2024) and dialogue quality (Mehri and Eskenazi, 2020). Concurrent to this research, the AutoNuggetizer framework (Pradeep et al., 2024) used LLMs to generate and assess the coverage of nuggets in text. The EXAM++ framework (Farzi and Dietz, 2024) evaluates information coverage by checking if system responses can answer query-related exam questions, focusing on high-level answerability. In contrast, our work provides more fine-grained explainability by aligning individual atomic claims with retrieved evidence to verify both factual accuracy and aspect coverage at a more granular level. This allows us to not only assess if an aspect is covered, but also identify the specific claims and evidence supporting that coverage.",
          "quote": [
            {
              "text": "Song et al., 2024)",
              "target": "#b29",
              "type": "bibr",
              "context": "l., 2023; ",
              "index": 157
            },
            {
              "text": "(Mehri and Eskenazi, 2020)",
              "target": "#b19",
              "type": "bibr",
              "context": "e quality ",
              "index": 197
            },
            {
              "text": "(Pradeep et al., 2024)",
              "target": "#b25",
              "type": "bibr",
              "context": "framework ",
              "index": 283
            },
            {
              "text": "(Farzi and Dietz, 2024)",
              "target": "#b10",
              "type": "bibr",
              "context": "framework ",
              "index": 393
            },
            {
              "text": "[(Minetal.,2023]",
              "type": "bibr",
              "index": 138,
              "context": "actuality ",
              "target": "bNaN"
            },
            {
              "text": "[]",
              "type": "bibr",
              "index": 138,
              "context": "actuality ",
              "target": "bNaN"
            }
          ]
        },
        {
          "text": "Our work builds upon these foundations, specifically addressing the challenge of evaluating topic coverage in long-form text generation while considering factuality at the same time.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "3",
        "name": "ICAT"
      },
      "p": [
        {
          "text": "Queries that require a long-form response, e.g., complex non-factoid questions, are often associated with multiple aspects. The response to these queries often include multiple claims, some of which may be factually accurate, while others may be inaccurate. An ideal response to these queries should not only contain factually accurate claims, but should also leave no aspect or perspective unaddressed. For instance, an ideal answer to a question about a legislation should cover perspectives from all political parties. An ideal answer to a question about the impact of a food or a medication on health should cover both positive, neutral, and negative perspectives. However, no existing evaluation metric can evaluate both factual accuracy and aspect coverage in long-form text generation. To address these, given a long output y produced in response to an input x, the ICAT framework computes two main scores: factuality score and coverage score.",
          "quote": []
        },
        {
          "text": "Factuality Score. Building upon prior work, such as FActScore (Min et al., 2023) and VERISCORE (Song et al., 2024), Factuality Score measures the ratio (or percentage) of factually ac-Approach for Obtaining Diverse Aspects Approach for Claim-Aspect Alignment ICAT-M Manual: ground-truth aspects Manual: retrieval-based method with aspect-level ground-truth alignment ICAT-S Manual: ground-truth aspects Automatic: retrieval-based method with aspect-level LLM-based alignment ICAT-A Automatic: LLM-based aspect generation Automatic: retrieval-based method with aspect-level LLM-based alignment curate claims in y. To do so, it is crucial that the generated claims are accurate. Let AC(y) be a function that extracts atomic claims from a generated response y. Given the set of atomic claims C = AC(y) made in y, we define the function C T = CG(C; K) that verifies the factuality of claims in C using a given knowledge source K.",
          "quote": [
            {
              "text": "(Min et al., 2023)",
              "target": "#b22",
              "type": "bibr",
              "context": "FActScore ",
              "index": 62
            },
            {
              "text": "(Song et al., 2024)",
              "target": "#b29",
              "type": "bibr",
              "context": "VERISCORE ",
              "index": 95
            }
          ]
        },
        {
          "text": "Therefore, C T ⊆ C denotes the set of factually verified claims in response y. Factuality Score is then defined as follows:",
          "quote": []
        },
        {
          "text": "where | • | denotes the cardinality of the given set.",
          "quote": []
        },
        {
          "text": "Coverage Score. To evaluate information coverage and diversity in y, Coverage Score measures the ratio of aspects being covered by the factually accurate claims in the given text. Hence, it is essential to identify which query aspects are accurately addressed in the generated response. Formally, coverage score can be defined as:",
          "quote": []
        },
        {
          "text": "where T O is a function that identifies the subtopics associated with claim c, and T Q is a function that returns all aspects related to the input x. Note that aspect coverage is only computed for factually verified claims, i.e., C T , instead of all claims. The reason is that non-factual claims should be avoided, regardless of the aspect they cover. Thus, they should not contribute to the coverage score.",
          "quote": []
        },
        {
          "text": "The ICAT β Score. Inspired by F-measure (Van Rijsbergen, 1979), we calculate the weighted harmonic average of these factuality and coverage scores, as follows:",
          "quote": [
            {
              "text": "(Van Rijsbergen, 1979)",
              "target": "#b34",
              "type": "bibr",
              "context": "F-measure ",
              "index": 40
            }
          ]
        },
        {
          "text": "where parameter β is a hyper-parameter that controls the trade-off between the factuality and coverage scores. In more detail, β controls the weight of Coverage Score compared to Factuality Score. Thus, a higher β signifies the impact of information coverage, while a lower β prioritizes factual accuracy. The default value for β is equal to 1, where factuality and coverage score are weighted uniformly. Throughout this paper, when the value of β is not explicitly mentioned, the default value of 1 is being used.",
          "quote": []
        },
        {
          "text": "Variants of ICAT We study three variants of ICAT implementations based on how they obtain query aspects and compute the alignment between atomic claims and aspects. Table  describes the approaches used in these three variants and highlights their differences.",
          "quote": []
        },
        {
          "text": "The rest of this section provides details on how to develop models for generating factual claims (i.e., function AC), how to validate the factuality of claims (i.e., function CG), and how to obtain query aspects and compute an alignment between factual claims and all query aspects (i.e., functions T Q and T O). The rest of this section describes the approaches at high level to introduce the generic ICAT, while Section 4 provides the implementation details used in our experiments.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "3.1",
        "name": "Atomic Claim Generation"
      },
      "p": [
        {
          "text": "The atomic claim generation process seeks to break down a given long text into standalone and atomic claim statements that preserve key context and maintain claim consistency (see Figure (Min et al., 2023)). The generated claims should strike an appropriate balance of granularity, ensuring they are self-contained and decontextualized. We assume the existence of a function C = AC(y), which returns a set of atomic claims C, given the long output text y. There are various ways to implement this; one might consider each sentence or paragraph in y as an atomic claim. However, this simple approach does not satisfy our expected self-containment and decontextualization qualities. Instead, we follow (Grattafiori et al., 2024) and utilize an LLM M claims with the prompt shown in Figure . This prompt instructs the LLM to decompose the generated response y into multiple self-explanatory and decontextualized sentences, each containing a single atomic fact. These sentences then constitute the set of atomic claims for the generated output, denoted as C = AC(y). An example of this process is illustrated in Figure . An instruction-tuned LLM can be used as M claims in a zero-of few-shot setting; however, we found that smaller-scale LLMs (such as LLaMA  with 8 billion parameters) cannot accurately perform this task without fine-tuning. Therefore, we either use an LLM with higher capacity or distill knowledge into a smaller-scale LLM, enabling faster inference for our framework. The details of this distillation process are provided in section 4.1.",
          "quote": [
            {
              "text": "(Min et al., 2023)",
              "target": "#b22",
              "type": "bibr",
              "context": "ee Figure ",
              "index": 187
            },
            {
              "text": "(Grattafiori et al., 2024)",
              "target": "#b11",
              "type": "bibr",
              "context": "we follow ",
              "index": 700
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "3.2",
        "name": "Claim Grounding"
      },
      "p": [
        {
          "text": "To design the claim grounding function CG, for each claim c ∈ C, we employ a retrieval model R to retrieve n documents from the given knowledge source K. Subsequently, a natural language inference (NLI) model M NLI is used to determine whether the claim can be supported by any of the retrieved documents. If the claim can be inferred from at least one of the retrieved documents, it is considered grounded (i.e., validated, thus factually accurate); otherwise, it is not. The function returns a subset of C that are found grounded.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "3.3",
        "name": "Aspect Coverage Assessment"
      },
      "p": [
        {
          "text": "To calculate aspect coverage for some given input prompt (query) x, it is essential to have a list of diverse aspects for x (i.e., T Q(x)) and a method to determine which aspect each claim pertains to (i.e., T O(c, K) : c ∈ C).",
          "quote": []
        },
        {
          "text": "Methods for Obtaining Diverse Query Aspects (T Q): We propose two main methods to identify all aspects related to the query x:",
          "quote": []
        },
        {
          "text": "• Manual-Ground-truth Aspects: In this case, the aspects that should be included in the re-sponse to the query x are provided as a reference for evaluation.",
          "quote": []
        },
        {
          "text": "• Automatic-LLM-based Aspect Generation: Building on previous work showing LLMs can effectively identify aspects of a query (Samarinas et al., 2022), we use an LLM M subtopic with the prompt shown in Figure . This prompt instructs the LLM to generate up to 10 aspects for the query, covering the key aspects about it. This approach is useful when ground-truth aspects are unavailable.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": -1,
        "name": "Methods of Obtaining the Aspects of an Atomic Claim (T O):"
      },
      "p": [
        {
          "text": "We use two methods to identify the aspects related to an atomic claim c:",
          "quote": []
        },
        {
          "text": "• Manual-retrieval-based method with aspectlevel ground-truth alignment: In this method, we assume access to a knowledge source K, where each document is annotated with the aspects it covers. To find the aspects that the claim c covers, we use the retrieval model R to retrieve n documents. Then, according to the ranking, we find the first ranked document that supports the claim c using the method in Section 3.2. The aspects of this document are considered as the aspects that the claim c covers. If none of the documents support claim c, we assume that it does not cover any query aspect.",
          "quote": []
        },
        {
          "text": "• Automatic-retrieval-based method with aspect-level LLM-based alignment: In this method, we use an aspect-claim alignment LLM M coverage to determine which aspects each grounded claim covers. Given a query x, its aspects T Q(x), and a set of grounded claims C T , we prompt the LLM to analyze each claim and identify which aspects it addresses. The prompt (shown in Figure ) instructs the LLM to output a structured mapping between claims and aspects, where each claim can be mapped to zero, one, or multiple aspects. This approach eliminates the need for aspect-level relevance judgments in the knowledge source while still maintaining a retrieval-based verification of factual accuracy. Unlike the manual method that assumes a claim covers the aspects associated with its supporting document, this method directly analyzes the semantic relationship between claims and aspects, leading to more accurate assessment. Though ICAT-A shows weaker correlation than ICAT-S, we observed that auto-generated topics were more comprehensive and higher quality than the existing ones in the TREC dataset, suggesting ICAT-A would perform better with more complete ground-truth topics.",
          "quote": []
        },
        {
          "text": "4 Implementation Details",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "4.1",
        "name": "Atomic Claim Generation"
      },
      "p": [
        {
          "text": "The claim generation module was trained with several key objectives in mind, building on recent work in atomic claim extraction (Min et al., 2023). The model learned to extract standalone factual statements from text while maintaining factual consistency and simplifying complex statements. Special attention was paid to preserving important context and qualifiers, and generating claims at an appropriate granularity level (Song et al., 2024). For this task we used Llama 3.1 8B (Grattafiori et al., 2024) fine-tuned using QLoRA (Dettmers et al., 2023) on synthetic examples. We tried using models of this size without fine-tuning, however we found that the generated claims are often not de-contextualized properly. Larger models with 70B or more parameters seem to be effective for this task without fine-tuning, however they are very expensive to run, especially for long texts. The synthetic training data was generated through a multi-stage process by prompting Llama 3.1 405B. We began by generating 200 diverse high-level topics across multiple domains. For each topic, we generated 5 relevant entities. We then created variable-length paragraphs for each entity and generated the associated list of atomic claims for each of them. Using these 1000 synthetic examples, we fine-tuned the model for 1 epoch with batch size 16, learning rate 2e-4 and LoRA parameters α = 16 and rank = 64.",
          "quote": [
            {
              "text": "(Min et al., 2023)",
              "target": "#b22",
              "type": "bibr",
              "context": "xtraction ",
              "index": 128
            },
            {
              "text": "(Song et al., 2024)",
              "target": "#b29",
              "type": "bibr",
              "context": "ity level ",
              "index": 424
            },
            {
              "text": "(Grattafiori et al., 2024)",
              "target": "#b11",
              "type": "bibr",
              "context": "ma 3.1 8B ",
              "index": 480
            },
            {
              "text": "(Dettmers et al., 2023)",
              "target": "#b8",
              "type": "bibr",
              "context": "ing QLoRA ",
              "index": 530
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "4.2",
        "name": "Topic Generation"
      },
      "p": [
        {
          "text": "Previous works have shown that LLMs can be very effective in query subtopic generation (Samarinas et al., 2022). In our framework, for generating ground-truth topics given a query, we use the same base LLM as the one used in claim generation. Here we found that even without fine-tuning, Llama 3.1 8B can produce relevant topics. In order to reduce the need for extra resources to use a base and fine-tuned version of the LLM for claim generation, we use the VLLM library (Kwon et al., 2023) to load the base model only once in memory and efficiently serve the adapter for the fine-tuned version.",
          "quote": [
            {
              "text": "(Samarinas et al., 2022)",
              "target": "#b26",
              "type": "bibr",
              "context": "eneration ",
              "index": 87
            },
            {
              "text": "(Kwon et al., 2023)",
              "target": "#b14",
              "type": "bibr",
              "context": "M library ",
              "index": 472
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "4.3",
        "name": "Claim Grounding"
      },
      "p": [
        {
          "text": "We implemented a two-stage approach for grounding atomic claims in the given text with a corpus. We first preprocess the corpus and generate chunks for each document with up to 128 words with 32 words overlap. We use a dense embedding model 2 (Merrick et al., 2024) to produce embeddings for all snippets and FAISS (Johnson et al., 2019) to build an efficient approximate nearest-neighbor index. We used IVF with HNSW for cluster assignment as our index type for fast search even when providing a large-scale corpus.",
          "quote": [
            {
              "text": "(Merrick et al., 2024)",
              "target": "#b21",
              "type": "bibr",
              "context": "g model 2 ",
              "index": 243
            },
            {
              "text": "(Johnson et al., 2019)",
              "target": "#b13",
              "type": "bibr",
              "context": "and FAISS ",
              "index": 315
            }
          ]
        },
        {
          "text": "In the first stage, a retriever is used to obtain the k = 10 most relevant snippets in the corpus for each claim. When web search is used instead of a corpus, we use the returned snippets from Brave Search API. In the second stage, a natural language inference model is used to filter only the supported claims. We use a model based on DeBERTa V3 (He et al., 2021) fine-tuned on MultiNLI, FEVER and Adversarial NLI (Williams et al., 2018; Thorne et al., 2018; Nie et al., 2020). A claim is kept if there is at least one snippet that supports it. Instead of using a LLM for filtering the supported claims, we use a much smaller BERT based model (Devlin et al., 2019) fine-tuned specifically for this task.",
          "quote": [
            {
              "text": "(He et al., 2021)",
              "target": "#b12",
              "type": "bibr",
              "context": "eBERTa V3 ",
              "index": 347
            },
            {
              "text": "Thorne et al., 2018;",
              "target": "#b33",
              "type": "bibr",
              "context": "l., 2018; ",
              "index": 439
            },
            {
              "text": "Nie et al., 2020)",
              "target": "#b23",
              "type": "bibr",
              "context": "l., 2018; ",
              "index": 460
            },
            {
              "text": "(Devlin et al., 2019)",
              "target": "#b9",
              "type": "bibr",
              "context": "sed model ",
              "index": 644
            },
            {
              "text": "[(Williamsetal.,2018]",
              "type": "bibr",
              "index": 415,
              "context": "arial NLI ",
              "target": "bNaN"
            },
            {
              "text": "[]",
              "type": "bibr",
              "index": 415,
              "context": "arial NLI ",
              "target": "bNaN"
            }
          ]
        },
        {
          "text": "We limit the snippet length because both NLI and dense embeddings models based on small pretrained transformer LMs like BERT tend to have lower performance as the input length increases.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "4.4",
        "name": "Aspect-Claim Alignment"
      },
      "p": [
        {
          "text": "Topic coverage is assessed using the same base LLM with claim and topic generation. Given a query, a list of enumerated atomic claims and a list of ground truth topics, the LLM is prompted to produce a list of covered topic ids with their associated claim ids in structured jsonl format.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "5",
        "name": "Experimental Setup"
      },
      "p": [
        {
          "text": "Dataset. We conducted our experiments using the ClueWeb09 Category B corpus-a large-scale web collection with over 50 million English documents (The Lemur Project, 2009). This corpus has been used in TREC Web Track from 2009, consisting of 200 topics, derived from a commercial search engine's query log, balanced for popularity. Each topic includes a topic title (i.e., often used as the keyword search query), a description (i.e., detailed description of the information need), type, and subtopics (i.e., diverse aspects of the topic). The relevance judgments encompass 38,637 query-document pairs, with 19.06% (7366) marked as relevant. The dataset's unique advantage lies in its comprehensive coverage of internet content and human-annotated relevance judgments for topical diversity assessment. Relevance was judged either binary or on a five-point scale (later converted to binary), with documents considered relevant when containing useful information for specific subtopics. In our experiments with this collection, we filtered out spam documents using the Waterloo spam scorer to 2012 with the threshold of 70%. We used BM25 to retrieve 1000 documents for each topic (given its title as the query string) and considered these documents for retrieval in our factual verification process. The query set comprises 179 carefully selected faceted queries, each containing 3-8 subtopics representing different aspects of the information need.",
          "quote": [
            {
              "text": "(The Lemur Project, 2009)",
              "target": "",
              "type": "bibr",
              "context": "documents ",
              "index": 144
            },
            {
              "text": "TREC Web Track from 2009",
              "target": "",
              "type": "bibr",
              "context": "n used in ",
              "index": 200
            },
            {
              "text": "to 2012",
              "target": "#b5",
              "type": "bibr",
              "context": "am scorer ",
              "index": 1086
            },
            {
              "text": "(Clarke et al., 2009",
              "target": "#b2",
              "type": "bibr"
            },
            {
              "text": ", 2010",
              "target": "#b3",
              "type": "bibr"
            },
            {
              "text": ", 2011",
              "target": "",
              "type": "bibr"
            },
            {
              "text": ", 2012)",
              "target": "#b5",
              "type": "bibr"
            },
            {
              "text": "(Cormack et al., 2011)",
              "target": "#b7",
              "type": "bibr"
            }
          ]
        },
        {
          "text": "Experimental Setup. We evaluated four stateof-the-art LLMs: GPT-4, Llama-3-70B-Instruct, Mixtral-8x22B-Instruct-v0.1, and Openchat 3.5 (a fine-tune of Mistral-7B) (Wang et al., 2023). For each model, we generated responses for each test query. For the baselines in this paper, we used the query descriptions in their original format from the ClueWeb09 dataset as prompts, which are not optimized for producing very diverse outputs.",
          "quote": [
            {
              "text": "(Wang et al., 2023)",
              "target": "",
              "type": "bibr",
              "context": "stral-7B) ",
              "index": 163
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "6",
        "name": "Experimental Results"
      },
      "p": [
        {
          "text": "Human Evaluation Study. To validate ICAT's effectiveness, we conducted a comprehensive human evaluation study using Amazon Mechanical Turk (AMT). For each query-answer pair, three independent annotators assessed the coverage of aspects through a custom interface (Figure  in Appendix A.4). We limited the HITs to adult workers from the US, UK, Australia and Ireland, with over 98% approval rate who have completed at least 5,000 assignments. The annotators were tasked with identifying whether specific aspects are present in a given LLM-generated text and highlighting corresponding text evidence for each identified aspect. To ensure quality annotations, we provided detailed guidelines with two reference examples. We use majority voting across annotators. The study achieved substantial inter-annotator agreement with Fleiss's κ = 0.829, which is considered as a substantial agreement. For each query, we calculated Coverage Scores based on the set of covered topics identified by each evaluation method (ICAT variants) and by human annotators, relative to the set of ground truth topics. These per-query coverage scores were then used to compute linear and rank-bsaed correlation metrics (i.e., Pearson's ρ, Spearman's ρ, and Kendall's τ ) between the automated ICAT methods and human judgments.",
          "quote": []
        },
        {
          "text": "The correlation analysis between ICAT variants and human judgments (see Table ) reveals strong performance across most evaluation methods. Using Llama-3.1-70B as the coverage model, ICAT-S achieves the strongest correlations (Pearson's ρ = 0.422, p < 0.01; Spearman's ρ = 0.446, p < 0.01). Interestingly, ICAT-A demonstrates better performance than ICAT-M despite not requiring groundtruth aspects. While ICAT-A seems to have much weaker correlation than ICAT-S, it is worth noting that in practice, we observed the automatically generated topics to be higher quality and more exhaustive. Using a more comprehensive set of groundtruth topics, ICAT-A would probably demonstrate higher correlation with human judgements.",
          "quote": []
        },
        {
          "text": "Comparing factuality and coverage of information in state-of-the-art LLMs using ICAT.",
          "quote": []
        },
        {
          "text": "Our experimental results reveal distinct patterns in how different LLMs balance factuality and coverage (see Table ). When using corpus-based retrieval, Llama-3-70B demonstrates superior Coverage Score (0.451), while GPT-4 and Mixtral-8x22B show comparable factuality scores (0.343  and 0.344, respectively). However, Mixtral exhibits notably lower Coverage Score (0.370) compared to GPT-4's 0.416, resulting in lower overall ICAT 1 scores (0.297 vs 0.327). Notably, when employing web-based retrieval, we observe substantially higher factuality scores across all models. This significant improvement can be attributed to the broader knowledge base available through web search, allowing more claims to be successfully grounded. Coverage scores also show improvement with web-based retrieval, though the increase is more modest. This suggests that while web search enables better fact verification, the comprehensiveness of aspect coverage is more dependent on the model's capabilities than the retrieval source.",
          "quote": []
        },
        {
          "text": "Controlling the trade-off between factuality and coverage using β. ",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "7",
        "name": "Conclusions and Future Work"
      },
      "p": [
        {
          "text": "We presented ICAT, a comprehensive framework for evaluating topic coverage in LLM-generated text. Through extensive experimentation using the ClueWeb09 dataset, we demonstrated the framework's effectiveness across different evaluation scenarios, with our best method achieving strong correlation with human judgments. The modular architecture of ICAT enables flexible adaptation to various evaluation requirements, from manual to automatic approaches for aspect identifications and alignment. Our results highlighted several key findings: (1) the importance of sophisticated coverage models in improving evaluation accuracy, (2) the viability of automatic evaluation approaches that maintain comparable performance to methods requiring ground truth annotations, and (3) the framework's ability to provide meaningful assessments across different LLM architectures and scales. In future work, individual components of our ICAT could be improved in terms of effectiveness and efficiency. Additionally, exploring the relationship between model size, evaluation accuracy, and computational efficiency could provide valuable insights for practical applications. Last but not least, the potential bias introduced by using the same or similar LLM when generating ground-truth aspects should be investigated. By using our metric, other works can explore methods for optimizing LLMs to produce more comprehensive outputs.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": -1,
        "name": "Limitations"
      },
      "p": [
        {
          "text": "Our evaluation framework, while showing promising results, suffers from several limitations that should be considered. First, our experiments reveal that even large language models with 70B parameters sometimes struggle with accurate aspect-claim alignment. This suggests that the correlation with human judgments could potentially be improved by specifically optimizing LLMs for this task, either through fine-tuning or more sophisticated prompting strategies. Second, our current implementation uses zeroshot prompting for query aspect generation without systematic evaluation of this component's effectiveness. Future work should explore methods to optimize and rigorously evaluate the aspect generation process, potentially through human evaluation or comparison with expert-curated aspect sets. This could lead to more reliable and comprehensive aspect coverage assessment.",
          "quote": []
        },
        {
          "text": "In addition, there is a potential source of bias when using the same or similar LLM architecture both for generating query aspects and for producing responses for evaluation. This circular dependency might lead to artificially inflated performance metrics if the evaluated model shares similar biases or knowledge patterns with the model used for aspect generation. Future research should investigate the extent of this potential bias and explore methods to mitigate it, such as using diverse model architectures or more comprehensive human-curated aspects for evaluation.",
          "quote": []
        },
        {
          "text": "Last but not least, our current evaluation is limited to English-language content using web-based corpora. This narrow focus excludes evaluation of multilingual capabilities and limits the framework's applicability to other languages and cultures. Additionally, the reliance on web corpora may not be suitable for domains requiring specialized knowledge bases or authoritative sources. Future work should explore extending ICAT to support multilingual evaluation and integration with domain-specific knowledge bases to broaden its applicability across different languages, cultures, and specialized fields.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": -1,
        "name": "A Appendix"
      },
      "p": []
    },
    {
      "section": {
        "index": -1,
        "name": "A.1 Atomic Claim Generation"
      },
      "p": [
        {
          "text": "Our atomic claim generation process transforms complex text into atomic, verifiable statements while preserving essential context, as shown in Figure . The process focuses on creating decontextualized, self-contained claims that each express a single verifiable fact. When breaking down complex sentences, the process maintains important qualifiers, conditions, and temporal information while replacing contextual references with their explicit referents. For instance, a complex sentence about coffee's effects would be decomposed into separate claims about its components and their individual effects, with each claim being fully selfcontained and independently verifiable.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": -1,
        "name": "A.2 Model Performance Analysis"
      },
      "p": [
        {
          "text": "The performance analysis presented in Tables  reveals several important patterns in model behavior. When comparing the 8B and 70B versions of Llama-3.1 as coverage models, we observe that the 8B model generally produces higher raw coverage scores, while the 70B model demonstrates stronger correlation with human judgments. This suggests a trade-off between computational efficiency and evaluation accuracy. The retrieval model comparison shows consistent advantages for dense retrieval methods over traditional BM25, with the Snowflake-Arctic-Embed models showing particular strength in handling queries where simple lexical matching is insufficient. Web-based retrieval consistently produces higher factuality scores compared to corpus-based approaches across all tested models. Table  analyzes the impact of different retrieval models on the correlation between ICAT variants and human judgments. The results show that larger dense retrieval models (Snowflake-Arctic-Embed-L) consistently outperform traditional BM25 across all ICAT variants, with improvements particularly notable in ICAT 2 and 3.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": -1,
        "name": "A.3 Prompting Details"
      },
      "p": [
        {
          "text": "The framework employs three specialized prompts, each carefully designed for its specific task. The subtopic coverage assessment prompt (Figure ) requests the identification of covered subtopics from a given list, requiring evidence in the form of fact numbers that explicitly appear in the text. The prompt specifies a structured JSON output format where each line contains a topic ID and its supporting evidence. The atomic claim generation prompt (Figure ) focuses on extracting decontextualized, self-explanatory fact sentences from the input text, emphasizing the importance of resolved pronouns and independent context. The topic generation prompt (Figure ) elicits possible subtopics or related queries for a given query, requiring them to be ordered by importance and formatted as JSON objects, with a maximum of 10 topics.  ",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": -1,
        "name": "A.4 Human Evaluation Interface"
      },
      "p": [
        {
          "text": "The human evaluation interface shown in Figure  was developed to facilitate consistent and accurate topic coverage assessment. The interface presents the query and response in a split-screen layout, enabling annotators to highlight evidence for different topics using a color-coded system. To ensure annotation quality, we implemented minimum time requirements per response and included attention check questions. Annotators were guided to read responses completely before beginning their annotations and to identify text spans that provide direct evidence for topic coverage. Table : Comparative analysis of retrieval model impact on ICAT variants' correlation with human judgments, using Llama-3.1-70B for topic-claim alignment. Results demonstrate the superiority of dense retrieval models (Snowflake-Arctic-Embed) over traditional BM25 across all ICAT variants, with the largest improvements seen in ICAT-S and ICAT-A. The analysis includes three correlation metrics (Pearson's, Spearman's, and Kendall's) to provide a comprehensive view of alignment with human assessments across different statistical measures.  Figure : Topic generation prompt used to automatically generate diverse subtopics for a given query. The prompt requests possible subtopics or related queries ordered by importance, with output formatted as JSON objects and limited to a maximum of 10 topics. This automated approach enables reference-free evaluation when ground-truth topics are unavailable.",
          "quote": []
        }
      ]
    }
  ],
  "chart": [
    "Figure 1: Retrieval-based evaluation of LLM responses with ICAT. Topic generation and coverage models are optional depending on the chosen evaluation method.",
    "Figure 2: Example of atomic claim generation",
    "Figure 3: ICAT-A β for various LLMs using Llama-3.1-70B as coverage model and the corpus as knowledge source.For low β values where factuality has higher weight, GPT-4 and Mixtral have better performance while for higher values of β, GPT-4 and Llama have higher scores due to higher S coverage .",
    "Figure 4: Human annotation interface showing the supporting evidence highlights from 3 annotators for the given query in the shown LLM response.",
    "Figure 5: Subtopic coverage prompt used to identify which subtopics are covered in a given text. The prompt instructs the model to analyze a list of atomic claims and return covered subtopics with supporting evidence in a structured JSON format. Each response must include topic IDs and corresponding fact numbers that explicitly appear in the text.",
    "Figure 6: Claim generation prompt designed to extract atomic factual statements from input text. The prompt emphasizes the importance of creating decontextualized, self-explanatory sentences with resolved pronouns and independent context, ensuring each generated claim can stand alone as a verifiable statement.",
    "Table 1 :The methods used for obtaining query aspects and claim-aspect alignment in each variant of ICAT.",
    "Table 2 :Correlation of information coverage in the proposed evaluation methods with manual human annotations.",
    "Table 3 :Evaluation of various LLMs using ICAT. Llama-3.1-70B is used for claim-aspect alignment.",
    "Comprehensive evaluation results comparing ICAT-S and ICAT-A variants using Llama-3.1-8B as the coverage model. Results show performance across different LLMs using both corpus-based and web-based retrieval methods, demonstrating consistent improvements in factuality and coverage scores when using web search compared to corpus-based retrieval. The table highlights how different retrieval methods affect both factuality (S fact ) and coverage scores across various model architectures."
  ],
  "reference": [
    {
      "index": "b0",
      "title": "Assessing reference-free peer evaluation for machine translation",
      "author": [
        {
          "forename": "Sweta",
          "surname": "Agrawal",
          "name": "Sweta Agrawal",
          "email": ""
        },
        {
          "forename": "George",
          "surname": "Foster",
          "name": "George Foster",
          "email": ""
        },
        {
          "forename": "Markus",
          "surname": "Freitag",
          "name": "Markus Freitag",
          "email": ""
        },
        {
          "forename": "Colin",
          "surname": "Cherry",
          "name": "Colin Cherry",
          "email": ""
        }
      ],
      "doi": "10.18653/v1/2021.naacl-main.91",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "date": "2021"
    },
    {
      "index": "b1",
      "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
      "author": [
        {
          "forename": "Satanjeev",
          "surname": "Banerjee",
          "name": "Satanjeev Banerjee",
          "email": ""
        },
        {
          "forename": "Alon",
          "surname": "Lavie",
          "name": "Alon Lavie",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
      "date": "2005"
    },
    {
      "index": "b2",
      "title": "Overview of the trec 2009 web track",
      "author": [
        {
          "forename": "L.A.",
          "surname": "Charles",
          "name": "L.A. Charles",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Text Retrieval Conference",
      "date": "2009"
    },
    {
      "index": "b3",
      "title": "Overview of the trec 2010 web track",
      "author": [
        {
          "forename": "L.A.",
          "surname": "Charles",
          "name": "L.A. Charles",
          "email": ""
        },
        {
          "forename": "Nick",
          "surname": "Clarke",
          "name": "Nick Clarke",
          "email": ""
        },
        {
          "forename": "Ian",
          "surname": "Craswell",
          "name": "Ian Craswell",
          "email": ""
        },
        {
          "forename": "Gordon V.",
          "surname": "Soboroff",
          "name": "Gordon V. Soboroff",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Text Retrieval Conference",
      "date": "2010"
    },
    {
      "index": "b4",
      "title": "Overview of the trec 2011 web track",
      "author": [
        {
          "forename": "L.A.",
          "surname": "Charles",
          "name": "L.A. Charles",
          "email": ""
        },
        {
          "forename": "Nick",
          "surname": "Clarke",
          "name": "Nick Clarke",
          "email": ""
        },
        {
          "forename": "Ian",
          "surname": "Craswell",
          "name": "Ian Craswell",
          "email": ""
        },
        {
          "forename": "Ellen M.",
          "surname": "Soboroff",
          "name": "Ellen M. Soboroff",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Text Retrieval Conference",
      "date": "2011"
    },
    {
      "index": "b5",
      "title": "Overview of the trec 2012 web track",
      "author": [
        {
          "forename": "L.A.",
          "surname": "Charles",
          "name": "L.A. Charles",
          "email": ""
        },
        {
          "forename": "Nick",
          "surname": "Clarke",
          "name": "Nick Clarke",
          "email": ""
        },
        {
          "forename": "Ellen M.",
          "surname": "Craswell",
          "name": "Ellen M. Craswell",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Text Retrieval Conference",
      "date": "2012"
    },
    {
      "index": "b6",
      "title": "Novelty and diversity in information retrieval evaluation",
      "author": [
        {
          "forename": "L.A.",
          "surname": "Charles",
          "name": "L.A. Charles",
          "email": ""
        },
        {
          "forename": "Maheedhar",
          "surname": "Clarke",
          "name": "Maheedhar Clarke",
          "email": ""
        },
        {
          "forename": "Gordon V.",
          "surname": "Kolla",
          "name": "Gordon V. Kolla",
          "email": ""
        },
        {
          "forename": "Olga",
          "surname": "Cormack",
          "name": "Olga Cormack",
          "email": ""
        },
        {
          "forename": "Azin",
          "surname": "Vechtomova",
          "name": "Azin Vechtomova",
          "email": ""
        },
        {
          "forename": "Stefan",
          "surname": "Ashkan",
          "name": "Stefan Ashkan",
          "email": ""
        },
        {
          "forename": "Ian",
          "surname": "Büttcher",
          "name": "Ian Büttcher",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the 31st Annual International ACM SI-GIR Conference",
      "date": "2008"
    },
    {
      "index": "b7",
      "title": "Efficient and effective spam filtering and re-ranking for large web datasets",
      "author": [
        {
          "forename": "Gordon V.",
          "surname": "Cormack",
          "name": "Gordon V. Cormack",
          "email": ""
        },
        {
          "forename": "Mark D.",
          "surname": "Smucker",
          "name": "Mark D. Smucker",
          "email": ""
        },
        {
          "forename": "Charles L A ",
          "surname": "Clarke",
          "name": "Charles L A  Clarke",
          "email": ""
        }
      ],
      "doi": "10.1007/s10791-011-9162-z",
      "venue": "Inf. Retr",
      "date": "2011"
    },
    {
      "index": "b8",
      "title": "Qlora: Efficient finetuning of quantized llms",
      "author": [
        {
          "forename": "Tim",
          "surname": "Dettmers",
          "name": "Tim Dettmers",
          "email": ""
        },
        {
          "forename": "Artidoro",
          "surname": "Pagnoni",
          "name": "Artidoro Pagnoni",
          "email": ""
        },
        {
          "forename": "Ari",
          "surname": "Holtzman",
          "name": "Ari Holtzman",
          "email": ""
        },
        {
          "forename": "Luke",
          "surname": "Zettlemoyer",
          "name": "Luke Zettlemoyer",
          "email": ""
        }
      ],
      "doi": "arXiv:2305.14314",
      "venue": "Qlora: Efficient finetuning of quantized llms",
      "date": "2023"
    },
    {
      "index": "b9",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author": [
        {
          "forename": "Jacob",
          "surname": "Devlin",
          "name": "Jacob Devlin",
          "email": ""
        },
        {
          "forename": "Ming-Wei",
          "surname": "Chang",
          "name": "Ming-Wei Chang",
          "email": ""
        },
        {
          "forename": "Kenton",
          "surname": "Lee",
          "name": "Kenton Lee",
          "email": ""
        },
        {
          "forename": "Kristina",
          "surname": "Toutanova",
          "name": "Kristina Toutanova",
          "email": ""
        }
      ],
      "doi": "10.18653/v1/N19-1423",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "date": "2019"
    },
    {
      "index": "b10",
      "title": "An exambased evaluation approach beyond traditional relevance judgments",
      "author": [
        {
          "forename": "Naghmeh",
          "surname": "Farzi",
          "name": "Naghmeh Farzi",
          "email": ""
        },
        {
          "forename": "Laura",
          "surname": "Dietz",
          "name": "Laura Dietz",
          "email": ""
        }
      ],
      "doi": "arXiv:2402.00309",
      "venue": "An exambased evaluation approach beyond traditional relevance judgments",
      "date": "2024"
    },
    {
      "index": "b11",
      "title": "The llama 3 herd of models",
      "author": [
        {
          "forename": "Aaron",
          "surname": "Grattafiori",
          "name": "Aaron Grattafiori",
          "email": ""
        },
        {
          "forename": "Abhimanyu",
          "surname": "Dubey",
          "name": "Abhimanyu Dubey",
          "email": ""
        },
        {
          "forename": "Abhinav",
          "surname": "Jauhri",
          "name": "Abhinav Jauhri",
          "email": ""
        },
        {
          "forename": "Abhinav",
          "surname": "Pandey",
          "name": "Abhinav Pandey",
          "email": ""
        },
        {
          "forename": "Abhishek",
          "surname": "Kadian",
          "name": "Abhishek Kadian",
          "email": ""
        },
        {
          "forename": "Ahmad",
          "surname": "Al-Dahle",
          "name": "Ahmad Al-Dahle",
          "email": ""
        },
        {
          "forename": "Aiesha",
          "surname": "Letman",
          "name": "Aiesha Letman",
          "email": ""
        },
        {
          "forename": "Akhil",
          "surname": "Mathur",
          "name": "Akhil Mathur",
          "email": ""
        }
      ],
      "doi": "arXiv:2407.21783",
      "venue": "The llama 3 herd of models",
      "date": "2024"
    },
    {
      "index": "b12",
      "title": "Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing",
      "author": [
        {
          "forename": "Pengcheng",
          "surname": "He",
          "name": "Pengcheng He",
          "email": ""
        },
        {
          "forename": "Jianfeng",
          "surname": "Gao",
          "name": "Jianfeng Gao",
          "email": ""
        },
        {
          "forename": "Weizhu",
          "surname": "Chen",
          "name": "Weizhu Chen",
          "email": ""
        }
      ],
      "doi": "arXiv:2111.09543",
      "venue": "Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing",
      "date": "2021"
    },
    {
      "index": "b13",
      "title": "Billion-scale similarity search with GPUs",
      "author": [
        {
          "forename": "Jeff",
          "surname": "Johnson",
          "name": "Jeff Johnson",
          "email": ""
        },
        {
          "forename": "Matthijs",
          "surname": "Douze",
          "name": "Matthijs Douze",
          "email": ""
        },
        {
          "forename": "Hervé",
          "surname": "Jégou",
          "name": "Hervé Jégou",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "IEEE Transactions on Big Data",
      "date": "2019"
    },
    {
      "index": "b14",
      "title": "Efficient memory management for large language model serving with pagedattention",
      "author": [
        {
          "forename": "Woosuk",
          "surname": "Kwon",
          "name": "Woosuk Kwon",
          "email": ""
        },
        {
          "forename": "Zhuohan",
          "surname": "Li",
          "name": "Zhuohan Li",
          "email": ""
        },
        {
          "forename": "Siyuan",
          "surname": "Zhuang",
          "name": "Siyuan Zhuang",
          "email": ""
        },
        {
          "forename": "Ying",
          "surname": "Sheng",
          "name": "Ying Sheng",
          "email": ""
        },
        {
          "forename": "Lianmin",
          "surname": "Zheng",
          "name": "Lianmin Zheng",
          "email": ""
        },
        {
          "forename": "Cody Hao ",
          "surname": "Yu",
          "name": "Cody Hao  Yu",
          "email": ""
        },
        {
          "forename": "Joseph E.",
          "surname": "Gonzalez",
          "name": "Joseph E. Gonzalez",
          "email": ""
        },
        {
          "forename": "Hao",
          "surname": "Zhang",
          "name": "Hao Zhang",
          "email": ""
        },
        {
          "forename": "Ion",
          "surname": "Stoica",
          "name": "Ion Stoica",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles",
      "date": "2023"
    },
    {
      "index": "b15",
      "title": "Llms-as-judges: A comprehensive survey on llm-based evaluation methods",
      "author": [
        {
          "forename": "Haitao",
          "surname": "Li",
          "name": "Haitao Li",
          "email": ""
        },
        {
          "forename": "Qian",
          "surname": "Dong",
          "name": "Qian Dong",
          "email": ""
        },
        {
          "forename": "Junjie",
          "surname": "Chen",
          "name": "Junjie Chen",
          "email": ""
        },
        {
          "forename": "Huixue",
          "surname": "Su",
          "name": "Huixue Su",
          "email": ""
        },
        {
          "forename": "Yujia",
          "surname": "Zhou",
          "name": "Yujia Zhou",
          "email": ""
        },
        {
          "forename": "Qingyao",
          "surname": "Ai",
          "name": "Qingyao Ai",
          "email": ""
        },
        {
          "forename": "Ziyi",
          "surname": "Ye",
          "name": "Ziyi Ye",
          "email": ""
        },
        {
          "forename": "Yiqun",
          "surname": "Liu",
          "name": "Yiqun Liu",
          "email": ""
        }
      ],
      "doi": "arXiv:2412.05579",
      "venue": "Llms-as-judges: A comprehensive survey on llm-based evaluation methods",
      "date": "2024"
    },
    {
      "index": "b16",
      "title": "A diversity-promoting objective function for neural conversation models",
      "author": [
        {
          "forename": "Jiwei",
          "surname": "Li",
          "name": "Jiwei Li",
          "email": ""
        },
        {
          "forename": "Michel",
          "surname": "Galley",
          "name": "Michel Galley",
          "email": ""
        },
        {
          "forename": "Chris",
          "surname": "Brockett",
          "name": "Chris Brockett",
          "email": ""
        },
        {
          "forename": "Jianfeng",
          "surname": "Gao",
          "name": "Jianfeng Gao",
          "email": ""
        },
        {
          "forename": "Bill",
          "surname": "Dolan",
          "name": "Bill Dolan",
          "email": ""
        }
      ],
      "doi": "10.18653/v1/N16-1014",
      "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "date": "2016"
    },
    {
      "index": "b17",
      "title": "Rouge: A package for automatic evaluation of summaries",
      "author": [
        {
          "forename": "Chin-Yew",
          "surname": "Lin",
          "name": "Chin-Yew Lin",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Text Summarization Branches Out",
      "date": "2004"
    },
    {
      "index": "b18",
      "title": "G-eval: NLG evaluation using gpt-4 with better human alignment",
      "author": [
        {
          "forename": "Yang",
          "surname": "Liu",
          "name": "Yang Liu",
          "email": ""
        },
        {
          "forename": "Dan",
          "surname": "Iter",
          "name": "Dan Iter",
          "email": ""
        },
        {
          "forename": "Yichong",
          "surname": "Xu",
          "name": "Yichong Xu",
          "email": ""
        },
        {
          "forename": "Shuohang",
          "surname": "Wang",
          "name": "Shuohang Wang",
          "email": ""
        },
        {
          "forename": "Ruochen",
          "surname": "Xu",
          "name": "Ruochen Xu",
          "email": ""
        },
        {
          "forename": "Chenguang",
          "surname": "Zhu",
          "name": "Chenguang Zhu",
          "email": ""
        }
      ],
      "doi": "10.18653/v1/2023.emnlp-main.153",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "date": "2023"
    },
    {
      "index": "b19",
      "title": "USR: An unsupervised and reference free evaluation metric for dialog generation",
      "author": [
        {
          "forename": "Shikib",
          "surname": "Mehri",
          "name": "Shikib Mehri",
          "email": ""
        },
        {
          "forename": "Maxine",
          "surname": "Eskenazi",
          "name": "Maxine Eskenazi",
          "email": ""
        }
      ],
      "doi": "10.18653/v1/2020.acl-main.64",
      "venue": "Proceedings of the 58th",
      "date": "2020"
    },
    {
      "index": "b21",
      "title": "Arctic-embed: Scalable, efficient, and accurate text embedding models",
      "author": [
        {
          "forename": "Luke",
          "surname": "Merrick",
          "name": "Luke Merrick",
          "email": ""
        },
        {
          "forename": "Danmei",
          "surname": "Xu",
          "name": "Danmei Xu",
          "email": ""
        },
        {
          "forename": "Gaurav",
          "surname": "Nuti",
          "name": "Gaurav Nuti",
          "email": ""
        },
        {
          "forename": "Daniel",
          "surname": "Campos",
          "name": "Daniel Campos",
          "email": ""
        }
      ],
      "doi": "abs/2405.05374",
      "venue": "ArXiv",
      "date": "2024"
    },
    {
      "index": "b22",
      "title": "FActScore: Fine-grained atomic evaluation of factual precision in long form text generation",
      "author": [
        {
          "forename": "Sewon",
          "surname": "Min",
          "name": "Sewon Min",
          "email": ""
        },
        {
          "forename": "Kalpesh",
          "surname": "Krishna",
          "name": "Kalpesh Krishna",
          "email": ""
        },
        {
          "forename": "Xinxi",
          "surname": "Lyu",
          "name": "Xinxi Lyu",
          "email": ""
        },
        {
          "forename": "Mike",
          "surname": "Lewis",
          "name": "Mike Lewis",
          "email": ""
        },
        {
          "forename": "Wen-Tau",
          "surname": "Yih",
          "name": "Wen-Tau Yih",
          "email": ""
        },
        {
          "forename": "Pang",
          "surname": "Koh",
          "name": "Pang Koh",
          "email": ""
        },
        {
          "forename": "Mohit",
          "surname": "Iyyer",
          "name": "Mohit Iyyer",
          "email": ""
        },
        {
          "forename": "Luke",
          "surname": "Zettlemoyer",
          "name": "Luke Zettlemoyer",
          "email": ""
        },
        {
          "forename": "Hannaneh",
          "surname": "Hajishirzi",
          "name": "Hannaneh Hajishirzi",
          "email": ""
        }
      ],
      "doi": "10.18653/v1/2023.emnlp-main.741",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "date": "2023"
    },
    {
      "index": "b23",
      "title": "Adversarial NLI: A new benchmark for natural language understanding",
      "author": [
        {
          "forename": "Yixin",
          "surname": "Nie",
          "name": "Yixin Nie",
          "email": ""
        },
        {
          "forename": "Adina",
          "surname": "Williams",
          "name": "Adina Williams",
          "email": ""
        },
        {
          "forename": "Emily",
          "surname": "Dinan",
          "name": "Emily Dinan",
          "email": ""
        },
        {
          "forename": "Mohit",
          "surname": "Bansal",
          "name": "Mohit Bansal",
          "email": ""
        },
        {
          "forename": "Jason",
          "surname": "Weston",
          "name": "Jason Weston",
          "email": ""
        },
        {
          "forename": "Douwe",
          "surname": "Kiela",
          "name": "Douwe Kiela",
          "email": ""
        }
      ],
      "doi": "10.18653/v1/2020.acl-main.441",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "date": "2020"
    },
    {
      "index": "b24",
      "title": "Bleu: a method for automatic evaluation of machine translation",
      "author": [
        {
          "forename": "Kishore",
          "surname": "Papineni",
          "name": "Kishore Papineni",
          "email": ""
        },
        {
          "forename": "Salim",
          "surname": "Roukos",
          "name": "Salim Roukos",
          "email": ""
        },
        {
          "forename": "Todd",
          "surname": "Ward",
          "name": "Todd Ward",
          "email": ""
        },
        {
          "forename": "Wei-Jing",
          "surname": "Zhu",
          "name": "Wei-Jing Zhu",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
      "date": "2002"
    },
    {
      "index": "b25",
      "title": "Initial nugget evaluation results for the trec 2024 rag track with the autonuggetizer framework",
      "author": [
        {
          "forename": "Ronak",
          "surname": "Pradeep",
          "name": "Ronak Pradeep",
          "email": ""
        },
        {
          "forename": "Nandan",
          "surname": "Thakur",
          "name": "Nandan Thakur",
          "email": ""
        },
        {
          "forename": "Shivani",
          "surname": "Upadhyay",
          "name": "Shivani Upadhyay",
          "email": ""
        },
        {
          "forename": "Daniel",
          "surname": "Campos",
          "name": "Daniel Campos",
          "email": ""
        },
        {
          "forename": "Nick",
          "surname": "Craswell",
          "name": "Nick Craswell",
          "email": ""
        },
        {
          "forename": "Jimmy",
          "surname": "Lin",
          "name": "Jimmy Lin",
          "email": ""
        }
      ],
      "doi": "arXiv:2411.09607",
      "venue": "Initial nugget evaluation results for the trec 2024 rag track with the autonuggetizer framework",
      "date": "2024"
    },
    {
      "index": "b26",
      "title": "Revisiting open domain query facet extraction and generation",
      "author": [
        {
          "forename": "Chris",
          "surname": "Samarinas",
          "name": "Chris Samarinas",
          "email": ""
        },
        {
          "forename": "Arkin",
          "surname": "Dharawat",
          "name": "Arkin Dharawat",
          "email": ""
        },
        {
          "forename": "Hamed",
          "surname": "Zamani",
          "name": "Hamed Zamani",
          "email": ""
        }
      ],
      "doi": "10.1145/3539813.3545138",
      "venue": "Proceedings of the 2022 ACM SIGIR International Conference on Theory of Information Retrieval, ICTIR '22",
      "date": "2022"
    },
    {
      "index": "b27",
      "title": "Simulating task-oriented dialogues with state transition graphs and large language models",
      "author": [
        {
          "forename": "Chris",
          "surname": "Samarinas",
          "name": "Chris Samarinas",
          "email": ""
        },
        {
          "forename": "Pracha",
          "surname": "Promthaw",
          "name": "Pracha Promthaw",
          "email": ""
        },
        {
          "forename": "Atharva",
          "surname": "Nijasure",
          "name": "Atharva Nijasure",
          "email": ""
        },
        {
          "forename": "Hansi",
          "surname": "Zeng",
          "name": "Hansi Zeng",
          "email": ""
        },
        {
          "forename": "Julian",
          "surname": "Killingback",
          "name": "Julian Killingback",
          "email": ""
        },
        {
          "forename": "Hamed",
          "surname": "Zamani",
          "name": "Hamed Zamani",
          "email": ""
        }
      ],
      "doi": "arXiv:2404.14772",
      "venue": "Simulating task-oriented dialogues with state transition graphs and large language models",
      "date": "2024"
    },
    {
      "index": "b28",
      "title": "Bleurt: Learning robust metrics for text generation",
      "author": [
        {
          "forename": "Thibault",
          "surname": "Sellam",
          "name": "Thibault Sellam",
          "email": ""
        },
        {
          "forename": "Dipanjan",
          "surname": "Das",
          "name": "Dipanjan Das",
          "email": ""
        },
        {
          "forename": "Ankur",
          "surname": "Parikh",
          "name": "Ankur Parikh",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "date": "2020"
    },
    {
      "index": "b29",
      "title": "VeriScore: Evaluating the factuality of verifiable claims in long-form text generation",
      "author": [
        {
          "forename": "Yixiao",
          "surname": "Song",
          "name": "Yixiao Song",
          "email": ""
        },
        {
          "forename": "Yekyung",
          "surname": "Kim",
          "name": "Yekyung Kim",
          "email": ""
        },
        {
          "forename": "Mohit",
          "surname": "Iyyer",
          "name": "Mohit Iyyer",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2024",
      "date": "2024"
    },
    {
      "index": "b30",
      "title": "The ClueWeb09 dataset. Accessed",
      "author": [],
      "doi": "",
      "venue": "The ClueWeb09 dataset. Accessed",
      "date": "2009"
    },
    {
      "index": "b31",
      "title": "Automatic machine translation evaluation in many languages via zero-shot paraphrasing",
      "author": [
        {
          "forename": "Brian",
          "surname": "Thompson",
          "name": "Brian Thompson",
          "email": ""
        },
        {
          "forename": "Matt",
          "surname": "Post",
          "name": "Matt Post",
          "email": ""
        }
      ],
      "doi": "10.18653/v1/2020.emnlp-main.8",
      "venue": "Proceedings of the 2020",
      "date": "2020"
    },
    {
      "index": "b33",
      "title": "FEVER: a large-scale dataset for fact extraction and VERification",
      "author": [
        {
          "forename": "James",
          "surname": "Thorne",
          "name": "James Thorne",
          "email": ""
        },
        {
          "forename": "Andreas",
          "surname": "Vlachos",
          "name": "Andreas Vlachos",
          "email": ""
        },
        {
          "forename": "Christos",
          "surname": "Christodoulopoulos",
          "name": "Christos Christodoulopoulos",
          "email": ""
        },
        {
          "forename": "Arpit",
          "surname": "Mittal",
          "name": "Arpit Mittal",
          "email": ""
        }
      ],
      "doi": "10.18653/v1/N18-1074",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "date": "2018"
    },
    {
      "index": "b34",
      "title": "Information retrieval. 2nd. newton, ma",
      "author": [
        {
          "forename": "Cornelius Joost",
          "surname": "Van Rijsbergen",
          "name": "Cornelius Joost Van Rijsbergen",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Information retrieval. 2nd. newton, ma",
      "date": "1979"
    },
    {
      "index": "b35",
      "title": "Openchat: Advancing open-source language models with mixed-quality data",
      "author": [
        {
          "forename": "Guan",
          "surname": "Wang",
          "name": "Guan Wang",
          "email": ""
        },
        {
          "forename": "Sijie",
          "surname": "Cheng",
          "name": "Sijie Cheng",
          "email": ""
        },
        {
          "forename": "Xianyuan",
          "surname": "Zhan",
          "name": "Xianyuan Zhan",
          "email": ""
        },
        {
          "forename": "Xiangang",
          "surname": "Li",
          "name": "Xiangang Li",
          "email": ""
        },
        {
          "forename": "Sen",
          "surname": "Song",
          "name": "Sen Song",
          "email": ""
        },
        {
          "forename": "Yang",
          "surname": "Liu",
          "name": "Yang Liu",
          "email": ""
        }
      ],
      "doi": "arXiv:2309.11235",
      "venue": "Openchat: Advancing open-source language models with mixed-quality data",
      "date": "2023"
    },
    {
      "index": "b36",
      "title": "A broad-coverage challenge corpus for sentence understanding through inference",
      "author": [
        {
          "forename": "Adina",
          "surname": "Williams",
          "name": "Adina Williams",
          "email": ""
        },
        {
          "forename": "Nikita",
          "surname": "Nangia",
          "name": "Nikita Nangia",
          "email": ""
        },
        {
          "forename": "Samuel",
          "surname": "Bowman",
          "name": "Samuel Bowman",
          "email": ""
        }
      ],
      "doi": "10.18653/v1/N18-1101",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "date": "2018"
    },
    {
      "index": "b37",
      "title": "Bertscore: Evaluating text generation with bert",
      "author": [
        {
          "forename": "Tianyi",
          "surname": "Zhang",
          "name": "Tianyi Zhang",
          "email": ""
        },
        {
          "forename": "Varsha",
          "surname": "Kishore",
          "name": "Varsha Kishore",
          "email": ""
        },
        {
          "forename": "Felix",
          "surname": "Wu",
          "name": "Felix Wu",
          "email": ""
        },
        {
          "forename": "Q.",
          "surname": "Kilian",
          "name": "Q. Kilian",
          "email": ""
        },
        {
          "forename": "Yoav",
          "surname": "Weinberger",
          "name": "Yoav Weinberger",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "International Conference on Learning Representations",
      "date": "2019"
    },
    {
      "index": "b38",
      "title": "Towards a unified multidimensional evaluator for text generation",
      "author": [
        {
          "forename": "Ming",
          "surname": "Zhong",
          "name": "Ming Zhong",
          "email": ""
        },
        {
          "forename": "Yang",
          "surname": "Liu",
          "name": "Yang Liu",
          "email": ""
        },
        {
          "forename": "Da",
          "surname": "Yin",
          "name": "Da Yin",
          "email": ""
        },
        {
          "forename": "Yuning",
          "surname": "Mao",
          "name": "Yuning Mao",
          "email": ""
        },
        {
          "forename": "Yizhu",
          "surname": "Jiao",
          "name": "Yizhu Jiao",
          "email": ""
        },
        {
          "forename": "Pengfei",
          "surname": "Liu",
          "name": "Pengfei Liu",
          "email": ""
        },
        {
          "forename": "Chenguang",
          "surname": "Zhu",
          "name": "Chenguang Zhu",
          "email": ""
        },
        {
          "forename": "Ji",
          "surname": "Heng",
          "name": "Ji Heng",
          "email": ""
        },
        {
          "forename": "Jiawei",
          "surname": "Han",
          "name": "Jiawei Han",
          "email": ""
        }
      ],
      "doi": "10.18653/v1/2022.emnlp-main.131",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
      "date": "2022"
    },
    {
      "index": "b39",
      "title": "Texygen: A benchmarking platform for text generation models",
      "author": [
        {
          "forename": "Yaoming",
          "surname": "Zhu",
          "name": "Yaoming Zhu",
          "email": ""
        },
        {
          "forename": "Sidi",
          "surname": "Lu",
          "name": "Sidi Lu",
          "email": ""
        },
        {
          "forename": "Lei",
          "surname": "Zheng",
          "name": "Lei Zheng",
          "email": ""
        },
        {
          "forename": "Jiaxian",
          "surname": "Guo",
          "name": "Jiaxian Guo",
          "email": ""
        },
        {
          "forename": "Weinan",
          "surname": "Zhang",
          "name": "Weinan Zhang",
          "email": ""
        },
        {
          "forename": "Jun",
          "surname": "Wang",
          "name": "Jun Wang",
          "email": ""
        },
        {
          "forename": "Yong",
          "surname": "Yu",
          "name": "Yong Yu",
          "email": ""
        }
      ],
      "doi": "10.1145/3209978.3210080",
      "venue": "The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR '18",
      "date": "2018"
    }
  ]
}