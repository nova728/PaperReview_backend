{
  "title": "Potential and Perils of Large Language Models as Judges of Unstructured Textual Data",
  "publication": {
    "publisher": {},
    "date": "2025-01-20"
  },
  "author": [
    {
      "forename": "Rewina",
      "surname": "Bedemariam",
      "name": "Rewina Bedemariam",
      "email": ""
    },
    {
      "forename": "Natalie",
      "surname": "Perez",
      "name": "Natalie Perez",
      "email": ""
    },
    {
      "forename": "Sreyoshi",
      "surname": "Bhaduri",
      "name": "Sreyoshi Bhaduri",
      "email": ""
    },
    {
      "forename": "Satya",
      "surname": "Kapoor",
      "name": "Satya Kapoor",
      "email": ""
    },
    {
      "forename": "Alex",
      "surname": "Gil",
      "name": "Alex Gil",
      "email": ""
    },
    {
      "forename": "Elizabeth",
      "surname": "Conjar",
      "name": "Elizabeth Conjar",
      "email": ""
    },
    {
      "forename": "David",
      "surname": "Theil",
      "name": "David Theil",
      "email": ""
    },
    {
      "forename": "Aman",
      "surname": "Chadha",
      "name": "Aman Chadha",
      "email": ""
    }
  ],
  "abstract": [
    [
      "Rapid advancements in large language models have unlocked remarkable capabilities when it comes to processing and summarizing unstructured text data. This has implications for the analysis of rich, open-ended datasets, such as survey responses, where LLMs hold the promise of efficiently distilling key themes and sentiments. However, as organizations increasingly turn to these powerful AI systems to make sense of textual feedback, a critical question arises, can we trust LLMs to accurately represent the perspectives contained within these text based datasets? While LLMs excel at generating human-like summaries, there is a risk that their outputs may inadvertently diverge from the true substance of the original responses. Discrepancies between the LLM-generated outputs and the actual themes present in the data could lead to flawed decision-making, with far-reaching consequences for organizations. This research investigates the effectiveness of LLM-as-judge models to evaluate the thematic alignment of summaries generated by other LLMs. We utilized an Anthropic Claude model to generate thematic summaries from open-ended survey responses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as judges. This LLM-as-judge approach was compared to human evaluations using Cohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable alternative to traditional human centric evaluation methods. Our findings reveal that while LLM-asjudge offer a scalable solution comparable to human raters, humans may still excel at detecting subtle, context-specific nuances. Our research contributes to the growing body of knowledge on AI assisted text analysis. Further, we provide recommendations for future research, emphasizing the need for careful consideration when generalizing LLM-as-judge models across various contexts and use cases."
    ]
  ],
  "body": [
    {
      "section": {
        "index": "1",
        "name": "Introduction"
      },
      "p": [
        {
          "text": "The rapid evolution of Large Language Models (LLMs) has expanded their potential uses, from generating content to assessing it. As organizations increasingly adopt these models, there is a growing need to evaluate the accuracy and alignment of LLM-generated outputs with human perspectives [13]. The concept of using LLMs as evaluative judges' dates back to efforts in natural language processing to improve evaluation metrics [24]. However, these traditional evaluation metrics often fall short when it comes to accurately assessing the nuances of natural language generation tasks [12].",
          "quote": [
            {
              "text": "[13]",
              "target": "#b12",
              "type": "bibr",
              "context": "spectives ",
              "index": 290
            },
            {
              "text": "[24]",
              "target": "#b23",
              "type": "bibr",
              "context": "n metrics ",
              "index": 427
            },
            {
              "text": "[12]",
              "target": "#b11",
              "type": "bibr",
              "context": "ion tasks ",
              "index": 583
            }
          ]
        },
        {
          "text": "As LLMs increasingly power the analysis of open-ended textual data in organizational settings, ensuring the fairness and accuracy of their outputs becomes crucial. In artificial intelligence, model alignment refers to techniques designed to align LLM behaviors with human values and expectations [22]. This involves methods like fine-tuning, human feedback, and reinforcement learning to ensure outputs reflect human-like reasoning and decision-making processes [3, 12] Model alignment is critical, especially when models are used as judge models in behavioral research [22].",
          "quote": [
            {
              "text": "[22]",
              "target": "#b21",
              "type": "bibr",
              "context": "ectations ",
              "index": 296
            },
            {
              "text": "[3,",
              "target": "#b2",
              "type": "bibr",
              "context": "processes ",
              "index": 462
            },
            {
              "text": "12]",
              "target": "#b11",
              "type": "bibr",
              "context": "esses [3, ",
              "index": 466
            },
            {
              "text": "[22]",
              "target": "#b21",
              "type": "bibr",
              "context": " research ",
              "index": 570
            }
          ]
        },
        {
          "text": "Our research makes a significant contribution by investigating the effectiveness of using LLMs as judges to evaluate the thematic alignment of summaries generated by other LLMs, specifically in the context of organizations using open-text survey responses. Our study is important because it addresses a critical gap in the responsible deployment of AI systems for decision-making processes that directly impact organizational decision-making.",
          "quote": []
        },
        {
          "text": "We employed an Anthropic Claude model to generate thematic summaries from open-ended survey responses and utilized Amazon's Titan Express and Nova Pro LLMs and Llama as judges to evaluate these summaries. By comparing the LLM-as-judge approach with human evaluations using Cohen's kappa, Spearman's rho, and Krippendorff's alpha, we present a scalable alternative to traditional human-centric evaluation methods. This research is particularly relevant to the AI in Talent Management research community, as it explores the potential for AI systems to serve as impartial arbiters of content accuracy and representation, while also highlighting the ethical considerations and potential biases inherent in such approaches.",
          "quote": []
        },
        {
          "text": "Our findings demonstrate that LLM-as-judges can produce results comparable to human raters, offering organizations a more efficient means of validating AI-generated insights. However, we also critically examine the limitations of this approach and provide recommendations for future research to ensure fairness, accountability, and transparency in the use of LLMs for organizational decision-making. This work contributes to the ongoing dialogue on responsible AI deployment and the development of trustworthy evaluation mechanisms for AI-generated content in high-stakes environments.",
          "quote": []
        },
        {
          "text": "The following research questions are explored:",
          "quote": []
        },
        {
          "text": "( ",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "2",
        "name": "Background"
      },
      "p": [
        {
          "text": "LLM-as-a-judge has emerged as an innovative solution, wherein an LLM evaluates another model's output to approximate human labeling [28]. This approach offers the promise of automating human judgment at scale, while maintaining high levels of reliability and consistency. For example, [26] demonstrated that, when properly calibrated, LLMs can reach agreement rates close to those of human annotators, which can be costly and time-intensive. From this lens, the utilization of LLMs can significantly reduce analysis time and turn around rapid results. By comparing an LLM's evaluation of generated content to human evaluations, researchers aim to refine LLM capabilities and ensure alignment with human interpretations [17, 24].This approach could simplify the evaluation process by reducing reliance on human resources, enabling faster iterations in development cycles. Therefore, understanding how well LLMs can replicate and adhere to human judgment is crucial for their effective deployment in organizational settings [24].",
          "quote": [
            {
              "text": "[28]",
              "target": "#b27",
              "type": "bibr",
              "context": " labeling ",
              "index": 132
            },
            {
              "text": "[26]",
              "target": "#b25",
              "type": "bibr",
              "context": " example, ",
              "index": 285
            },
            {
              "text": "[17,",
              "target": "#b16",
              "type": "bibr",
              "context": "retations ",
              "index": 719
            },
            {
              "text": "24]",
              "target": "#b23",
              "type": "bibr",
              "context": "ions [17, ",
              "index": 724
            },
            {
              "text": "[24]",
              "target": "#b23",
              "type": "bibr",
              "context": " settings ",
              "index": 1022
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "2.1",
        "name": "Open-Text Survey Data"
      },
      "p": [
        {
          "text": "Open-ended survey data is a type of textual data where respondents provide unstructured text-based responses. This type of data is useful for obtaining authentic and sometimes unexpected information that illuminates \"why\" or \"how\" respondents think, feel, or behave [18]. Unlike scaled or categorical survey items, openended questions encourage respondents to provide information about their experiences, perceptions, thoughts, values, and/or feelings that researchers might not anticipate or share information about topics that might be sensitive or personal in nature [1]. Opentext data can be particularly valuable in telling nuanced stories and highlighting diverse responses [18].",
          "quote": [
            {
              "text": "[18]",
              "target": "#b17",
              "type": "bibr",
              "context": "or behave ",
              "index": 266
            },
            {
              "text": "[1]",
              "target": "#b0",
              "type": "bibr",
              "context": "in nature ",
              "index": 570
            },
            {
              "text": "[18]",
              "target": "#b17",
              "type": "bibr",
              "context": "responses ",
              "index": 680
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "2.2",
        "name": "Thematic Summaries"
      },
      "p": [
        {
          "text": "LLMs can be used for a variety of textual analysis, including topic modeling [4, 9] and summarizing large amounts of unstructured text-based data [23]. The objective of textual summarization is to communicate the primary meaning of an original dataset into a simplified and straightforward form without sacrificing the integrity of the information being conveyed [7]. Scholars in the LLM field have explored textual summarization as a valuable use-case for LLMs [23]. Yet research has revealed factual inconsistencies in using LLMs to generate textual summaries, with a range of errors or biases [2, 23].",
          "quote": [
            {
              "text": "[4,",
              "target": "#b3",
              "type": "bibr",
              "context": " modeling ",
              "index": 77
            },
            {
              "text": "9]",
              "target": "#b8",
              "type": "bibr",
              "context": "eling [4, ",
              "index": 81
            },
            {
              "text": "[23]",
              "target": "#b22",
              "type": "bibr",
              "context": "ased data ",
              "index": 146
            },
            {
              "text": "[7]",
              "target": "#b6",
              "type": "bibr",
              "context": " conveyed ",
              "index": 363
            },
            {
              "text": "[23]",
              "target": "#b22",
              "type": "bibr",
              "context": " for LLMs ",
              "index": 462
            },
            {
              "text": "[2,",
              "target": "#b1",
              "type": "bibr",
              "context": "or biases ",
              "index": 596
            },
            {
              "text": "23]",
              "target": "#b22",
              "type": "bibr",
              "context": "iases [2, ",
              "index": 600
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "2.3",
        "name": "LLMs and Classification Evaluation Strategy"
      },
      "p": [
        {
          "text": "There are a number of LLM evaluation strategies available, and classification prompts are one way to classify text into predefined categories [17]. In particular, classification prompts allow researchers to classify content into categories, inductively or deductively [17].",
          "quote": [
            {
              "text": "[17]",
              "target": "#b16",
              "type": "bibr",
              "context": "ategories ",
              "index": 142
            },
            {
              "text": "[17]",
              "target": "#b16",
              "type": "bibr",
              "context": "ductively ",
              "index": 268
            }
          ]
        },
        {
          "text": "Historically, text classification has been conducted using traditional ML approaches that are often complex and resource-intensive; tasks include extraction, dimensionality reduction, classifier selection, and model evaluation, not including pre-processing steps [24]. However, LLM advancements have made text classification easier with only three required elements: 1) data collection, 2) feeding LLM data, and 3) obtaining classification results from an LLM [24]. Since LLMs have been pre-trained on diverse datasets, researchers have found that they require little, if any, additional training specific to a task or domain area [24]. Given recent LLM advancements and prompt engineering, LLMs offer a cheaper alternative to classify thematic summaries based on unstructured open-text survey data. This ability has the potential to help researchers not only analyze textual data, but also classify textual generation into distinct categories [27]. In particular, researchers can use classification prompts and evaluation to assess the accuracy, also known as correctness, of LLM classifications, by measuring the proportion of accurately classified thematic summaries [17]. That said, the absence of robust methodological strategies to evaluate open-text thematic summaries reveals a gap in the research field addressing this novel use-case. This study aims to evaluate how well classification prompts can accurately rate thematic summary content alignment.",
          "quote": [
            {
              "text": "[24]",
              "target": "#b23",
              "type": "bibr",
              "context": "ing steps ",
              "index": 263
            },
            {
              "text": "[24]",
              "target": "#b23",
              "type": "bibr",
              "context": "om an LLM ",
              "index": 460
            },
            {
              "text": "[24]",
              "target": "#b23",
              "type": "bibr",
              "context": "main area ",
              "index": 631
            },
            {
              "text": "[27]",
              "target": "#b26",
              "type": "bibr",
              "context": "ategories ",
              "index": 944
            },
            {
              "text": "[17]",
              "target": "#b16",
              "type": "bibr",
              "context": "summaries ",
              "index": 1170
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "3",
        "name": "Methods"
      },
      "p": [
        {
          "text": "The process of implementing the LLM-as-judge methodology involves taking the text output from one AI model and feeding it back into another LLM. The second model, now serving as the LLM-asjudge, evaluates the text based on an evaluation prompt provided by the user. The second LLM then returns a score, label, or descriptive judgment depending on the specific evaluation criteria set by the user. This allows for a high degree of customization, as users can instruct the LLM to assess specific properties, which makes the approach adaptable to various applications. This framework also means that LLM-as-judge is not an evaluation metric in the traditional sense, like accuracy or precision [26]. Rather, it serves as a general technique for approximating human judgment, where the LLM relies on its training to assess qualities like \"faithfulness to source\" or \"correctness. \" This way, LLMs act as proxy evaluators, following detailed prompts much like a human evaluator would. While this technique does not produce a fixed measure, it offers a flexible proxy metric that can align with specific use cases.",
          "quote": [
            {
              "text": "[26]",
              "target": "#b25",
              "type": "bibr",
              "context": "precision ",
              "index": 691
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "3.1",
        "name": "Dataset"
      },
      "p": [
        {
          "text": "We obtained access to a novel, non-open source dataset for this analysis. The benefit of testing across a new dataset, not previously used for model pre-training, is that the researchers can determine how well LLMs analyze and generate predictions (outputs) accurately. The data used in this study was collected using a census survey that addressed work-related topics and was launched across a multi-global population who reported working full-time. The survey contained one free-text question that asked the respondents to share one thing about work they want senior leaders to be aware of. Over 13,000 comments were collected. The researchers pre-processed the responses by removing short samples, noise (e.g., text with only symbols such as periods or dashes and no other content), and personally identifiable information (PII). The 13K dataset was further grouped into 70 smaller datasets by segmenting the data into groups based on business lines, so insights could be shared with business lines and the datasets would not be corrupted with comments from individuals that did not report to certain business lines.",
          "quote": []
        },
        {
          "text": "Next, a previously engineered thematic summary prompt was used to analyze the dataset and create LLM-generated thematic summaries. The thematic summaries were produced based on a validated prompt that leverages thematic quality principles, as outlined by [16], including JSON formatting instructions focused on identifying a theme name (must contain at least one topic and sentiment details), thematic summary (must contain words that reflect genuine experiences or perceptions of respondents, must contain words that are rich in meaning and provide detailed understanding of the topic or focus area, must describe the relationship of theme to related topics that aid to contribute to new insight), and raw verbatim comments (must include verbatim words expressed by respondents). The data was run through different LLM API calls. A total of 70 thematic summaries containing three themes per summary were generated (i.e., each theme contained a theme name, theme description with 3-4 sentences, and one representative verbatim comment), based on the 70 input datasets. The thematic summaries provide aggregated findings across the topthree most salient and prevalent themes within each of the respective input datasets. This study only used the 70 LLM-generated thematic summaries to evaluate how well the thematic summary content was aligned across theme name, theme description, and representative comments to reflect the theme description and theme name.",
          "quote": [
            {
              "text": "[16]",
              "target": "#b15",
              "type": "bibr",
              "context": "tlined by ",
              "index": 255
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "3.2",
        "name": "Overview of Evaluation"
      },
      "p": [
        {
          "text": "Our general methodology used a Claude model to generate thematic summary outputs from survey comments. For evaluations, this study utilized a three-stage methodology to evaluate the thematic alignment of LLM-generated summaries, which involved human evaluators, and several LLM models including Anthropic Claude (v2.1), Claude Sonnet (v3.5) Amazon Titan Express and Nova Pro, and Llama evaluations. Each model was tasked with assessing alignment across three dimensions: theme name, description, and representative quote.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "3.2.1",
        "name": "Step 1. Human Evaluation as Baseline."
      },
      "p": [
        {
          "text": "Human evaluators were first tasked with reviewing the thematic summaries based on alignment among the theme name, description, and representative verbatim comment/quote generated by a Claude model. The human ratings served as the baseline for comparison against LLM assessments. To evaluate the prompt's ability to accurately evaluate content alignment, human evaluators used the same content alignment criteria as the LLM-as-judge model (see scale below).",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "3.2.2",
        "name": "Step 2. LLM Evaluation: Claude as the Initial"
      },
      "p": [
        {
          "text": "Evaluator. The Claude model was provided with the same summaries and instructed to assign alignment scores based on a structured evaluation prompt. The prompt specifically asked Claude to rate the thematic coherence across each theme's name, description, and quote. The prompt engineered for this study to classify thematic summaries into predefined categories was based on content alignment using the following rating scale: 1 to 3, with 1 (\"Not Aligned\"), 2 (\"Somewhat Aligned\"), and 3 (\"Completely Aligned\"). Since each thematic summary contained three dimensions, the prompt evaluated each dimension independently (refer to Appendix for prompt and output examples).",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": -1,
        "name": "3.2.3"
      },
      "p": [
        {
          "text": "Step 3. Multiple Models as LLM-as-judge of Claude's Output. Following Claude's evaluation, Titan Express, Claude Sonnet 3.5, Llama 3.3 (70b), and Nova Pro models were tasked with evaluating the scores generated by the human evaluations and across the models to identify a baseline for agreement cross the human and model evaluations. Inferential parameters were configured with the following settings: top-p value of 0.9, top-k value: 0.25, and a Temperature of 0. This multi-model evaluation aimed to mirror a human review process while leveraging different models' unique capabilities. This experiment compared the evaluation results of several distinct LLM ratings of the same outputs. The objective was to determine if LLMs could replicate each other's judgments and thus validate the consistency of the LLM-as-judge approach. Claude, Titan, Nova, and Llama models served as the evaluators, examining thematic alignment, accuracy, and relevance of the outputs.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "4",
        "name": "Evaluation Strategy"
      },
      "p": [
        {
          "text": "The evaluation strategy for this study involved testing and validating a classification prompt focused on content alignment, then using the prompt and running Anthropic's Claude on the 70 thematic summaries generated from over 13,000 open-ended survey responses. A blind review was conducted with human evaluators to rate the same 70 thematic summaries. Both the classification prompt and human raters were assigned the same directions and rating criteria to rate the thematic summaries based on content accuracy. Reliability was assessed using four metrics: Percentage Agreement, Cohen's kappa, Spearman's rho, and Krippendorff's alpha. Percentage Agreement provides an intuitive measure of exact matches but does not account for chance alignment. Cohen's kappa adjusts for chance but assumes nominal categories. Spearman's rho determines if enough data has been rated, while Krippendorff's alpha (Ordinal) accounts for ordinal data's inherent structure, offering a more nuanced perspective on agreement [6, 8, 25].",
          "quote": [
            {
              "text": "13",
              "target": "#b12",
              "type": "bibr",
              "context": "from over ",
              "index": 227
            },
            {
              "text": "[6,",
              "target": "#b5",
              "type": "bibr",
              "context": "agreement ",
              "index": 1005
            },
            {
              "text": "8,",
              "target": "#b7",
              "type": "bibr",
              "context": "ement [6, ",
              "index": 1009
            },
            {
              "text": "25]",
              "target": "#b24",
              "type": "bibr",
              "context": "nt [6, 8, ",
              "index": 1012
            }
          ]
        },
        {
          "text": "Evaluation Processes A structured evaluation process was employed to determine the alignment between the LLM and human ratings. The process involved three key steps:",
          "quote": []
        },
        {
          "text": "(1) Evaluation Classification Prompt Development. A classification prompt was engineered and used by human evaluators through reviewing and rating 70 LLM-generated thematic summary outputs. The aim of the prompt engineering focused on the scope of the evaluation, whether to evaluate the entire summary prompt, or evaluate each theme independently within one summary prompt. The researchers decided to focus on engineering a prompt to focus on each theme independently across each thematic summary, which would result in three ratings per thematic summary, since each thematic summary contained three themes.",
          "quote": []
        },
        {
          "text": "(2) Prompt Testing and Validation. The classification prompt underwent testing to ascertain its effectiveness in identifying misalignment. This phase was conducted in partnership between different research teams, ensuring the prompt was accurate and reliable for broader application in model alignment tasks. The aim of the prompt was to accurately classify content alignment within thematic summaries. Multiple iterations were conducted until the research teams determined the details, instructions, and results aligned with the anticipated outputs, including ratings for each independent theme and details that describe the LLM's reason(s) for classifying each theme into a particular category (refer to Appendix A for prompts used for the evaluations). (3) Implementation. After finalizing the classification prompt, all 70 thematic summaries were run through 70 different LLM API calls. The researchers securely stored the ratings for further comparative analysis with the human evaluations, LLM evaluations and the LLM-as-judge model.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "5",
        "name": "Results"
      },
      "p": [
        {
          "text": "Our study sought to answer two research questions: 1) To what extent can LLMs replicate human judgment in evaluating thematic alignment, and what factors contribute to discrepancies between LLM and human ratings? and 2) What are the implications of higher inter-model agreement compared to human-model agreement for the development and application of LLMs in content analysis and theme evaluation tasks? We used the following metrics to compare alignment and consequently, performance, across and among models and humans:",
          "quote": []
        },
        {
          "text": "(1) Percentage Agreement definition: Percentage agreement tells us how many times two raters provide the same rating (e.g., 1 -5) of the same thing, such as two people providing the same 5-star rating of a movie. The more times they agree, the better. This is expressed as a percentage of the total number of cases rated and calculated by dividing the total agreements by the total number of ratings and multiplying by 100 [15]. (2) Cohen's kappa definition: Cohen's kappa is essentially a smarter-version of percentage agreement. It is like when two people guess how many of their 5 co-workers will wear the color blue in the office each day; sometimes both people guess the same number (e.g., 1-5) by chance. Cohen's kappa takes into account how well the two people agree, beyond any lucky guesses. The coefficients range from -1 to +1, where 1 represents perfect agreement, 0 represents agreement equivalent to chance, and negative values indicate agreement less than chance [6]. (3) Spearman's rho definition: Spearman's rho is like a friendship meter for numbers. It shows how well two sets of numbers \"get along\" or move together. If one set of numbers goes up and the other set also goes up, they have a positive relationship. If one goes up while the other goes down, they have a negative relationship. Coefficients range from 1 to +1, with values closer to ±1 indicate stronger correlations [20]. In this study, rho was used as a statistical test on Cohen's kappa as a parameter. Rho allowed researchers to determine if enough data was used to ensure the rating agreement was sound [21]. (4) Krippendorff's alpha definition: Krippendorff's alpha is a test used to determine how much all raters agree on something. Imagine two people taste-testing different foods at a restaurant and rating the foods on a scale of 1-5. Krippendorff's alpha provides a score to show how much the two people agree on their food ratings, even if they did not taste every dish in the restaurant. The alpha coefficient ranges from 0 to 1, where values closer to 1 indicate higher agreement among raters. Generally, an alpha above 0.80 signifies strong agreement, between 0.67 and 0.80 indicates acceptable agreement, and below 0.67 suggests low agreement [10]. If calculated with the rationale that the levels (1, 2 and 3) are ordinal, Krippendorff's Alpha considers not just agreement but also the magnitude of disagreement. It is less affected by marginal distributions compared to kappa and provides a more nuanced assessment when ratings are ranked (ordinal). That is, while percentage agreement and kappa treat all disagreements equally, Alpha recognizes the difference between minor (e.g., \"1\" vs. \"2\") and major disagreements (e.g., \"1\" vs. \"3\"; [11].",
          "quote": [
            {
              "text": "[15]",
              "target": "#b14",
              "type": "bibr",
              "context": "ng by 100 ",
              "index": 423
            },
            {
              "text": "[6]",
              "target": "#b5",
              "type": "bibr",
              "context": "an chance ",
              "index": 978
            },
            {
              "text": "[20]",
              "target": "#b19",
              "type": "bibr",
              "context": "relations ",
              "index": 1400
            },
            {
              "text": "[21]",
              "target": "#b20",
              "type": "bibr",
              "context": "was sound ",
              "index": 1591
            },
            {
              "text": "[10]",
              "target": "#b9",
              "type": "bibr",
              "context": "agreement ",
              "index": 2242
            },
            {
              "text": "[11]",
              "target": "#b10",
              "type": "bibr",
              "context": " vs. \"3\"; ",
              "index": 2740
            }
          ]
        },
        {
          "text": "We examined the alignment between ratings provided by a LLM and human evaluators for a set of 70 thematic summaries. The inter-rater agreement between human evaluators and LLMs demonstrated consistency across various LLM architectures. When comparing the Cohen's kappa results between the human ratings and the models, Sonnet 3.5 had the highest rate of agreement with human raters with a score of 0.44, indicating moderate agreement [6]. However, nuanced differences emerged when examining Cohen's kappa and Krippendorff's alpha coefficients. This variability in agreement metrics can be attributed to several factors: the differential sensitivity of agreement measures, with Cohen's kappa being more sensitive to marginal distributions and Krippendorff's alpha accounting for different types of data and multiple raters; the inherent stochasticity in LLM outputs, which introduces a degree of randomness potentially affecting the stability of agreement metrics; task-specific performance variations, where certain LLMs may exhibit slight advantages in specific evaluation criteria or content domains; and variations in human raters' expertise and their interpretation of annotation guidelines. These findings underscore the importance of employing multiple agreement metrics and considering the underlying factors that influence inter-rater reliability when assessing LLM performance in evaluation tasks.",
          "quote": [
            {
              "text": "[6]",
              "target": "#b5",
              "type": "bibr",
              "context": "agreement ",
              "index": 434
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "5.1",
        "name": "Moderate Agreement Between LLM and Human Ratings"
      },
      "p": [
        {
          "text": "Our results show that despite the high percentage agreement between the human ratings and those of the models (e.g., two highest percentage agreements -Human vs. Claude (v2.1): 79% and Human vs. Llama: 79%), the corresponding Cohen's kappa and Krippendorff's alpha values indicate a range of low to high substantial reliability (Kappa range between human and models: 0.34-0.44; Kappa range between models: 0.32-0.70; Alpha range between human and models (ordinal): 0.49-0.60; Alpha range between models (ordinal): 0.41-0.87; Alpha range between human and models (nominal): 0.35-0.42; Alpha range between models: 0.30-0.70). The Spearman's rho results suggest moderate to strong correlation between the human and model ratings (0.50-0.62). In most cases, Spearman's rho is higher than Cohen's kappa. This suggests that while the humans and models might not always provide the exact same rating, they tend to rank items similarly. In addition, while percentage agreement only considers exact matches, metrics like Cohen's Kappa and Krippendorff's alpha adjust for chance agreement and weighting of ordinal disagreements, respectively (refer to Table ). Krippendorff's alpha further accounts for the ordinal nature of the data, reflecting nuanced differences between raters more accurately than percentage agreement (Artsein and Poesio, 2008; Krippendorff, 2018).",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "5.2",
        "name": "Variability in Performance Across LLM Models"
      },
      "p": [
        {
          "text": "Interestingly, when considering all the metrics, Claude (v.2.1) performed well across all metrics, with one of the highest percentage agreements with the human ratings (79%), second-highest Cohen's Kappa score (0.41), tied for highest Krippendorff's alpha (Ordinal) score (0.60), and highest Krippendorff's alpha (Nominal) score (0.42).",
          "quote": []
        },
        {
          "text": "The model that performed the second best across all metrics was Llama 3.3 (70b). Sonnet 3.5 and Nova Pro did not perform as well across most metrics. These findings suggest that Claude (v2.1) appears to be the most consistently aligned with human ratings across all metrics, followed by Llama 3.3, with Titan Express, Sonnet 3.5, and Nova Pro presenting mixed results. Although the choice of prioritizing metrics could change the ranking, Claude (v2.1) is most consistent across all metrics. As an older model in the Claude suite, the results suggest that \"newer\" models do not always produce better results for all use-cases. Researchers might benefit from examining several models to determine which bests suites their respective use-cases.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "5.3",
        "name": "Higher Inter-Model Agreement Compared to Human-Model Agreement"
      },
      "p": [
        {
          "text": "Overall, the results suggest a moderate to high level of agreement and reliability between the human and LLM model ratings, as well as between the different LLM model ratings. However, when examining the results more closely, the models indicate being generally more consistent with each other compared with the human ratings, as evidenced by the higher percentage agreement, Cohen's Kappa, and Krippendorff's alpha values for the model-to-model comparisons. Overall, our findings suggest that while models such as Claude's Sonnet 2.1 demonstrates a reasonable degree of alignment with human evaluations, there remains room for improvement. The moderate Cohen's Kappa highlights some discrepancies between human and LLM ratings. It is possible that the observed variations could stem from nuanced differences in human interpretation, especially when themes involve complex issues. Examples of discrepancies included instances where the LLM rated themes as completely aligned despite minor misalignments identified by human evaluators.",
          "quote": []
        },
        {
          "text": "Finally, our study results highlighted areas of alignment and discrepancies between LLM and human ratings:",
          "quote": []
        },
        {
          "text": "• High Agreement Cases. In many instances where both the LLM and human rated the themes as \"Completely Aligned, \" the thematic summaries displayed a high degree of coherence across theme name, description, and quotes. Both raters frequently aligned in recognizing specific work environment topics. Topics that were aligned included but were not limited to topics addressing role function, leadership, and policies.",
          "quote": []
        },
        {
          "text": "• Discrepancies and Over-Estimation by LLM. In cases where the human rated alignment as \"somewhat aligned\" or \"not aligned, \" the LLM sometimes rated itself as \"Completely Aligned. \" For example, the LLM might have focused on general content similarities, while the human raters noted discrepancies in specific details, such as incomplete coverage of the theme or misaligned quotes. This finding suggests that humans might perform better with more nuanced content compared with LLMs.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": "6",
        "name": "Recommendation"
      },
      "p": [
        {
          "text": "To address the observed discrepancies, researchers should consider incorporating additional evaluation metrics beyond content alignment. Possible improvements include investigating and mitigating biases in the evaluation of LLMs, which is crucial for refining the assessment process and ensuring reliable outcomes. Position bias, which favors options presented earlier, and verbosity bias, which can lead to an overvaluation of longer responses, are among the key challenges that need to be addressed [19]. Other potential biases include recency bias, confirmation bias, and anchoring bias. Mitigating these biases requires a multifaceted approach, encompassing careful design of evaluation protocols, randomization of response order, utilization of diverse evaluators, and development of objective metrics [19]. Defining comprehensive success metrics for LLMs necessitates interdisciplinary contributions from various fields [5, 14]. Computer science and AI can develop specific capability benchmarks and quantify properties like coherence and factual accuracy. Linguistics can assess grammatical correctness and pragmatic aspects of communication. Psychology can design experiments to measure human preferences and evaluate cognitive load. Philosophy can explore ethical considerations and refine definitions of key concepts. Domain experts can assess task-specific performance, while sociology and anthropology can examine societal implications and cultural sensitivities. Human-computer interaction can focus on user experience, and statistics can develop robust methodologies for data analysis. The integration of these diverse perspectives into cohesive evaluation frameworks represents an ongoing challenge in the rapidly evolving field of LLM development and assessment.",
          "quote": [
            {
              "text": "[19]",
              "target": "#b18",
              "type": "bibr",
              "context": "addressed ",
              "index": 501
            },
            {
              "text": "[19]",
              "target": "#b18",
              "type": "bibr",
              "context": "e metrics ",
              "index": 807
            },
            {
              "text": "[5,",
              "target": "#b4",
              "type": "bibr",
              "context": "us fields ",
              "index": 926
            },
            {
              "text": "14]",
              "target": "#b13",
              "type": "bibr",
              "context": "ields [5, ",
              "index": 930
            }
          ]
        }
      ]
    },
    {
      "section": {
        "index": "7",
        "name": "Conclusion"
      },
      "p": [
        {
          "text": "This study contributes to a deeper understanding of how well LLMs align with human judgments in thematic analysis. While the percentage agreement and Cohen's Kappa results indicate fair agreement, the findings point to the need for ongoing adjustments and improvements to the LLM's evaluation prompt. The analysis reveals areas for improvement, particularly in instances where the LLM overestimated the degree of alignment compared to human raters. This tendency may stem from the LLM's limitations in fully comprehending the nuanced details and contextual factors that contribute to human interpretations of thematic content. The discrepancies highlight the continued need for human oversight and the refinement of evaluation frameworks to better capture the qualitative aspects of thematic understanding. Future research should explore strategies to further enhance the LLM's alignment with human judgments. This may involve developing more sophisticated prompts and evaluation criteria that account for thematic salience, repetition of quotes, and other contextual factors. Additionally, fine-tuning the LLM to better identify and handle personal identifiable information (PII) and theme recurrence could improve the reliability and trustworthiness of the generated insights. As researchers continue to leverage the power of LLMs in analyzing open-ended survey data, ongoing collaboration between human experts and machine learning systems will be crucial. By iteratively improving the evaluation methods and incorporating human feedback, researchers can unlock the full potential of LLMs to generate high-quality thematic summaries that reliably reflect the perceptions and experiences of survey respondents.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": -1,
        "name": "A Appendix"
      },
      "p": [
        {
          "text": "Note: The content in this prompt is mock-content generated to provide an example of the thematic structure only; it does not reflect real data used in this study.",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": -1,
        "name": "Input Pseudo-Prompt"
      },
      "p": [
        {
          "text": "Task Description You are a behavioral research scientist whose job is to review outputs for thematic alignment. You will skillfully review, analyze, and determine whether or not the content in the <theme></theme>, <description></description>, and <quote></quote> are aligned in meaning or not using this scale:1 (not aligned), 2 (somewhat aligned), or 3 (completely aligned). Follow these steps in order and do NOT to miss theme2 and theme3:",
          "quote": []
        },
        {
          "text": "(1) Review the information in each theme: <theme1></theme1>, <theme2></theme2>, <theme3></theme3> (2) Check for alignment between the meaning within each of the themes: <theme1></theme1>, <theme2></theme2>, and <them3></theme3>. (3) Based on each theme reviewed in step 1,2,and 3 assign an alignment rating score for each theme based on the following scale: 1 (not aligned), 2 (somewhat aligned), or 3 (completely aligned). Assistant:",
          "quote": []
        },
        {
          "text": "• Rating score for each theme • Reasoning for each rating",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": -1,
        "name": "Themes"
      },
      "p": [
        {
          "text": "Task Description Theme 1: Learning and Development Desires",
          "quote": []
        },
        {
          "text": "• Description: Many individuals expressed their satisfaction with learning and development opportunities. However, some individuals requested more opportunities to engage in multi-day hands-on learning sessions with expert mentors to aid in learning and skill development. Some individuals suggested more learning and development trainings on a more regular cadence.",
          "quote": []
        },
        {
          "text": "• Quote: \"I really love the opportunities to learn and grow here. There are a lot of options for new and tenured people to learn. That said, I would love more hands-on training options with mentors; it would be great to be able to ask questions with an expert on-hand. \"",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": -1,
        "name": "Task Description"
      },
      "p": [
        {
          "text": "Theme 2: Recognition Perceived as Important",
          "quote": []
        },
        {
          "text": "• Description: Individuals indicated there are many different recognition opportunities for career development, training, and promotion, some feel this is contributing to motivation and engagement. The ability to provide pathways for recognition and advancement was frequently cited as something that is valued. Some individuals connected recognition and advancement, reinforcing perceptions of value associated with recognition.",
          "quote": []
        },
        {
          "text": "• Quote: \"Lot of opportunities to be recognized, which is something I appreciate. Being recognized is important to advancement I think. \"",
          "quote": []
        },
        {
          "text": "Task Description",
          "quote": []
        }
      ]
    },
    {
      "section": {
        "index": -1,
        "name": "Theme 3: Global Connections"
      },
      "p": [
        {
          "text": "• Description: There is a perception that the culture is valued as innovative and there are many opportunities to connect with individuals globally. Having the ability to partner, interact, and work with different individuals from around the globe was valued and meaningful. Some individuals feel opportunities to continue cross-cultural exchange opportunities.",
          "quote": []
        },
        {
          "text": "• Quote: I feel it is amazing to be able to collaborate with peers around the world. Would be great to be able to continue to partner in a variety of ways.",
          "quote": []
        }
      ]
    }
  ],
  "chart": [],
  "reference": [
    {
      "index": "b0",
      "title": "The SAGE encyclopedia of communication research methods",
      "author": [],
      "doi": "",
      "venue": "The SAGE encyclopedia of communication research methods",
      "date": "2017"
    },
    {
      "index": "b1",
      "title": "Using large language models for qualitative analysis can introduce serious bias",
      "author": [
        {
          "forename": "Julian",
          "surname": "Ashwin",
          "name": "Julian Ashwin",
          "email": ""
        },
        {
          "forename": "Aditya",
          "surname": "Chhabra",
          "name": "Aditya Chhabra",
          "email": ""
        },
        {
          "forename": "Vijayendra",
          "surname": "Rao",
          "name": "Vijayendra Rao",
          "email": ""
        }
      ],
      "doi": "arXiv:2309.17147",
      "venue": "Using large language models for qualitative analysis can introduce serious bias",
      "date": "2023"
    },
    {
      "index": "b2",
      "title": "Parameter Efficient Fine Tuning: A Comprehensive Analysis Across Applications",
      "author": [
        {
          "forename": "Charith",
          "surname": "Chandra Sai Balne",
          "name": "Charith Chandra Sai Balne",
          "email": ""
        },
        {
          "forename": "Sreyoshi",
          "surname": "Bhaduri",
          "name": "Sreyoshi Bhaduri",
          "email": ""
        },
        {
          "forename": "Tamoghna",
          "surname": "Roy",
          "name": "Tamoghna Roy",
          "email": ""
        },
        {
          "forename": "Vinija",
          "surname": "Jain",
          "name": "Vinija Jain",
          "email": ""
        },
        {
          "forename": "Aman",
          "surname": "Chadha",
          "name": "Aman Chadha",
          "email": ""
        }
      ],
      "doi": "arXiv:2404.13506",
      "venue": "Parameter Efficient Fine Tuning: A Comprehensive Analysis Across Applications",
      "date": "2024"
    },
    {
      "index": "b3",
      "title": "Reconciling methodological paradigms: Employing large language models as novice qualitative research assistants in talent management research",
      "author": [
        {
          "forename": "Sreyoshi",
          "surname": "Bhaduri",
          "name": "Sreyoshi Bhaduri",
          "email": ""
        },
        {
          "forename": "Satya",
          "surname": "Kapoor",
          "name": "Satya Kapoor",
          "email": ""
        },
        {
          "forename": "Alex",
          "surname": "Gil",
          "name": "Alex Gil",
          "email": ""
        },
        {
          "forename": "Anshul",
          "surname": "Mittal",
          "name": "Anshul Mittal",
          "email": ""
        },
        {
          "forename": "Rutu",
          "surname": "Mulkar",
          "name": "Rutu Mulkar",
          "email": ""
        }
      ],
      "doi": "arXiv:2408.11043",
      "venue": "Reconciling methodological paradigms: Employing large language models as novice qualitative research assistants in talent management research",
      "date": "2024"
    },
    {
      "index": "b4",
      "title": "Multi-disciplinary) Teamwork makes the (real) dream work: Pragmatic recommendations from industry for engineering classrooms",
      "author": [
        {
          "forename": "Sreyoshi",
          "surname": "Bhaduri",
          "name": "Sreyoshi Bhaduri",
          "email": ""
        },
        {
          "forename": "Kenneth",
          "surname": "Ohnemus",
          "name": "Kenneth Ohnemus",
          "email": ""
        },
        {
          "forename": "Jess",
          "surname": "Blackburn",
          "name": "Jess Blackburn",
          "email": ""
        },
        {
          "forename": "Anshul",
          "surname": "Mittal",
          "name": "Anshul Mittal",
          "email": ""
        },
        {
          "forename": "Yan",
          "surname": "Dong",
          "name": "Yan Dong",
          "email": ""
        },
        {
          "forename": "Savannah",
          "surname": "Laferriere",
          "name": "Savannah Laferriere",
          "email": ""
        },
        {
          "forename": "Robert",
          "surname": "Pulvermacher",
          "name": "Robert Pulvermacher",
          "email": ""
        },
        {
          "forename": "Marina",
          "surname": "Dias",
          "name": "Marina Dias",
          "email": ""
        },
        {
          "forename": "Alex",
          "surname": "Gil",
          "name": "Alex Gil",
          "email": ""
        },
        {
          "forename": "Shahriar",
          "surname": "Sadighi",
          "name": "Shahriar Sadighi",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Multi-disciplinary) Teamwork makes the (real) dream work: Pragmatic recommendations from industry for engineering classrooms",
      "date": "2024"
    },
    {
      "index": "b5",
      "title": "A coefficient of agreement for nominal scales",
      "author": [
        {
          "forename": "Jacob",
          "surname": "Cohen",
          "name": "Jacob Cohen",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Educational and psychological measurement",
      "date": "1960"
    },
    {
      "index": "b6",
      "title": "Data Analytics for Social Microblogging Platforms",
      "author": [
        {
          "forename": "Soumi",
          "surname": "Dutta",
          "name": "Soumi Dutta",
          "email": ""
        },
        {
          "forename": "Asit",
          "surname": "Kumar Das",
          "name": "Asit Kumar Das",
          "email": ""
        },
        {
          "forename": "Saptarshi",
          "surname": "Ghosh",
          "name": "Saptarshi Ghosh",
          "email": ""
        },
        {
          "forename": "Debabrata",
          "surname": "Samanta",
          "name": "Debabrata Samanta",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Data Analytics for Social Microblogging Platforms",
      "date": "2022"
    },
    {
      "index": "b7",
      "title": "Answering the call for a standard reliability measure for coding data",
      "author": [
        {
          "forename": "F.",
          "surname": "Andrew",
          "name": "F. Andrew",
          "email": ""
        },
        {
          "forename": "Klaus",
          "surname": "Hayes",
          "name": "Klaus Hayes",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Communication methods and measures",
      "date": "2007"
    },
    {
      "index": "b8",
      "title": "Anshul Mittal, and Rutu Mulkar. 2024. Qualitative Insights Tool (QualIT): LLM Enhanced Topic Modeling",
      "author": [
        {
          "forename": "Alex",
          "surname": "Satya Kapoor",
          "name": "Alex Satya Kapoor",
          "email": ""
        },
        {
          "forename": "Sreyoshi",
          "surname": "Gil",
          "name": "Sreyoshi Gil",
          "email": ""
        }
      ],
      "doi": "arXiv:2409.15626",
      "venue": "Anshul Mittal, and Rutu Mulkar. 2024. Qualitative Insights Tool (QualIT): LLM Enhanced Topic Modeling",
      "date": "2024"
    },
    {
      "index": "b9",
      "title": "Computing Krippendorff's alpha-reliability",
      "author": [
        {
          "forename": "Klaus",
          "surname": "Krippendorff",
          "name": "Klaus Krippendorff",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Computing Krippendorff's alpha-reliability",
      "date": "2011"
    },
    {
      "index": "b10",
      "title": "Content analysis: An introduction to its methodology",
      "author": [
        {
          "forename": "Klaus",
          "surname": "Krippendorff",
          "name": "Klaus Krippendorff",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Content analysis: An introduction to its methodology",
      "date": "2018"
    },
    {
      "index": "b11",
      "title": "Ruochen Xu, and Chenguang Zhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human alignment",
      "author": [
        {
          "forename": "Yang",
          "surname": "Liu",
          "name": "Yang Liu",
          "email": ""
        },
        {
          "forename": "Dan",
          "surname": "Iter",
          "name": "Dan Iter",
          "email": ""
        },
        {
          "forename": "Yichong",
          "surname": "Xu",
          "name": "Yichong Xu",
          "email": ""
        },
        {
          "forename": "Shuohang",
          "surname": "Wang",
          "name": "Shuohang Wang",
          "email": ""
        }
      ],
      "doi": "arXiv:2303.16634",
      "venue": "Ruochen Xu, and Chenguang Zhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human alignment",
      "date": "2023"
    },
    {
      "index": "b12",
      "title": "Aligning Large Language Models with Human Opinions through Persona Selection and Value-Belief-Norm Reasoning",
      "author": [
        {
          "forename": "Kenji",
          "surname": "Do Xuan Long",
          "name": "Kenji Do Xuan Long",
          "email": ""
        },
        {
          "forename": "Min-Yen",
          "surname": "Kawaguchi",
          "name": "Min-Yen Kawaguchi",
          "email": ""
        },
        {
          "forename": "Nancy F.",
          "surname": "Kan",
          "name": "Nancy F. Kan",
          "email": ""
        }
      ],
      "doi": "arXiv-2311 pages",
      "venue": "Aligning Large Language Models with Human Opinions through Persona Selection and Value-Belief-Norm Reasoning",
      "date": "2023"
    },
    {
      "index": "b13",
      "title": "Beyond the Algorithm: Empowering AI Practitioners through Liberal Education",
      "author": [
        {
          "forename": "Tammy",
          "surname": "Mackenzie",
          "name": "Tammy Mackenzie",
          "email": ""
        },
        {
          "forename": "Leslie",
          "surname": "Salgado",
          "name": "Leslie Salgado",
          "email": ""
        },
        {
          "forename": "Sreyoshi",
          "surname": "Bhaduri",
          "name": "Sreyoshi Bhaduri",
          "email": ""
        },
        {
          "forename": "Victoria",
          "surname": "Kuketz",
          "name": "Victoria Kuketz",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "2024 ASEE Annual Conference & Exposition",
      "date": "2024"
    },
    {
      "index": "b14",
      "title": "Interrater reliability: the kappa statistic",
      "author": [
        {
          "forename": "L.",
          "surname": "Mary",
          "name": "L. Mary",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Biochemia medica",
      "date": "2012"
    },
    {
      "index": "b15",
      "title": "A step-by-step process of thematic analysis to develop a conceptual model in qualitative research",
      "author": [
        {
          "forename": "Muhammad",
          "surname": "Naeem",
          "name": "Muhammad Naeem",
          "email": ""
        },
        {
          "forename": "Wilson",
          "surname": "Ozuem",
          "name": "Wilson Ozuem",
          "email": ""
        },
        {
          "forename": "Kerry",
          "surname": "Howell",
          "name": "Kerry Howell",
          "email": ""
        },
        {
          "forename": "Silvia",
          "surname": "Ranfagni",
          "name": "Silvia Ranfagni",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "International Journal of Qualitative Methods",
      "date": "2023"
    },
    {
      "index": "b16",
      "title": "Dan Calacci, and Andrés Monroy-Hernández",
      "author": [
        {
          "forename": "Eesha",
          "surname": "Varun Nagaraj Rao",
          "name": "Eesha Varun Nagaraj Rao",
          "email": ""
        },
        {
          "forename": "Samantha",
          "surname": "Agarwal",
          "name": "Samantha Agarwal",
          "email": ""
        }
      ],
      "doi": "arXiv:2405.05345",
      "venue": "QuaLLM: An LLM-based Framework to Extract Quantitative Insights from Online Forums",
      "date": "2024"
    },
    {
      "index": "b17",
      "title": "What to do with all those open-ended responses? Data visualization techniques for survey researchers",
      "author": [
        {
          "forename": "Jessie",
          "surname": "Rouder",
          "name": "Jessie Rouder",
          "email": ""
        },
        {
          "forename": "Olivia",
          "surname": "Saucier",
          "name": "Olivia Saucier",
          "email": ""
        },
        {
          "forename": "Rachel",
          "surname": "Kinder",
          "name": "Rachel Kinder",
          "email": ""
        },
        {
          "forename": "Matt",
          "surname": "Jans",
          "name": "Matt Jans",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Survey Practice",
      "date": "2021"
    },
    {
      "index": "b18",
      "title": "Verbosity bias in preference labeling by large language models",
      "author": [
        {
          "forename": "Keita",
          "surname": "Saito",
          "name": "Keita Saito",
          "email": ""
        },
        {
          "forename": "Akifumi",
          "surname": "Wachi",
          "name": "Akifumi Wachi",
          "email": ""
        },
        {
          "forename": "Koki",
          "surname": "Wataoka",
          "name": "Koki Wataoka",
          "email": ""
        },
        {
          "forename": "Youhei",
          "surname": "Akimoto",
          "name": "Youhei Akimoto",
          "email": ""
        }
      ],
      "doi": "arXiv:2310.10076",
      "venue": "Verbosity bias in preference labeling by large language models",
      "date": "2023"
    },
    {
      "index": "b19",
      "title": "Correlation coefficients: appropriate use and interpretation",
      "author": [
        {
          "forename": "Patrick",
          "surname": "Schober",
          "name": "Patrick Schober",
          "email": ""
        },
        {
          "forename": "Christa",
          "surname": "Boer",
          "name": "Christa Boer",
          "email": ""
        },
        {
          "forename": "Lothar A.",
          "surname": "Schwarte",
          "name": "Lothar A. Schwarte",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Anesthesia & analgesia",
      "date": "2018"
    },
    {
      "index": "b21",
      "title": "Large language model alignment: A survey",
      "author": [
        {
          "forename": "Tianhao",
          "surname": "Shen",
          "name": "Tianhao Shen",
          "email": ""
        },
        {
          "forename": "Renren",
          "surname": "Jin",
          "name": "Renren Jin",
          "email": ""
        },
        {
          "forename": "Yufei",
          "surname": "Huang",
          "name": "Yufei Huang",
          "email": ""
        },
        {
          "forename": "Chuang",
          "surname": "Liu",
          "name": "Chuang Liu",
          "email": ""
        },
        {
          "forename": "Weilong",
          "surname": "Dong",
          "name": "Weilong Dong",
          "email": ""
        },
        {
          "forename": "Zishan",
          "surname": "Guo",
          "name": "Zishan Guo",
          "email": ""
        },
        {
          "forename": "Xinwei",
          "surname": "Wu",
          "name": "Xinwei Wu",
          "email": ""
        },
        {
          "forename": "Yan",
          "surname": "Liu",
          "name": "Yan Liu",
          "email": ""
        },
        {
          "forename": "Deyi",
          "surname": "Xiong",
          "name": "Deyi Xiong",
          "email": ""
        }
      ],
      "doi": "arXiv:2309.15025",
      "venue": "Large language model alignment: A survey",
      "date": "2023"
    },
    {
      "index": "b22",
      "title": "Evaluating large language models on medical evidence summarization",
      "author": [
        {
          "forename": "Liyan",
          "surname": "Tang",
          "name": "Liyan Tang",
          "email": ""
        },
        {
          "forename": "Zhaoyi",
          "surname": "Sun",
          "name": "Zhaoyi Sun",
          "email": ""
        },
        {
          "forename": "Betina",
          "surname": "Idnay",
          "name": "Betina Idnay",
          "email": ""
        },
        {
          "forename": "G.",
          "surname": "Jordan",
          "name": "G. Jordan",
          "email": ""
        },
        {
          "forename": "Ali",
          "surname": "Nestor",
          "name": "Ali Nestor",
          "email": ""
        },
        {
          "forename": "A.",
          "surname": "Pierre",
          "name": "A. Pierre",
          "email": ""
        },
        {
          "forename": "Ziyang",
          "surname": "Elias",
          "name": "Ziyang Elias",
          "email": ""
        },
        {
          "forename": "Ying",
          "surname": "Xu",
          "name": "Ying Xu",
          "email": ""
        },
        {
          "forename": "Greg",
          "surname": "Ding",
          "name": "Greg Ding",
          "email": ""
        },
        {
          "forename": "Justin F.",
          "surname": "Durrett",
          "name": "Justin F. Durrett",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "NPJ digital medicine",
      "date": "2023"
    },
    {
      "index": "b23",
      "title": "Adaptable and Reliable Text Classification using Large Language Models",
      "author": [
        {
          "forename": "Zhiqiang",
          "surname": "Wang",
          "name": "Zhiqiang Wang",
          "email": ""
        },
        {
          "forename": "Yiran",
          "surname": "Pang",
          "name": "Yiran Pang",
          "email": ""
        },
        {
          "forename": "Yanbin",
          "surname": "Lin",
          "name": "Yanbin Lin",
          "email": ""
        },
        {
          "forename": "Xingquan",
          "surname": "Zhu",
          "name": "Xingquan Zhu",
          "email": ""
        }
      ],
      "doi": "arXiv:2405.10523",
      "venue": "Adaptable and Reliable Text Classification using Large Language Models",
      "date": "2024"
    },
    {
      "index": "b24",
      "title": "Five ways to look at Cohen's kappa",
      "author": [
        {
          "forename": "J.",
          "surname": "Matthijs",
          "name": "J. Matthijs",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Journal of Psychology & Psychotherapy",
      "date": "2015"
    },
    {
      "index": "b25",
      "title": "Evaluating the Effectiveness of LLM-Evaluators (aka LLM-as-Judge)",
      "author": [
        {
          "forename": "Ziyou",
          "surname": "Yan",
          "name": "Ziyou Yan",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Evaluating the Effectiveness of LLM-Evaluators (aka LLM-as-Judge)",
      "date": "2024"
    },
    {
      "index": "b26",
      "title": "Recommender systems in the era of large language models (llms)",
      "author": [
        {
          "forename": "Zihuai",
          "surname": "Zhao",
          "name": "Zihuai Zhao",
          "email": ""
        },
        {
          "forename": "Wenqi",
          "surname": "Fan",
          "name": "Wenqi Fan",
          "email": ""
        },
        {
          "forename": "Jiatong",
          "surname": "Li",
          "name": "Jiatong Li",
          "email": ""
        },
        {
          "forename": "Yunqing",
          "surname": "Liu",
          "name": "Yunqing Liu",
          "email": ""
        },
        {
          "forename": "Xiaowei",
          "surname": "Mei",
          "name": "Xiaowei Mei",
          "email": ""
        },
        {
          "forename": "Yiqi",
          "surname": "Wang",
          "name": "Yiqi Wang",
          "email": ""
        },
        {
          "forename": "Zhen",
          "surname": "Wen",
          "name": "Zhen Wen",
          "email": ""
        },
        {
          "forename": "Fei",
          "surname": "Wang",
          "name": "Fei Wang",
          "email": ""
        },
        {
          "forename": "Xiangyu",
          "surname": "Zhao",
          "name": "Xiangyu Zhao",
          "email": ""
        },
        {
          "forename": "Jiliang",
          "surname": "Tang",
          "name": "Jiliang Tang",
          "email": ""
        }
      ],
      "doi": "arXiv:2307.02046",
      "venue": "Recommender systems in the era of large language models (llms)",
      "date": "2023"
    },
    {
      "index": "b27",
      "title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
      "author": [
        {
          "forename": "Lianmin",
          "surname": "Zheng",
          "name": "Lianmin Zheng",
          "email": ""
        },
        {
          "forename": "Wei-Lin",
          "surname": "Chiang",
          "name": "Wei-Lin Chiang",
          "email": ""
        },
        {
          "forename": "Ying",
          "surname": "Sheng",
          "name": "Ying Sheng",
          "email": ""
        },
        {
          "forename": "Siyuan",
          "surname": "Zhuang",
          "name": "Siyuan Zhuang",
          "email": ""
        },
        {
          "forename": "Zhanghao",
          "surname": "Wu",
          "name": "Zhanghao Wu",
          "email": ""
        },
        {
          "forename": "Yonghao",
          "surname": "Zhuang",
          "name": "Yonghao Zhuang",
          "email": ""
        },
        {
          "forename": "Zi",
          "surname": "Lin",
          "name": "Zi Lin",
          "email": ""
        },
        {
          "forename": "Zhuohan",
          "surname": "Li",
          "name": "Zhuohan Li",
          "email": ""
        },
        {
          "forename": "Dacheng",
          "surname": "Li",
          "name": "Dacheng Li",
          "email": ""
        },
        {
          "forename": "Eric",
          "surname": "Xing",
          "name": "Eric Xing",
          "email": ""
        }
      ],
      "doi": "",
      "venue": "Advances in Neural Information Processing Systems",
      "date": "2023"
    }
  ]
}