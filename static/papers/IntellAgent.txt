{
    "target": "en",
    "model": "intellagent",
    "title": "IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems",
    "publication": {
        "publisher": {},
        "date": "2025"
    },
    "author": [
        {
            "forename": "Elad",
            "surname": "Levi",
            "name": "Elad Levi",
            "email": "eladl@plurai.ai"
        },
        {
            "forename": "Ilan",
            "surname": "Kadar",
            "name": "Ilan Kadar",
            "email": "ilan@plurai.ai"
        }
    ],
    "abstract": [
        [
            "Large Language Models (LLMs) are transforming artificial intelligence, evolving into task-oriented systems capable of autonomous planning, execution, and refinement. One of the primary applications of LLMs is conversational AI systems, which must navigate multi-turn dialogues, integrate domain-specific APIs, and adhere to strict policy constraints. However, evaluating these agents remains a significant challenge, as traditional methods fail to capture the complexity and variability of real-world interactions. We introduce IntellAgent, a scalable, opensource multi-agent framework designed to evaluate conversational AI systems comprehensively. IntellAgent automates the creation of diverse, synthetic benchmarks by combining policy-driven graph modeling, realistic event generation, and interactive user-agent simulations. This innovative approach provides fine-grained diagnostics, addressing the limitations of static and manually curated benchmarks with coarse-grained metrics. IntellAgent represents a paradigm shift in evaluating conversational AI. By simulating realistic, multi-policy scenarios across varying levels of complexity, IntellAgent captures the nuanced interplay of agent capabilities and policy constraints. Unlike traditional methods, it employs a graph-based policy model to represent relationships, likelihoods, and complexities of policy interactions, enabling highly detailed diagnostics. IntellAgent also identifies critical performance gaps, offering actionable insights for targeted optimization. Its modular, open-source design supports seamless integration of new domains, policies, and APIs, fostering reproducibility and community collaboration. Our findings demonstrate that IntellAgent serves as an effective framework for advancing conversational AI by addressing challenges in bridging research and deployment. The framework is available at https://github.com/plurai-ai/intellagent."
        ]
    ],
    "body": [
        {
            "section": {
                "index": "1",
                "name": "Introduction"
            },
            "p": [
                {
                    "text": "Large Language Models (LLMs) are revolutionizing the field of artificial intelligence by transitioning from static language processors to dynamic, task-oriented agents that can autonomously plan, execute, and refine their actions. These agents promise transformative applications across a wide range of domains, including healthcare [20, 1], finance [30, 26, 7], customer support [21, 14] and education [31, 29]. This evolution positions LLM agents as foundational technologies for reshaping human-computer interaction, enabling intelligent systems to tackle complex, real-world challenges with unprecedented efficiency and adaptability.",
                    "quote": [
                        {
                            "text": "[20,",
                            "target": "#b19",
                            "type": "bibr",
                            "context": "healthcare ",
                            "index": 185
                        },
                        {
                            "text": "1]",
                            "target": "#b0",
                            "type": "bibr",
                            "context": "care [20, ",
                            "index": 189
                        },
                        {
                            "text": "[30,",
                            "target": "#b29",
                            "type": "bibr",
                            "context": "finance ",
                            "index": 200
                        },
                        {
                            "text": "26,",
                            "target": "#b25",
                            "type": "bibr",
                            "context": "ce [30, ",
                            "index": 204
                        },
                        {
                            "text": "7]",
                            "target": "#b6",
                            "type": "bibr",
                            "context": "0, 26, ",
                            "index": 208
                        },
                        {
                            "text": "[21,",
                            "target": "#b20",
                            "type": "bibr",
                            "context": "support ",
                            "index": 230
                        },
                        {
                            "text": "14]",
                            "target": "#b13",
                            "type": "bibr",
                            "context": "t [21, ",
                            "index": 234
                        },
                        {
                            "text": "[31,",
                            "target": "#b30",
                            "type": "bibr",
                            "context": "education ",
                            "index": 252
                        },
                        {
                            "text": "29]",
                            "target": "#b28",
                            "type": "bibr",
                            "context": "n [31, ",
                            "index": 256
                        }
                    ]
                },
                {
                    "text": "Among these advancements, conversational AI agents present a particularly demanding frontier. Unlike single-turn systems, these agents must navigate multi-turn dialogues, integrate domainspecific tools and APIs, and adhere to stringent policy constraints. The interplay of these requirements introduces a new level of complexity, where the cost of errors—ranging from inconsistent responses to policy violations—can severely undermine reliability and trust. Reliability is not just desirable but critical, as it directly determines the feasibility of deploying such agents in real-world, high-stakes environments. Despite significant progress, evaluating conversational AI agents at this level of complexity remains a significant challenge [19, 9]. Traditional evaluation methods rely on static manually curated benchmarks [33, 15] that fail to scale or reflect the intricate dynamics of multi-turn interactions, policy adherence, and tool usage. These approaches often prioritize coarse-grained metrics, offering limited insights into agents' specific strengths and vulnerabilities. Consequently, existing methods leave critical gaps in both understanding and optimizing conversational agents for real-world applications.",
                    "quote": [
                        {
                            "text": "[19,",
                            "target": "#b18",
                            "type": "bibr",
                            "context": "challenge ",
                            "index": 681
                        },
                        {
                            "text": "9]",
                            "target": "#b8",
                            "type": "bibr",
                            "context": "e [19, ",
                            "index": 685
                        },
                        {
                            "text": "[33,",
                            "target": "#b32",
                            "type": "bibr",
                            "context": "benchmarks ",
                            "index": 756
                        },
                        {
                            "text": "15]",
                            "target": "#b14",
                            "type": "bibr",
                            "context": "s [33, ",
                            "index": 760
                        }
                    ]
                },
                {
                    "text": "In this work, we present IntellAgent, a scalable open-source multi-agent framework powered by AI agents, specifically designed to simulate and evaluate conversational AI agents comprehensively. IntellAgent represents a paradigm shift in evaluation by automating the generation of diverse, synthetic scenarios that rigorously test agents across multiple dimensions. Unlike existing approaches, IntellAgent leverages a novel pipeline that combines policy-driven graph modeling, realistic event generation, and interactive user-agent simulation to holistically assess agent performance, including a full spectrum of complexity levels, combinations of domain policies modeled through graph-based policy representations, and integration with API tools. The IntellAgent framework employs an automated process that involves constructing a policy graph to represent the relationships, complexity, and likelihood of various policies, generating events by sampling combinations of policies from the graph that align with real-world tasks and database states, and simulating interactions between a user agent and the chatbot. These simulations are then analyzed to provide fine-grained performance insights, identifying failure points, strengths, and opportunities for improvement. An overview of the system is illustrated in Figure 1, which outlines the key components of the IntellAgent pipeline.",
                    "quote": []
                },
                {
                    "text": "Our study demonstrates the effectiveness and versatility of IntellAgent as a benchmarking tool for evaluating conversational AI agents. The results reveal a strong correlation between model performance on the IntellAgent benchmark and the τ-bench [33], despite IntellAgent relying entirely on synthetic data. This validates IntellAgent as a robust alternative for evaluating conversational agents across diverse scenarios and challenges.",
                    "quote": [
                        {
                            "text": "[33]",
                            "target": "#b32",
                            "type": "bibr",
                            "context": "τ-bench ",
                            "index": 233
                        }
                    ]
                },
                {
                    "text": "Key findings from our analysis indicate that model performance decreases with increasing complexity levels, but the rate of decline varies significantly across models. Additionally, our policy-specific evaluation uncovers significant variations in model capabilities across different policy categories. This highlights IntellAgent's ability to provide detailed diagnostic insights, empowering users to identify the most suitable configuration for their specific requirements.",
                    "quote": []
                }
            ]
        },
        {
            "section": {
                "index": "2",
                "name": "Related Work"
            },
            "p": [
                {
                    "text": "Recent advancements in using LLMs for synthetic data generation, automated evaluation have significantly influenced the development of AI systems. This section delves into these key areas, outlining existing methodologies and their limitations while highlighting how our approach advances the state of the art.",
                    "quote": []
                }
            ]
        },
        {
            "section": {
                "index": "2.1",
                "name": "Synthetic Benchmarks"
            },
            "p": [
                {
                    "text": "Data generation. Synthetic data generation using Large Language Models (LLMs) has become a transformative technique for advancing AI across various domains, including code generation [22, 32], mathematical reasoning [34, 18], text embedding [24], and text-to-image synthesis [5]. Synthetic data reduces the costs and time of human-annotated datasets while providing control over sample distribution, crucial for fine-tuning and optimizing performance in downstream tasks [16, 27]. Evaluating synthetic data focuses on two key metrics: faithfulness and diversity.",
                    "quote": [
                        {
                            "text": "[22,",
                            "target": "#b21",
                            "type": "bibr",
                            "context": "generation ",
                            "index": 175
                        },
                        {
                            "text": "32]",
                            "target": "#b31",
                            "type": "bibr",
                            "context": "n [22, ",
                            "index": 179
                        },
                        {
                            "text": "[34,",
                            "target": "#b33",
                            "type": "bibr",
                            "context": "reasoning ",
                            "index": 205
                        },
                        {
                            "text": "18]",
                            "target": "#b17",
                            "type": "bibr",
                            "context": "g [34, ",
                            "index": 209
                        },
                        {
                            "text": "[24]",
                            "target": "#b23",
                            "type": "bibr",
                            "context": "embedding ",
                            "index": 227
                        },
                        {
                            "text": "[5]",
                            "target": "#b4",
                            "type": "bibr",
                            "context": "synthesis ",
                            "index": 265
                        },
                        {
                            "text": "[16,",
                            "target": "#b15",
                            "type": "bibr",
                            "context": "tasks ",
                            "index": 456
                        },
                        {
                            "text": "27]",
                            "target": "#b26",
                            "type": "bibr",
                            "context": "6, ",
                            "index": 460
                        }
                    ]
                },
                {
                    "text": "Faithfulness ensures synthetic data reflects real-world patterns and relationships, while diversity captures a wide range of scenarios to enhance model robustness and mitigate overfitting [17]. Achieving both metrics is challenging; recent research explores conditional prompting and multi-step generation to balance them. Conditional prompting improves diversity by defining attributes through conditionvalue pairs [12, 28]. Multi-step generation enhances coherence and domain coverage by decomposing tasks into smaller subtasks [8, 25, 13, 23], though these methods often require significant manual effort and may not scale well to complex domains.",
                    "quote": [
                        {
                            "text": "[17]",
                            "target": "#b16",
                            "type": "bibr",
                            "context": "overfitting ",
                            "index": 172
                        },
                        {
                            "text": "[12,",
                            "target": "#b11",
                            "type": "bibr",
                            "context": "pairs ",
                            "index": 359
                        },
                        {
                            "text": "28]",
                            "target": "#b27",
                            "type": "bibr",
                            "context": "2, ",
                            "index": 363
                        },
                        {
                            "text": "[8,",
                            "target": "#b7",
                            "type": "bibr",
                            "context": "subtasks ",
                            "index": 483
                        },
                        {
                            "text": "25,",
                            "target": "#b24",
                            "type": "bibr",
                            "context": "s [8, ",
                            "index": 487
                        },
                        {
                            "text": "13,",
                            "target": "#b12",
                            "type": "bibr",
                            "context": ", 25, ",
                            "index": 491
                        },
                        {
                            "text": "23]",
                            "target": "#b22",
                            "type": "bibr",
                            "context": "3, ",
                            "index": 495
                        }
                    ]
                },
                {
                    "text": "Our approach automates synthetic dataset generation to ensure both faithfulness and diversity by using a policies graph inspired by GraphRAG [10], where nodes represent policies and edges capture their complexity relationships. This framework enables fine-grained generation across various combinations of policies, tools, and tasks, producing datasets that reflect application requirements while covering a diverse range of scenarios. It addresses challenges from simple tasks to complex edge cases, ensuring rigorous evaluation of agents under diverse conditions.",
                    "quote": [
                        {
                            "text": "[10]",
                            "target": "#b9",
                            "type": "bibr",
                            "context": "GraphRAG ",
                            "index": 140
                        }
                    ]
                },
                {
                    "text": "Automated evaluation. Synthetic datasets have become invaluable for evaluating retrieval-augmented generation (RAG) systems, offering metrics to measure retrieval quality, generation fidelity, and robustness. Frameworks like RAGAS [11] automate the evaluation of RAG pipelines, focusing on aspects such as retrieval accuracy and generation relevance. Unlike traditional metrics, RAGAS operates without reference answers, enabling broader applicability. However, it primarily targets isolated components and does not fully address the complexities of multi-turn dialogues or real-world conversational AI applications. This underscores the need for expanded evaluation frameworks that encompass diverse, dynamic use cases.",
                    "quote": [
                        {
                            "text": "[11]",
                            "target": "#b10",
                            "type": "bibr",
                            "context": "RAGAS ",
                            "index": 240
                        }
                    ]
                }
            ]
        },
        {
            "section": {
                "index": "2.2",
                "name": "Conversational AI Benchmarks"
            },
            "p": [
                {
                    "text": "Evaluating conversational AI systems in real-world applications has been the focus of various benchmarks, each targeting specific capabilities. For instance, τ-bench [33] assesses agents' ability to interact with users, adhere to domain-specific policies, and utilize API tools effectively, with simulations in domains like retail and airline customer service. However, τ-bench is limited by its reliance on manual curation, with only 50 samples for airlines and 115 for retail, restricting scalability. Additionally, its evaluation focuses solely on coarse-grained end-to-end metrics, overlooking policy violations and dialogue flow errors, which limits comprehensive assessment.",
                    "quote": [
                        {
                            "text": "[33]",
                            "target": "#b32",
                            "type": "bibr",
                            "context": "τ-bench ",
                            "index": 131
                        }
                    ]
                },
                {
                    "text": "The ALMITA benchmark [3] proposes a novel dataset and framework specifically tailored for evaluating tool-augmented conversational AI agents in customer support scenarios. It uses a combination of automated and manual processes to generate diverse and realistic conversations grounded in user-defined procedures. Despite its rigorous evaluation, ALMITA focuses primarily on customer support, and the generalizability to other domains remains an open question. Moreover, the reliance on manually curated samples, while ensuring quality, limits scalability.",
                    "quote": [
                        {
                            "text": "[3]",
                            "target": "#b2",
                            "type": "bibr",
                            "context": "benchmark ",
                            "index": 19
                        }
                    ]
                },
                {
                    "text": "The LTM Benchmark [6] effectively highlights limitations in conversational multitasking, especially with interleaved tasks. However, its reliance on predefined interaction structures limits its ability to capture the unpredictable and non-linear nature of real-world conversational flows, such as spontaneous topic shifts or revisiting earlier contexts. Similarly, E2E Benchmark [4] evaluates chatbot responses based on accuracy and usefulness, emphasizing conversational coherence in non-task-oriented interactions. Nevertheless, its lack of support for complex tool use and multi-turn interactions restricts its applicability to broader real-world contexts. The CURATe framework [2] addresses alignment challenges for personalized conversational agents by focusing on user-specific safety-critical contexts. While it introduces valuable techniques for multi-turn personalization, its emphasis on alignment rather than general performance testing narrows its scope.",
                    "quote": [
                        {
                            "text": "[6]",
                            "target": "#b5",
                            "type": "bibr",
                            "context": "Benchmark ",
                            "index": 16
                        },
                        {
                            "text": "[4]",
                            "target": "#b3",
                            "type": "bibr",
                            "context": "Benchmark ",
                            "index": 357
                        },
                        {
                            "text": "[2]",
                            "target": "#b1",
                            "type": "bibr",
                            "context": "framework ",
                            "index": 615
                        }
                    ]
                },
                {
                    "text": "Although these benchmarks provide valuable tools for assessing conversational AI systems, their reliance on manual curation limits scalability and adaptability to diverse real-world applications, making it challenging to generate the extensive datasets required for comprehensive evaluations. Our approach, in contrast, is fully automated, enabling the generation of diverse scenarios and dialogues at scale, thus supporting evaluations across varied domains. Additionally, our framework addresses a broader range of challenges, from simple tasks to complex edge cases, ensuring rigorous agent evaluation under diverse conditions. Unlike existing benchmarks that use coarse-grained metrics for overall performance, our method offers fine-grained insights by evaluating agents across all policy and tool combinations, identifying specific strengths and weaknesses.",
                    "quote": []
                }
            ]
        },
        {
            "section": {
                "index": "3",
                "name": "Method"
            },
            "p": [
                {
                    "text": "Our multi-agent system is illustrated in Figure 1. The system pipeline consists of the following steps: (1) The IntellAgent system receives a schema of the system database along with either a chatbot system prompt or a document outlining the company policies. Based on this input, the system constructs a policy graph (3.1.1). It then samples a list of policies from the graph at varying levels of complexity and generates an event addressing these policies (3.1.2). The event includes a scenario description with a user request and corresponding samples for the initial database state, ensuring the validity of the user requests. (2) The system simulates a dialog between the chatbot and a user agent using the information provided in the event (3.2). (3) Finally, a critique is provided with the dialog and provides an analysis of the chatbot's performances with respect to the event policies list (3.3).",
                    "quote": []
                }
            ]
        },
        {
            "section": {
                "index": "3.1",
                "name": "Event Generation"
            },
            "p": [
                {
                    "text": "To address the challenge of developing advanced chatbots capable of interacting with a system database, the system must generate complex and natural user requests that cover various policies. Additionally, it should create an initial database state for the chatbot, ensuring that when the chatbot processes the user request and queries the database, it does not encounter failures.",
                    "quote": []
                },
                {
                    "text": "An IntellAgent event is defined by the following components: (1) A list of policies. (2) A description of a user request that aligns with the specified policies. (3) The initial state of the chatbot's system database.",
                    "quote": []
                }
            ]
        },
        {
            "section": {
                "index": "3.1.1",
                "name": "Policies graph"
            },
            "p": [
                {
                    "text": "IntellAgent aims to generate a diverse set of events with varying levels of complexity. To create and complex user-chatbot interaction scenarios, IntellAgent constructs a policy graph. In this graph, nodes represent individual policies, and edge weights indicate the likelihood of two policies appearing together in the same interaction. Each node is also assigned a weight that reflects the complexity of its associated policy.",
                    "quote": []
                },
                {
                    "text": "The graph is built through multiple queries to a large language model (LLM). First, the system extracts a list of policies from the prompt and assigns a difficulty ranking to each. Then, for every pair of policies, the LLM assigns a score (on a scale of 1–10) representing the likelihood of the two policies co-occurring in a conversation.",
                    "quote": []
                }
            ]
        },
        {
            "section": {
                "index": "3.1.2",
                "name": "Event generator"
            },
            "p": [
                {
                    "text": "The complexity of an event is defined as the sum of the complexities of its policies. Given a policy graph with a set of policies G, weighted edges E and nodes complexity weights {ng}g∈G. the event generator aims to produce a valid set of policies that satisfies the following criteria: 1. The event complexity is uniformly distributed within a specified range. 2. The distribution of the first policy in the event is uniformly distributed across all possible policies. 3. For a given complexity level and initial policy, the distribution of the resulting policy list aligns with real-world distributions.",
                    "quote": []
                },
                {
                    "text": "Policies graph sampling. To satisfy the outlined criteria, the IntellAgent sampling algorithm operates in batches. The process for each iteration is as follows: The complexity of events in the current batch is sampled first. The batch distribution is adjusted to ensure that the overall distribution of all generated events (including previous batches) remains uniform. Next, the initial policy for each event is sampled uniformly across all nodes in the policy graph. Then For each event, the system generates a policy path by performing a random walk on the graph. The walk terminates once the cumulative complexity of the visited nodes exceeds the sampled event complexity. An overview of the entire sampling method is provided in Algorithm 1.",
                    "quote": []
                },
                {
                    "text": "This approach ensures that the generated events policies list maintains the desired complexity distribution and follows realistic transitions between policies as determined by the graph structure.",
                    "quote": []
                },
                {
                    "text": "Event generator agent. The goal of the event generator agent is to create an event based on a given list of policies. The primary challenge is to generate a valid and consistent initial database state that the chatbot can interact with during the conversation. The agent's architecture is shown in Figure 4. To manage complex database schemas, the event generator agent first creates a symbolic representation of all the entities involved in the event. These entities are determined based on the provided chatbot prompt and the database schema. Typical entities may include users, products, reservations, etc.",
                    "quote": []
                },
                {
                    "text": "The agent then iterates over these symbols, instantiating them by inserting the relevant rows into the database and replacing the symbolic variables with the corresponding data. This symbolic representation enables the agent to generate valid and consistent events, even across complex chatbot databases. Refer to Appendix B for an example of a generated symbolic representation and its corresponding database.",
                    "quote": []
                }
            ]
        },
        {
            "section": {
                "index": "3.2",
                "name": "Dialog simulation"
            },
            "p": [
                {
                    "text": "For each event in the events database, IntellAgent simulates an interaction between a user and the chatbot being tested. Figure 5 provides an overview of the simulation architecture. The user agent is given the event details, which include the description of the event and all relevant information inserted into the chatbot's system database by the event generator agent. Additionally, the user agent is provided with the expected behavior of the chatbot at each step of the interaction, based on the event's policy list. The user has the option to terminate the interaction at any point, either when the chatbot successfully completes the task or if the chatbot fails to adhere to one of the policies and does not follow the expected behavior.",
                    "quote": []
                }
            ]
        },
        {
            "section": {
                "index": "3.3",
                "name": "Dialog Critique"
            },
            "p": [
                {
                    "text": "The Dialog critique component is given the user-chatbot dialog and the chatbot system prompt to assess whether the reason for the dialog's termination, as provided by the user agent, is correct. If the reason is incorrect, the critique provides feedback to the user agent, and the dialog resumes. If the reason is correct, the critique then determines: (1) The subset of event policies that were tested during the dialog. (2) The subset of policies that the chatbot did not adhere to (this list may be empty). Using this information, a comprehensive report on the chatbot's performance is generated.",
                    "quote": []
                }
            ]
        },
        {
            "section": {
                "index": "4",
                "name": "Experiments"
            },
            "p": [
                {
                    "text": "We evaluated the performance of our system in real-world scenarios, utilizing the τ-bench [33] environments. Specifically, we employed the benchmark's prompts, database schema, and tools for the two benchmark's environments: airline and retail. For each environment, the system generates a policy graph from the prompt and simulates 1,000 events—a substantial increase that enables more fine-grained analysis compared to the original benchmark, which used 50 samples for airlines and 115 for retail. The complexity levels of the generated events range from 2 to 11.",
                    "quote": [
                        {
                            "text": "[33]",
                            "target": "#b32",
                            "type": "bibr",
                            "context": "τ-bench ",
                            "index": 91
                        }
                    ]
                }
            ]
        },
        {
            "section": {
                "index": "4.1",
                "name": "Benchmark"
            },
            "p": [
                {
                    "text": "Dataset construction. To evaluate the performance of our system in real-world scenarios, we utilized the τ-bench [33] environments. Specifically, we employed the benchmark's prompts, database schema, and tools for the two benchmark's environments: airline and retail. For each environment, the system generates a policy graph from the prompt and simulates 1,000 events—a substantial increase that enables more fine-grained analysis compared to the original benchmark, which used 50 samples for airlines and 115 for retail. The complexity levels of the generated events range from 2 to 11. Table 1 presents a comparison of various random walk sampling strategies. Selecting the next node uniformly leads to unrelated policies, whereas choosing the next node based on maximal weight produces a cohesive cluster of policies. In contrast, IntellAgent-weighted probability sampling achieves a balance between diversity and alignment with the real-world distribution. Appendix B provides a detailed example of the event generation process.",
                    "quote": [
                        {
                            "text": "[33]",
                            "target": "#b32",
                            "type": "bibr",
                            "context": "τ-bench ",
                            "index": 91
                        }
                    ]
                },
                {
                    "text": "Tested agents. We evaluated several state-of-the-art proprietary LLMs: GPT-4o, GPT-4o-mini, Gemini-1.5-pro, Gemini-1.5-flash, Claude-3.5-sonnet, and Claude-3.5-haiku. For all these models we employed the native tool-calling agent (supported by all the tested LLMs) with the tested environment system prompt. During each iteration, the model determines whether to send a message to the user or to call a tool.",
                    "quote": []
                },
                {
                    "text": "IntellAgent Implementation details. The IntellAgent multi-agent system was implemented using the Langgraph framework. GPT-4o served as the system's LLM model, used for the event generation, the user agent, and dialog critique. For full implementation details including all the system prompts, we provide the source code and documentation.",
                    "quote": []
                }
            ]
        },
        {
            "section": {
                "index": "4.2",
                "name": "Results"
            },
            "p": [
                {
                    "text": "Benchmarks comparison. Table 2 presents a comparison of the tested model's success rates on the τ-bench [33] and the IntellAgent benchmark. The Pearson correlation coefficients are 0.98 for the Airline environment and 0.92 for the Retail environment, highlighting a strong alignment in model performance across the two benchmarks. Notably, this strong correlation persists despite IntellAgent being generated exclusively with synthetic data.",
                    "quote": [
                        {
                            "text": "[33]",
                            "target": "#b32",
                            "type": "bibr",
                            "context": "τ-bench ",
                            "index": 82
                        }
                    ]
                },
                {
                    "text": "Models comparison. Figure 2 illustrates the success rates of the top four models as a function of challenge level. As expected, model performance declines as the challenge level increases, though the pattern of decline varies across models. For instance, while Gemini-pro-1.5 significantly outperforms GPT-4o-mini in the airline environment up to level 10, their performances converge at higher challenge levels. This highlights the value of IntellAgent's detailed analysis, enabling users to select the most suitable model based on the desired complexity the chatbot should handle.",
                    "quote": []
                },
                {
                    "text": "Policies comparison. Figure 3 shows the performance of the top four models models across different policy categories. The relative ranking of models shifts across different categories. IntellAgent provides a detailed analysis of the specific policies where the tested chatbot may encounter difficulties. It is also important to note that all models face challenges with user consent policies. This policy category is not assessed in the τ-bench [33] since its evaluation focuses solely on the final state of the database.",
                    "quote": [
                        {
                            "text": "[33]",
                            "target": "#b32",
                            "type": "bibr",
                            "context": "τ-bench ",
                            "index": 391
                        }
                    ]
                }
            ]
        },
        {
            "section": {
                "index": "5",
                "name": "Conclusion"
            },
            "p": [
                {
                    "text": "In this work, we introduced IntellAgent, a scalable, open-source multi-agent framework designed to comprehensively evaluate conversational AI systems. IntellAgent addresses the limitations of traditional evaluation methods by automating the generation of diverse, policy-driven scenarios and providing fine-grained diagnostics. By leveraging a graph-based policy model, realistic event generation, and user-agent simulations, IntellAgent captures the nuanced complexities of multi-turn dialogues, policy adherence, and tool integration.",
                    "quote": []
                },
                {
                    "text": "Our findings demonstrate IntellAgent's ability to uncover critical performance gaps, offering actionable insights to optimize conversational agents for real-world applications. Its modular design supports extensibility, ensuring it remains relevant across diverse domains and use cases.",
                    "quote": []
                },
                {
                    "text": "In future work, we plan to explore the benefits of incorporating additional real-world context into the environment, such as a small set of user-chatbot interactions. We hypothesize that this context could significantly enhance the policies graph quality by deriving edge weights and node challenge-level weights from the real-data distributions. Furthermore, we anticipate that this added context could improve the overall performance of the system database generation process.",
                    "quote": []
                }
            ]
        }
    ],
    "reference": [
        {
            "index": "b0",
            "title": "Conversational health agents: A personalized llm-powered agent framework",
            "author": [
                {
                    "forename": "M.",
                    "surname": "Abbasian",
                    "name": "M. Abbasian",
                    "email": ""
                },
                {
                    "forename": "I.",
                    "surname": "Azimi",
                    "name": "I. Azimi",
                    "email": ""
                },
                {
                    "forename": "A. M.",
                    "surname": "Rahmani",
                    "name": "A. M. Rahmani",
                    "email": ""
                },
                {
                    "forename": "R.",
                    "surname": "Jain",
                    "name": "R. Jain",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "",
            "date": "2023"
        },
        {
            "index": "b1",
            "title": "Curate: Benchmarking personalised alignment of conversational ai assistants",
            "author": [
                {
                    "forename": "L.",
                    "surname": "Alberts",
                    "name": "L. Alberts",
                    "email": ""
                },
                {
                    "forename": "B.",
                    "surname": "Ellis",
                    "name": "B. Ellis",
                    "email": ""
                },
                {
                    "forename": "A.",
                    "surname": "Lupu",
                    "name": "A. Lupu",
                    "email": ""
                },
                {
                    "forename": "J.",
                    "surname": "Foerster",
                    "name": "J. Foerster",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "International Conference on Learning Representations (ICLR)",
            "date": "2025"
        },
        {
            "index": "b2",
            "title": "Automated test generation to evaluate tool-augmented llms as conversational ai agents",
            "author": [
                {
                    "forename": "S.",
                    "surname": "Arcadinho",
                    "name": "S. Arcadinho",
                    "email": ""
                },
                {
                    "forename": "D.",
                    "surname": "Aparício",
                    "name": "D. Aparício",
                    "email": ""
                },
                {
                    "forename": "M. S. C.",
                    "surname": "Almeida",
                    "name": "M. S. C. Almeida",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "arXiv preprint arXiv:2409.15934",
            "date": "2024"
        },
        {
            "index": "b3",
            "title": "Benchmarking llm powered chatbots: Methods and metrics",
            "author": [
                {
                    "forename": "D.",
                    "surname": "Banerjee",
                    "name": "D. Banerjee",
                    "email": ""
                },
                {
                    "forename": "P.",
                    "surname": "Singh",
                    "name": "P. Singh",
                    "email": ""
                },
                {
                    "forename": "A.",
                    "surname": "Avadhanam",
                    "name": "A. Avadhanam",
                    "email": ""
                },
                {
                    "forename": "S.",
                    "surname": "Srivastava",
                    "name": "S. Srivastava",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "",
            "date": "2023"
        },
        {
            "index": "b4",
            "title": "Improving image generation with better captions",
            "author": [
                {
                    "forename": "J.",
                    "surname": "Betker",
                    "name": "J. Betker",
                    "email": ""
                },
                {
                    "forename": "G.",
                    "surname": "Goh",
                    "name": "G. Goh",
                    "email": ""
                },
                {
                    "forename": "L.",
                    "surname": "Jing",
                    "name": "L. Jing",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "",
            "date": "2023"
        },
        {
            "index": "b5",
            "title": "Beyond prompts: Dynamic conversational benchmarking of large language models",
            "author": [
                {
                    "forename": "D.",
                    "surname": "Castillo-Bolado",
                    "name": "D. Castillo-Bolado",
                    "email": ""
                },
                {
                    "forename": "J.",
                    "surname": "Davidson",
                    "name": "J. Davidson",
                    "email": ""
                },
                {
                    "forename": "F.",
                    "surname": "Gray",
                    "name": "F. Gray",
                    "email": ""
                },
                {
                    "forename": "M.",
                    "surname": "Rosa",
                    "name": "M. Rosa",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "arXiv preprint arXiv:2409.20222",
            "date": "2024"
        },
        {
            "index": "b6",
            "title": "Large language model agent in financial trading: A survey",
            "author": [
                {
                    "forename": "H.",
                    "surname": "Ding",
                    "name": "H. Ding",
                    "email": ""
                },
                {
                    "forename": "Y.",
                    "surname": "Li",
                    "name": "Y. Li",
                    "email": ""
                },
                {
                    "forename": "J.",
                    "surname": "Wang",
                    "name": "J. Wang",
                    "email": ""
                },
                {
                    "forename": "H.",
                    "surname": "Chen",
                    "name": "H. Chen",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "",
            "date": "2024"
        },
        {
            "index": "b7",
            "title": "Enhancing chat language models by scaling high-quality instructional conversations",
            "author": [
                {
                    "forename": "N.",
                    "surname": "Ding",
                    "name": "N. Ding",
                    "email": ""
                },
                {
                    "forename": "Y.",
                    "surname": "Chen",
                    "name": "Y. Chen",
                    "email": ""
                },
                {
                    "forename": "B.",
                    "surname": "Xu",
                    "name": "B. Xu",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "",
            "date": "2023"
        },
        {
            "index": "b8",
            "title": "Botchat: Evaluating llms' capabilities of having multi-turn dialogues",
            "author": [
                {
                    "forename": "H.",
                    "surname": "Duan",
                    "name": "H. Duan",
                    "email": ""
                },
                {
                    "forename": "J.",
                    "surname": "Wei",
                    "name": "J. Wei",
                    "email": ""
                },
                {
                    "forename": "C.",
                    "surname": "Wang",
                    "name": "C. Wang",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "",
            "date": "2023"
        },
        {
            "index": "b9",
            "title": "From local to global: A graph rag approach to query-focused summarization",
            "author": [
                {
                    "forename": "D.",
                    "surname": "Edge",
                    "name": "D. Edge",
                    "email": ""
                },
                {
                    "forename": "H.",
                    "surname": "Trinh",
                    "name": "H. Trinh",
                    "email": ""
                },
                {
                    "forename": "N.",
                    "surname": "Cheng",
                    "name": "N. Cheng",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "arXiv preprint arXiv:2404.16130",
            "date": "2024"
        },
        {
            "index": "b10",
            "title": "Ragas: Automated evaluation of retrieval-augmented generation",
            "author": [
                {
                    "forename": "S.",
                    "surname": "Es",
                    "name": "S. Es",
                    "email": ""
                },
                {
                    "forename": "J.",
                    "surname": "James",
                    "name": "J. James",
                    "email": ""
                },
                {
                    "forename": "L.",
                    "surname": "Espinosa-Anke",
                    "name": "L. Espinosa-Anke",
                    "email": ""
                },
                {
                    "forename": "S.",
                    "surname": "Schockaert",
                    "name": "S. Schockaert",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "arXiv preprint arXiv:2309.15217",
            "date": "2023"
        },
        {
            "index": "b11",
            "title": "Textbooks are all you need",
            "author": [
                {
                    "forename": "S.",
                    "surname": "Gunasekar",
                    "name": "S. Gunasekar",
                    "email": ""
                },
                {
                    "forename": "Y.",
                    "surname": "Zhang",
                    "name": "Y. Zhang",
                    "email": ""
                },
                {
                    "forename": "J.",
                    "surname": "Aneja",
                    "name": "J. Aneja",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "CoRR, abs/2306.11644",
            "date": "2023"
        },
        {
            "index": "b12",
            "title": "Unnatural instructions: Tuning language models with (almost) no human labor",
            "author": [
                {
                    "forename": "O.",
                    "surname": "Honovich",
                    "name": "O. Honovich",
                    "email": ""
                },
                {
                    "forename": "T.",
                    "surname": "Scialom",
                    "name": "T. Scialom",
                    "email": ""
                },
                {
                    "forename": "O.",
                    "surname": "Levy",
                    "name": "O. Levy",
                    "email": ""
                },
                {
                    "forename": "T.",
                    "surname": "Schick",
                    "name": "T. Schick",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "ACL",
            "date": "2023"
        },
        {
            "index": "b13",
            "title": "Crmarena: Understanding the capacity of llm agents to perform professional crm tasks in realistic environments",
            "author": [
                {
                    "forename": "K.-H.",
                    "surname": "Huang",
                    "name": "K.-H. Huang",
                    "email": ""
                },
                {
                    "forename": "A.",
                    "surname": "Prabhakar",
                    "name": "A. Prabhakar",
                    "email": ""
                },
                {
                    "forename": "S.",
                    "surname": "Dhawan",
                    "name": "S. Dhawan",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "",
            "date": "2024"
        },
        {
            "index": "b14",
            "title": "MT-eval: A multi-turn capabilities evaluation benchmark for large language models",
            "author": [
                {
                    "forename": "W.-C.",
                    "surname": "Kwan",
                    "name": "W.-C. Kwan",
                    "email": ""
                },
                {
                    "forename": "X.",
                    "surname": "Zeng",
                    "name": "X. Zeng",
                    "email": ""
                },
                {
                    "forename": "Y.",
                    "surname": "Jiang",
                    "name": "Y. Jiang",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
            "date": "2024"
        },
        {
            "index": "b15",
            "title": "Intent-based prompt calibration: Enhancing prompt optimization with synthetic boundary cases",
            "author": [
                {
                    "forename": "E.",
                    "surname": "Levi",
                    "name": "E. Levi",
                    "email": ""
                },
                {
                    "forename": "E.",
                    "surname": "Brosh",
                    "name": "E. Brosh",
                    "email": ""
                },
                {
                    "forename": "M.",
                    "surname": "Friedmann",
                    "name": "M. Friedmann",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "",
            "date": "2024"
        },
        {
            "index": "b16",
            "title": "On llms-driven synthetic data generation, curation, and evaluation: A survey",
            "author": [
                {
                    "forename": "L.",
                    "surname": "Long",
                    "name": "L. Long",
                    "email": ""
                },
                {
                    "forename": "R.",
                    "surname": "Wang",
                    "name": "R. Wang",
                    "email": ""
                },
                {
                    "forename": "R.",
                    "surname": "Xiao",
                    "name": "R. Xiao",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "",
            "date": "2024"
        },
        {
            "index": "b17",
            "title": "Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct",
            "author": [
                {
                    "forename": "H.",
                    "surname": "Luo",
                    "name": "H. Luo",
                    "email": ""
                },
                {
                    "forename": "Q.",
                    "surname": "Sun",
                    "name": "Q. Sun",
                    "email": ""
                },
                {
                    "forename": "C.",
                    "surname": "Xu",
                    "name": "C. Xu",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "CoRR, abs/2308.09583",
            "date": "2023"
        },
        {
            "index": "b18",
            "title": "Evaluation and continual improvement for an enterprise ai assistant",
            "author": [
                {
                    "forename": "A. V.",
                    "surname": "Maharaj",
                    "name": "A. V. Maharaj",
                    "email": ""
                },
                {
                    "forename": "K.",
                    "surname": "Qian",
                    "name": "K. Qian",
                    "email": ""
                },
                {
                    "forename": "U.",
                    "surname": "Bhattacharya",
                    "name": "U. Bhattacharya",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "",
            "date": "2024"
        },
        {
            "index": "b19",
            "title": "Evaluating large language models as agents in the clinic",
            "author": [
                {
                    "forename": "N.",
                    "surname": "Mehandru",
                    "name": "N. Mehandru",
                    "email": ""
                },
                {
                    "forename": "B. Y.",
                    "surname": "Miao",
                    "name": "B. Y. Miao",
                    "email": ""
                },
                {
                    "forename": "E. R.",
                    "surname": "Almaraz",
                    "name": "E. R. Almaraz",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "npj Digital Medicine",
            "date": "2024"
        },
        {
            "index": "b20",
            "title": "\"ask me anything\": How comcast uses llms to assist agents in real time",
            "author": [
                {
                    "forename": "S.",
                    "surname": "Rome",
                    "name": "S. Rome",
                    "email": ""
                },
                {
                    "forename": "T.",
                    "surname": "Chen",
                    "name": "T. Chen",
                    "email": ""
                },
                {
                    "forename": "R.",
                    "surname": "Tang",
                    "name": "R. Tang",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024",
            "date": "2024"
        },
        {
            "index": "b21",
            "title": "Code llama: Open foundation models for code",
            "author": [
                {
                    "forename": "B.",
                    "surname": "Rozière",
                    "name": "B. Rozière",
                    "email": ""
                },
                {
                    "forename": "J.",
                    "surname": "Gehring",
                    "name": "J. Gehring",
                    "email": ""
                },
                {
                    "forename": "F.",
                    "surname": "Gloeckle",
                    "name": "F. Gloeckle",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "CoRR, abs/2308.12950",
            "date": "2023"
        },
        {
            "index": "b22",
            "title": "Explore-instruct: Enhancing domain-specific instruction coverage through active exploration",
            "author": [
                {
                    "forename": "F.",
                    "surname": "Wan",
                    "name": "F. Wan",
                    "email": ""
                },
                {
                    "forename": "X.",
                    "surname": "Huang",
                    "name": "X. Huang",
                    "email": ""
                },
                {
                    "forename": "T.",
                    "surname": "Yang",
                    "name": "T. Yang",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "EMNLP",
            "date": "2023"
        },
        {
            "index": "b23",
            "title": "Improving text embeddings with large language models",
            "author": [
                {
                    "forename": "L.",
                    "surname": "Wang",
                    "name": "L. Wang",
                    "email": ""
                },
                {
                    "forename": "N.",
                    "surname": "Yang",
                    "name": "N. Yang",
                    "email": ""
                },
                {
                    "forename": "X.",
                    "surname": "Huang",
                    "name": "X. Huang",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "CoRR, abs/2401.00368",
            "date": "2024"
        },
        {
            "index": "b24",
            "title": "Let's synthesize step by step: Iterative dataset synthesis with large language models by extrapolating errors from small models",
            "author": [
                {
                    "forename": "R.",
                    "surname": "Wang",
                    "name": "R. Wang",
                    "email": ""
                },
                {
                    "forename": "W.",
                    "surname": "Zhou",
                    "name": "W. Zhou",
                    "email": ""
                },
                {
                    "forename": "M.",
                    "surname": "Sachan",
                    "name": "M. Sachan",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "EMNLP (Findings)",
            "date": "2023"
        },
        {
            "index": "b25",
            "title": "Tradingagents: Multi-agents llm financial trading framework",
            "author": [
                {
                    "forename": "Y.",
                    "surname": "Xiao",
                    "name": "Y. Xiao",
                    "email": ""
                },
                {
                    "forename": "E.",
                    "surname": "Sun",
                    "name": "E. Sun",
                    "email": ""
                },
                {
                    "forename": "D.",
                    "surname": "Luo",
                    "name": "D. Luo",
                    "email": ""
                },
                {
                    "forename": "W.",
                    "surname": "Wang",
                    "name": "W. Wang",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "",
            "date": "2024"
        },
        {
            "index": "b26",
            "title": "Wizardlm: Empowering large language models to follow complex instructions",
            "author": [
                {
                    "forename": "C.",
                    "surname": "Xu",
                    "name": "C. Xu",
                    "email": ""
                },
                {
                    "forename": "Q.",
                    "surname": "Sun",
                    "name": "Q. Sun",
                    "email": ""
                },
                {
                    "forename": "K.",
                    "surname": "Zheng",
                    "name": "K. Zheng",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "",
            "date": "2023"
        },
        {
            "index": "b27",
            "title": "Knowledge-infused prompting: Assessing and advancing clinical text data generation with large language models",
            "author": [
                {
                    "forename": "R.",
                    "surname": "Xu",
                    "name": "R. Xu",
                    "email": ""
                },
                {
                    "forename": "H.",
                    "surname": "Cui",
                    "name": "H. Cui",
                    "email": ""
                },
                {
                    "forename": "Y.",
                    "surname": "Yu",
                    "name": "Y. Yu",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "CoRR, abs/2311.00287",
            "date": "2023"
        },
        {
            "index": "b28",
            "title": "Eduagent: Generative student agents in learning",
            "author": [
                {
                    "forename": "S.",
                    "surname": "Xu",
                    "name": "S. Xu",
                    "email": ""
                },
                {
                    "forename": "X.",
                    "surname": "Zhang",
                    "name": "X. Zhang",
                    "email": ""
                },
                {
                    "forename": "L.",
                    "surname": "Qin",
                    "name": "L. Qin",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "",
            "date": "2024"
        },
        {
            "index": "b29",
            "title": "Finrobot: An open-source ai agent platform for financial applications using large language models",
            "author": [
                {
                    "forename": "H.",
                    "surname": "Yang",
                    "name": "H. Yang",
                    "email": ""
                },
                {
                    "forename": "B.",
                    "surname": "Zhang",
                    "name": "B. Zhang",
                    "email": ""
                },
                {
                    "forename": "N.",
                    "surname": "Wang",
                    "name": "N. Wang",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "",
            "date": "2024"
        },
        {
            "index": "b30",
            "title": "Content Knowledge Identification with Multi-agent Large Language Models (LLMs)",
            "author": [
                {
                    "forename": "K.",
                    "surname": "Yang",
                    "name": "K. Yang",
                    "email": ""
                },
                {
                    "forename": "Y.",
                    "surname": "Chu",
                    "name": "Y. Chu",
                    "email": ""
                },
                {
                    "forename": "T.",
                    "surname": "Darwin",
                    "name": "T. Darwin",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "Springer Nature Switzerland",
            "date": "2024"
        },
        {
            "index": "b31",
            "title": "Decoding data quality via synthetic corruptions: Embedding-guided pruning of code data",
            "author": [
                {
                    "forename": "Y.",
                    "surname": "Yang",
                    "name": "Y. Yang",
                    "email": ""
                },
                {
                    "forename": "A. K.",
                    "surname": "Singh",
                    "name": "A. K. Singh",
                    "email": ""
                },
                {
                    "forename": "M.",
                    "surname": "Elhoushi",
                    "name": "M. Elhoushi",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "CoRR, abs/2312.02418",
            "date": "2023"
        },
        {
            "index": "b32",
            "title": "τ-bench: A benchmark for tool-agent-user interaction in real-world domains",
            "author": [
                {
                    "forename": "S.",
                    "surname": "Yao",
                    "name": "S. Yao",
                    "email": ""
                },
                {
                    "forename": "N.",
                    "surname": "Shinn",
                    "name": "N. Shinn",
                    "email": ""
                },
                {
                    "forename": "P.",
                    "surname": "Razavi",
                    "name": "P. Razavi",
                    "email": ""
                },
                {
                    "forename": "K.",
                    "surname": "Narasimhan",
                    "name": "K. Narasimhan",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "arXiv preprint arXiv:2406.12045",
            "date": "2024"
        },
        {
            "index": "b33",
            "title": "Scaling relationship on learning mathematical reasoning with large language models",
            "author": [
                {
                    "forename": "Z.",
                    "surname": "Yuan",
                    "name": "Z. Yuan",
                    "email": ""
                },
                {
                    "forename": "H.",
                    "surname": "Yuan",
                    "name": "H. Yuan",
                    "email": ""
                },
                {
                    "forename": "C.",
                    "surname": "Li",
                    "name": "C. Li",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "CoRR, abs/2308.01825",
            "date": "2023"
        }
    ]
}