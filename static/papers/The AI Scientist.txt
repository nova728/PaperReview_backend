{
    "target": "zh",
    "model": "baidu",
    "title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
    "publication": {
        "publisher": {},
        "date": "2024-9-4"
    },
    "author": [
        {
            "forename": "Chris",
            "surname": "Lu",
            "name": "Chris Lu",
            "email": "chrislu@sakana.ai"
        },
        {
            "forename": "Cong",
            "surname": "Lu",
            "name": "Cong Lu",
            "email": "conglu@cs.ubc.ca"
        },
        {
            "forename": "Robert Tjarko",
            "surname": "Lange",
            "name": "Robert Tjarko Lange",
            "email": "robert@sakana.ai"
        },
        {
            "forename": "Jakob",
            "surname": "Foerster",
            "name": "Jakob Foerster",
            "email": ""
        },
        {
            "forename": "Jeff",
            "surname": "Clune",
            "name": "Jeff Clune",
            "email": ""
        },
        {
            "forename": "David",
            "surname": "Ha",
            "name": "David Ha",
            "email": ""
        }
    ],
    "abstract": [
        [
            "One of the grand challenges of artificial general intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used as aides to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they still conduct only a small part of the scientific process. This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models (LLMs) to perform research independently and communicate their findings. We introduce The AI Scientist, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation. In principle, this process can be repeated to iteratively develop ideas in an open-ended fashion and add them to a growing archive of knowledge, acting like the human scientific community. We demonstrate the versatility of this approach by applying it to three distinct subfields of machine learning: diffusion modeling, transformer-based language modeling, and learning dynamics. Each idea is implemented and developed into a full paper at a meager cost of less than $15 per paper, illustrating the potential for our framework to democratize research and significantly accelerate scientific progress. To evaluate the generated papers, we design and validate an automated reviewer, which we show achieves near-human performance in evaluating paper scores. The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by our automated reviewer. This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking us closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems. Our code is open-sourced at https://github.com/SakanaAI/AI-Scientist."
        ]
    ],
    "body": [
        {
            "section": {
                "index": "1",
                "name": "Introduction"
            },
            "p": [
                {
                    "text": "The modern scientific method is arguably one of the greatest achievements of the Enlightenment. Traditionally, a human researcher collects background knowledge, drafts a set of plausible hypotheses to test, constructs an evaluation procedure, collects evidence for the different hypotheses, and finally assesses and communicates their findings. Afterward, the resulting manuscript undergoes peer review and subsequent iterations of refinement. This procedure has led to countless breakthroughs in science and technology, improving human quality of life. However, this iterative process is inherently limited by human researchers' ingenuity, background knowledge, and finite time. Attempting to automate general scientific discovery has been a long ambition of the community since at least the early 70s, with computer-assisted works like the Automated Mathematician and DENDRAL. In the field of AI, researchers have envisioned the possibility of automating AI research using AI itself, leading to \"AI-generating algorithms\". More recently, foundation models have seen tremendous advances in their general capabilities, but they have only been shown to accelerate individual parts of the research pipeline, e.g. the writing of scientific manuscripts, as a muse to brainstorm ideas, or aides to coding. To date, the community has yet to show the possibility of executing entire research endeavors without human involvement.",
                    "quote": []
                },
                {
                    "text": "Traditional approaches to automating research projects have so far relied on carefully constraining the search space of potential discoveries, which severely limits the scope of exploration and requires substantial human expertise and design. For example, significant advancements in materials discovery and synthetic biology have been achieved by restricting exploration to well-characterized domains with predefined parameters, which allows for targeted progress but limits broader, open-ended discovery and addressing only a subset of the scientific process, without encompassing tasks such as manuscript preparation. Within the field of machine learning itself, research automation has largely been restricted to hyperparameter and architecture search or algorithm discovery within a hand-crafted search space. Recent advances in LLMs have shown the potential to extend the search space to more generalized, code-level solutions. However, these approaches remain constrained by rigorously-defined search spaces and objectives, which limit the breadth and depth of possible discoveries.",
                    "quote": []
                },
                {
                    "text": "In this paper, we introduce The AI Scientist, the first fully automated and scalable pipeline for end-to-end paper generation, enabled by recent advances in foundation models. Given a broad research direction and a simple initial codebase, The AI Scientist seamlessly performs ideation, a literature search, experiment planning, experiment iterations, manuscript writing, and peer reviewing to produce insightful papers. Furthermore, in principle The AI Scientist can run in an open-ended loop, building on its previous scientific discoveries to improve the next generation of ideas. This allows us to speed up the slow nature of scientific iteration at a surprisingly low financial cost (~$15/paper) and represents a step towards turning the world's ever-increasing computing resources into the scientific breakthroughs needed to tackle the core challenges of the 21st century. Here, we focus on Machine Learning (ML) applications, but this approach can more generally be applied to almost any other discipline, e.g. biology or physics, given an adequate way of automatically executing experiments.",
                    "quote": []
                },
                {
                    "text": "By leveraging modern LLM frameworks like chain-of-thought and self-reflection to improve decision-making, The AI Scientist is able to generate its own scientific ideas and hypotheses, as well as a plan for testing them with experiments. Next, The AI Scientist implements plan-directed code-level changes to the experiment \"template\" using the state-of-the-art coding assistant Aider, and executes experiments to collect a set of computational results, which are in turn used to draft a scientific paper. The AI Scientist then performs an automated paper-reviewing process using guidelines from a standard machine learning conference. Finally, The AI Scientist adds the completed ideas and reviewer feedback to its archive of scientific findings, and the process repeats. Crucially, the generated paper and experimental artifacts The AI Scientist produces allow us to easily interpret and judge its findings post-hoc, allowing human scientists to also benefit from what is learned.",
                    "quote": []
                }
            ]
        },
        {
            "section": {
                "index": "2",
                "name": "Background"
            },
            "p": [
                {
                    "text": "Large Language Models. In this paper, we build our automated scientist from autoregressive large language models (LLMs), which learn to generate text completions by modeling the conditional probability of a new token (similar to a word) given the preceding tokens, p(x_t|x_<t; θ), and sampling at test-time. Together with vast data and model scaling, this enables LLMs to not only generate coherent text, but crucially also exhibit human-like abilities, including commonsense knowledge, reasoning, and the ability to write code.",
                    "quote": []
                },
                {
                    "text": "LLM Agent Frameworks. Typical applications of LLMs often involve embedding the model into an \"agent\" framework, including the following possibilities: the structuring of language queries (e.g. few-shot prompting), encouraging reasoning traces (e.g. chain-of-thought), or asking the model to iteratively refine its outputs (e.g., self-reflection). These leverage the language model's ability to learn in-context and can greatly improve its performance, robustness and reliability on many tasks.",
                    "quote": []
                },
                {
                    "text": "Aider: An LLM-Based Coding Assistant. Our automated scientist directly implements ideas in code and uses a state-of-the-art open-source coding assistant, Aider. Aider is an agent framework that is designed to implement requested features, fix bugs, or refactor code in existing codebases. While Aider can in principle use any underlying LLM, with frontier models it achieves a remarkable success rate of 18.9% on the SWE Bench benchmark, a collection of real-world GitHub issues. In conjunction with new innovations added in this work, this level of reliability enables us, for the first time, to fully automate the ML research process.",
                    "quote": []
                }
            ]
        },
        {
            "section": {
                "index": "3",
                "name": "The AI Scientist"
            },
            "p": [
                {
                    "text": "Overview. The AI Scientist has three main phases: (1) Idea Generation, (2) Experimental Iteration, and (3) Paper Write-up. After the write-up, we introduce and validate an LLM-generated review to assess the quality of the generated paper. We provide The AI Scientist with a starting code template that reproduces a lightweight baseline training run from a popular model or benchmark. For example, this could be code that trains a small transformer on the works of Shakespeare, a classic proof-of-concept training run from natural language processing that completes within a few minutes. The AI Scientist is then free to explore any possible research direction. The template also includes a LaTeX folder that contains style files and section headers, along with simple plotting code. We provide further details on the templates in Section 6, but in general, each run starts with a representative small-scale experiment relevant to the topic area. The focus on small-scale experiments is not a fundamental limitation of our method, but simply for computational efficiency reasons and compute constraints on our end. We provide the prompts for all stages in Appendix A.",
                    "quote": []
                },
                {
                    "text": "1. Idea Generation. Given a starting template, The AI Scientist first \"brainstorms\" a diverse set of novel research directions. We take inspiration from evolutionary computation and open-endedness research and iteratively grow an archive of ideas using LLMs as the mutation operator. Each idea comprises a description, experiment execution plan, and (self-assessed) numerical scores of interestingness, novelty, and feasibility. At each iteration, we prompt the language model to generate an interesting new research direction conditional on the existing archive, which can include the numerical review scores from completed previous ideas. We use multiple rounds of chain-of-thought and self-reflection to refine and develop each idea. After idea generation, we filter ideas by connecting the language model with the Semantic Scholar API and web access as a tool. This allows The AI Scientist to discard any idea that is too similar to existing literature.",
                    "quote": []
                },
                {
                    "text": "2. Experiment Iteration. Given an idea and a template, the second phase of The AI Scientist first executes the proposed experiments and then visualizes its results for the downstream write-up. The AI Scientist uses Aider to first plan a list of experiments to run and then executes them in order. We make this process more robust by returning any errors upon a failure or time-out (e.g. experiments taking too long to run) to Aider to fix the code and re-attempt up to four times. After the completion of each experiment, Aider is then given the results and told to take notes in the style of an experimental journal. Currently, it only conditions on text but in future versions, this could include data visualizations or any modality. Conditional on the results, it then re-plans and implements the next experiment. This process is repeated up to five times. Upon completion of experiments, Aider is prompted to edit a plotting script to create figures for the paper using Python. The AI Scientist makes a note describing what each plot contains, enabling the saved figures and experimental notes to provide all the information required to write up the paper. At all steps, Aider sees its history of execution.",
                    "quote": []
                },
                {
                    "text": "Note that, in general, the provided initial seed plotting and experiment templates are small, self-contained files. The AI Scientist frequently implements entirely new plots and collects new metrics that are not in the seed templates. This ability to arbitrarily edit the code occasionally leads to unexpected outcomes.",
                    "quote": []
                },
                {
                    "text": "3. Paper Write-up. The third phase of The AI Scientist produces a concise and informative write-up of its progress in the style of a standard machine learning conference proceeding in LaTeX. We note that writing good LaTeX can even take competent human researchers some time, so we take several steps to robustify the process. This consists of the following:",
                    "quote": []
                },
                {
                    "text": "(a) Per-Section Text Generation: The recorded notes and plots are passed to Aider, which is prompted to fill in a blank conference template section by section. This goes in order of introduction, background, methods, experimental setup, results, and then the conclusion (all sections apart from the related work). All previous sections of the paper it has already written are in the context of the language model. We include brief tips and guidelines on what each section should include, based on the popular \"How to ML Paper\" guide, and include details in Appendix A.3. At each step of writing, Aider is prompted to only use real experimental results in the form of notes and figures generated from code, and real citations to reduce hallucination. Each section is initially refined with one round of self-reflection as it is being written. Aider is prompted to not include any citations in the text at this stage, and fill in only a skeleton for the related work, which will be completed in the next stage.",
                    "quote": []
                },
                {
                    "text": "(b) Web Search for References: In a similar vein to idea generation, The AI Scientist is allowed 20 rounds to poll the Semantic Scholar API looking for the most relevant sources to compare and contrast the near-completed paper against for the related work section. This process also allows The AI Scientist to select any papers it would like to discuss and additionally fill in any citations that are missing from other sections of the paper. Alongside each selected paper, a short description is produced of where and how to include the citation, which is then passed to Aider. The paper's bibtex is automatically appended to the LaTeX file to guarantee correctness.",
                    "quote": []
                },
                {
                    "text": "(c) Refinement: After the previous two stages, The AI Scientist has a completed first draft, but can often be overly verbose and repetitive. To resolve this, we perform one final round of self-reflection section-by-section, aiming to remove any duplicated information and streamline the arguments of the paper.",
                    "quote": []
                },
                {
                    "text": "(d) Compilation: Once the LaTeX template has been filled in with all the appropriate results, this is fed into a LaTeX compiler. We use a LaTeX linter and pipe compilation errors back into Aider so that it can automatically correct any issues.",
                    "quote": []
                }
            ]
        },
        {
            "section": {
                "index": "4",
                "name": "Automated Paper Reviewing"
            },
            "p": [
                {
                    "text": "An LLM Reviewer Agent. A key component of an effective scientific community is its reviewing system, which evaluates and improves the quality of scientific papers. To mimic such a process using large language models, we design a GPT-4o-based agent to conduct paper reviews based on the Neural Information Processing Systems (NeurIPS) conference review guidelines. The review agent processes the raw text of the PDF manuscript using the PyMuPDF parsing library. The output contains numerical scores (soundness, presentation, contribution, overall, confidence), lists of weaknesses and strengths as well as a preliminary binary decision (accept or reject). These decisions may then be post-calibrated by thresholding using the reviewer score. We leverage this automated reviewing process to obtain an initial evaluation of the papers generated by The AI Scientist. We provide the entire reviewing prompt template in Appendix A.4.",
                    "quote": []
                },
                {
                    "text": "Evaluating the Automated Reviewer. To evaluate the LLM-based reviewer's performance, we compared the artificially generated decisions with ground truth data for 500 ICLR 2022 papers extracted from the publicly available OpenReview dataset. Similar to the previous section, we combine many recent advancements in LLM agents to make the decision-making process robust. More specifically, we improve the base LLM's decision-making process by leveraging self-reflection, providing few-shot examples and response ensembling. With GPT-4o, The AI Scientist's reviewing procedure achieves 70% accuracy when combining 5 rounds of self-reflection, 5 ensembled reviews, and a 1-shot review example taken from the ICLR 2022 review guidelines. Afterward, we perform an LLM-based meta-review, which prompts the agent to act as an Area Chair (full prompts in Appendix A.4). While this number is lower than the 73% accuracy that was reported for humans in the NeurIPS 2021 consistency experiment, the automated reviewer achieves superhuman F1 Scores (0.57 vs. 0.49) and human-level AUC (0.65 for both) when thresholding the decision at a score of 6 (a \"Weak Accept\" in the NeurIPS review guidelines). This choice corresponds roughly to the average score of accepted papers.",
                    "quote": []
                },
                {
                    "text": "The considered ICLR 2022 paper dataset is very class-imbalanced, i.e. it contains many more rejected papers. When considering a balanced dataset of papers, The AI Scientist's reviewing process achieves human-level accuracy (0.65% vs. 0.66%). Furthermore, the False Negative Rate (FNR) is much lower than the human baseline (0.39 vs. 0.52). Hence, the LLM-based review agent rejects fewer high-quality papers. The False Positive Rate (FNR), on the other hand, is higher (0.31 vs. 0.17) highlighting room for potential future improvements.",
                    "quote": []
                },
                {
                    "text": "To further validate the performance of the automated reviewer, we compare the consistency of the overall paper scores between anonymous OpenReview reviewers randomly sampled pairwise per paper and between the average of all reviewers and the LLM score. For the set of 500 ICLR 2022 papers, we find that the correlation between the score of two human reviewers is smaller (0.14) than the correlation between the LLM score and the average score across the reviewers (0.18). Overall, across all metrics, the results suggest that LLM-based reviews can not only provide valuable feedback but also align more closely with the average human reviewer score than individual human reviewers align with each other.",
                    "quote": []
                },
                {
                    "text": "Each review is generated for $0.25 to $0.50 in API costs. We additionally compared the reviewing performance of various other foundation models. While Claude Sonnet 3.5 and GPT-4o-mini provide a more cost-efficient approach, their performance was substantially worse. Moreover, we had to threshold scores at 8 for Sonnet 3.5 to obtain calibrated results, due to persistent over-optimism bias. Llama 3.1 405B struggled to follow the reviewer output template consistently. We open-source our code, providing a new and interesting LLM benchmark for the community.",
                    "quote": []
                },
                {
                    "text": "LLM Reviewer Ablations. We compare various prompt configurations for GPT-4o and find that both Reflexion (+2%) and one-shot prompting (+2%) substantially help with performing more accurate reviewing. On the other hand, using review ensembling does not appear to improve the reviewer's performance substantially but can reduce variance. In the following sections, we used our best overall reviewer: GPT-4o with 5 rounds of self-reflection, 5 ensembled reviews, a meta-aggregation step, and 1 few-shot example.",
                    "quote": []
                }
            ]
        },
        {
            "section": {
                "index": "5",
                "name": "In-Depth Case Study"
            },
            "p": [
                {
                    "text": "Before we present extensive experiments and metrics for The AI Scientist's generated papers in Section 6, we first visualize a representative sample from a run of the The AI Scientist which illustrates both its strengths and shortcomings, followed by a broader discussion of its potential. The selected paper \"Adaptive Dual-Scale Denoising\" is generated from a run where The AI Scientist is asked to do research on diffusion modeling, which is fully detailed in Section 6.1. The base foundation model was Claude Sonnet 3.5.",
                    "quote": []
                },
                {
                    "text": "Generated Idea. As discussed in Section 3, The AI Scientist first generates an idea based on the provided template and its previous archive of discoveries. The idea in the selected paper was proposed in the 6th iteration of the algorithm and aims to improve the ability of diffusion models to capture both global structure and local details in a 2D dataset, by proposing two branches in the standard denoiser network. This is a well-motivated direction that has been the primary reason for researchers adopting diffusion models over prior styles of generative models such as VAEs and GANs, and to the best of our knowledge has not been widely studied.",
                    "quote": []
                },
                {
                    "text": "We highlight that The AI Scientist generates an impressive experimental plan that includes the proposed code modification, comparison to baselines, evaluation metrics, and the design of additional plots. As has been previously observed in the literature, judgments by LLMs can often have bias which we can observe in over-estimation of an idea's interestingness, feasibility, or novelty. The \"novel\" flag at the end indicates The AI Scientist believes the idea is novel after searching for related papers using the Semantic Scholar API.",
                    "quote": []
                },
                {
                    "text": "Generated Experiments. We display the generated code diff (deletions are in red, and additions are in green) for the substantial algorithmic changes below. The code matches the experimental description and is well-commented. The AI Scientist is able to iterate on the code with results from intermediate experiments in the loop, and it eventually ends up with interesting design choices for the adaptive weight network, e.g. a LeakyReLU. Importantly, this network has a well-behaved output that is guaranteed to be between 0 and 1. We additionally note that The AI Scientist changed the output of the network to return the adaptive weights to make new visualizations.",
                    "quote": []
                },
                {
                    "text": "Generated Paper. The AI Scientist generates an 11-page scientific manuscript in the style of a standard machine learning conference submission complete with visualizations and all standard sections. We display a preview of the completely AI-generated paper in Figure 3, with the full-sized version available in Appendix D.1.",
                    "quote": []
                }
            ]
        },
        {
            "section": {
                "index": "6",
                "name": "Experiments"
            },
            "p": [
                {
                    "text": "We extensively evaluate The AI Scientist on three templates as described in Section 3 across different publicly available LLMs: Claude Sonnet 3.5, GPT-4o, DeepSeek Coder, and Llama-3.1 405b. The first two models are only available by a public API, whilst the second two models are open-weight. For each run, we provide 1-2 basic seed ideas as examples (e.g. modifying the learning rate or batch size) and have it generate another 50 new ideas. We visualize an example progression of proposed ideas in Appendix C.",
                    "quote": []
                },
                {
                    "text": "Each run of around fifty ideas in total takes approximately 12 hours on 8× NVIDIA H100s. We report the number of ideas that pass the automated novelty check, successfully complete experiments, and result in valid compilable manuscripts. Note that the automated novelty check and search are self-assessed by each model for its own ideas, making relative \"novelty\" comparisons challenging. Additionally, we provide the mean and max reviewer scores of the generated papers and the total cost of the run. Finally, we select and briefly analyze some of the generated papers, which are listed below. The full papers can be found in Appendix D, alongside the generated reviews and code.",
                    "quote": []
                },
                {
                    "text": "In practice, we make one departure from the formal description of The AI Scientist, and generate ideas without waiting for paper evaluations to be appended to the archive in order to parallelize more effectively. This allowed us to pay the cost of the idea generation phase only once and iterate faster; furthermore, we did not observe any reduction in the quality of the papers generated as measured by the average review score with this modification.",
                    "quote": []
                },
                {
                    "text": "From manual inspection, we find that Claude Sonnet 3.5 consistently produces the highest quality papers, with GPT-4o coming in second. We provide a link to all papers, run files, and logs in our GitHub repository, and recommend viewing the uploaded Claude papers for a qualitative analysis. This observation is also validated by the scores obtained from the LLM reviewer. When dividing the number of generated papers by the total cost, we end up at a cost of around $10-15 per paper. Notably, GPT-4o struggles with writing LaTeX, which prevents it from completing many of its papers. For the open-weight models, DeepSeek Coder is significantly cheaper but often fails to correctly call the Aider tools. Llama-3.1 405b performed the worst overall but was the most convenient to work with, as we were frequently rate-limited by other providers. Both DeepSeek Coder and Llama-3.1 405b often had missing sections and results in their generated papers. In the following subsections, we will describe each template, its corresponding results, and specific papers.",
                    "quote": []
                }
            ]
        },
        {
            "section": {
                "index": "6.1",
                "name": "Diffusion Modeling"
            },
            "p": [
                {
                    "text": "General Description: This template studies improving the performance of diffusion generative models on low-dimensional datasets. Compared to image generation, low-dimensional diffusion is much less well-studied, and thus there may be interesting algorithmic contributions to be made here.",
                    "quote": []
                },
                {
                    "text": "Code Template: We base this template on a modified version of the popular 'tanelp/tiny-diffusion' repository with additional minor hyperparameter tuning added and exponential moving average on the weights. The diffusion models are DDPM models trained to generate samples from four distributions including geometric shapes, the two moons dataset, and a 2D dinosaur. The denoiser network is parameterized as an MLP with sinusoidal embeddings for the diffusion timestep and input data. The plotting script visualizes generated samples and plots training loss by default. Estimated KL is provided as an additional metric for sample quality via non-parametric entropy estimation.",
                    "quote": []
                }
            ]
        },
        {
            "section": {
                "index": "6.2",
                "name": "Language Modeling"
            },
            "p": [
                {
                    "text": "General Description: This template investigates transformer-based autoregressive next-token prediction tasks. Because this task is widely studied and optimized, it is difficult for The AI Scientist to find significant improvements. There are some common failure modes for this template that result in impressive-looking, but deceptive results. For example, a few of its ideas effectively cheat by subtly leaking information from future tokens, which results in lower perplexity.",
                    "quote": []
                },
                {
                    "text": "Code Template: The code is modified from the popular NanoGPT repository. The provided script template trains a small transformer language model on the character-level Shakespeare dataset, the enwik8 dataset, and the text8 dataset. It runs three seeds on the Shakespeare dataset, and one each on the remaining ones. The code saves the runtime, validation losses, and train losses. The plotting script visualizes training curves by default.",
                    "quote": []
                }
            ]
        },
        {
            "section": {
                "index": "6.3",
                "name": "Grokking Analysis"
            },
            "p": [
                {
                    "text": "General Description: This template investigates questions about generalization and learning speed in deep neural networks. We follow the classic experimental paradigm reported in Power et al. (2022) for analyzing \"grokking\", a poorly understood phenomenon in which validation accuracy dramatically improves long after the train loss saturates. Unlike the previous templates, this one is more amenable to open-ended empirical analysis (e.g. what conditions grokking occurs) rather than just trying to improve performance metrics.",
                    "quote": []
                },
                {
                    "text": "Code Template: We base our implementation off of two popular open source re-implementations of Power et al. (2022). The code generates four synthetic datasets of modular arithmetic tasks and trains a transformer on each across three random seeds. It returns train losses, validation losses, and the number of update steps required to reach perfect validation accuracy. The plotting scripts visualize the training and validation curves by default.",
                    "quote": []
                }
            ]
        },
        {
            "section": {
                "index": "7",
                "name": "Related Work"
            },
            "p": [
                {
                    "text": "While there has been a long tradition of automatically optimizing individual parts of the ML pipeline (AutoML), none come close to the full automation of the entire research process, particularly in communicating obtained scientific insights in an interpretable and general format.",
                    "quote": []
                },
                {
                    "text": "LLMs for Machine Learning Research. Most closely related to our work are those that use LLMs to assist machine learning research. Huang et al. (2024) propose a benchmark for measuring how successfully LLMs can write code to solve a variety of machine learning tasks. Lu et al. (2024a) use LLMs to propose, implement, and evaluate new state-of-the-art algorithms for preference optimization. Liang et al. (2024) use LLMs to provide feedback on research papers and find that they provide similar feedback to human reviewers, while Girotra et al. (2023) find that LLMs can consistently produce higher quality ideas for innovation than humans. Baek et al. (2024); Wang et al. (2024b) use LLMs to propose research ideas based on scientific literature search but do not execute them. Wang et al. (2024c) automatically writes surveys based on an extensive literature search. Our work can be seen as the synthesis of all these distinct threads, resulting in a single autonomous open-ended system that can execute the entire machine learning research process.",
                    "quote": []
                },
                {
                    "text": "LLMs for Structured Exploration. Because LLMs contain many human-relevant priors, they are commonly used as a tool to explore large search spaces. For example, recent works have used LLM coding capabilities to explore reward functions, virtual robotic design, environment design, and neural architecture search. LLMs can also act as evaluators for \"interestingness\" and as recombination operators for black-box optimization with Evolution Strategies and for Quality-Diversity approaches. Our work combines many of these notions, including that our LLM Reviewer judges papers on novelty and interestingness, and that many proposed ideas are new combinations of previous ones.",
                    "quote": []
                },
                {
                    "text": "AI for Scientific Discovery. There has been a long tradition of AI assisting scientific discovery across many other fields. For example, AI has been used for chemistry, synthetic biology, materials discovery, mathematics, and algorithm search. Other works aim to analyze existing pre-collected datasets and find novel insights. Unlike our work, these are usually restricted to a well-defined search space in a single domain and do not involve \"ideation\", writing, or peer review from the AI system. In its current form, The AI Scientist excels at conducting research ideas implemented via code; with future advances (e.g. robotic automation for wet labs), the transformative benefits of our approach could reach across all science, especially as foundation models continue to improve.",
                    "quote": []
                }
            ]
        },
        {
            "section": {
                "index": "8",
                "name": "Limitations & Ethical Considerations"
            },
            "p": [
                {
                    "text": "While The AI Scientist produces research that can provide novel insights, it has many limitations and raises several important ethical considerations. We believe future versions of The AI Scientist will be able to address many of its current shortcomings.",
                    "quote": []
                },
                {
                    "text": "Limitations of the Automated Reviewer. While the automated reviewer shows promising initial results, there are several potential areas for improvement. The dataset used, from ICLR 2022, is old enough to potentially appear in the base model pre-training data - this is a hard claim to test in practice since typical publicly available LLMs do not share their training data. However, preliminary analysis showed that LLMs were far from being able to reproduce old reviews exactly from initial segments, which suggests they have not memorized this data. Furthermore, the rejected papers in our dataset used the original submission file, whereas for the accepted papers only the final camera-ready copies were available on OpenReview. Future iterations could use more recent submissions (e.g. from TMLR) for evaluation. Unlike standard reviewers, the automated reviewer is unable to ask questions to the authors in a rebuttal phase, although this could readily be incorporated into our framework. Finally, since it does not currently use any vision capabilities, The AI Scientist (including the reviewer) is unable to view figures and must rely on textual descriptions of them.",
                    "quote": []
                },
                {
                    "text": "Common Failure Modes. The AI Scientist, in its current form, has several shortcomings in addition to those already identified in Section 5. These also include, but are not limited to:",
                    "quote": []
                },
                {
                    "text": "• The idea generation process often results in very similar ideas across different runs and even models. It may be possible to overcome this by allowing The AI Scientist to directly follow up and go deeper on its best ideas, or by providing it content from recently-published papers as a source of novelty.",
                    "quote": []
                },
                {
                    "text": "• As shown in Tables 3 to 5, Aider fails to implement a significant fraction of the proposed ideas. Furthermore, GPT-4o in particular frequently fails to write LaTeX that compiles. While The AI Scientist can come up with creative and promising ideas, they are often too challenging for it to implement.",
                    "quote": []
                },
                {
                    "text": "• The AI Scientist may incorrectly implement an idea, which can be difficult to catch. An adversarial code-checking reviewer may partially address this. As-is, one should manually check the implementation before trusting the reported results.",
                    "quote": []
                },
                {
                    "text": "• Because of The AI Scientist's limited number of experiments per idea, the results often do not meet the expected rigor and depth of a standard ML conference paper. Furthermore, due to the limited number of experiments we could afford to give it, it is difficult for The AI Scientist to conduct fair experiments that control for the number of parameters, FLOPs, or runtime. This often leads to deceptive or inaccurate conclusions. We expect that these issues will be mitigated as the cost of compute and foundation models continues to drop.",
                    "quote": []
                },
                {
                    "text": "• Since we do not currently use the vision capabilities of foundation models, it is unable to fix visual issues with the paper or read plots. For example, the generated plots are sometimes unreadable, tables sometimes exceed the width of the page, and the page layout (including the overall visual appearance of the paper) is often suboptimal. Future versions with vision and other modalities should fix this.",
                    "quote": []
                },
                {
                    "text": "• When writing, The AI Scientist sometimes struggles to find and cite the most relevant papers. It also commonly fails to correctly reference figures in LaTeX, and sometimes even hallucinates invalid file paths.",
                    "quote": []
                },
                {
                    "text": "• Importantly, The AI Scientist occasionally makes critical errors when writing and evaluating results. For example, it struggles to compare the magnitude of two numbers, which is a known pathology with LLMs. Furthermore, when it changes a metric (e.g. the loss function), it sometimes does not take this into account when comparing it to the baseline. To partially address this, we make sure all experimental results are reproducible, storing copies of all files when they are executed.",
                    "quote": []
                },
                {
                    "text": "• Rarely, The AI Scientist can hallucinate entire results. For example, an early version of our writing prompt told it to always include confidence intervals and ablation studies. Due to computational constraints, The AI Scientist did not always collect additional results; however, in these cases, it would sometimes hallucinate an entire ablations table. We resolved this by instructing The AI Scientist explicitly to only include results it directly observed. Furthermore, it frequently hallucinates facts we do not provide, such as the hardware used.",
                    "quote": []
                },
                {
                    "text": "• More generally, we do not recommend taking the scientific content of this version of The AI Scientist at face value. Instead, we advise treating generated papers as hints of promising ideas for practitioners to follow up on. Nonetheless, we expect the trustworthiness of The AI Scientist to increase dramatically in the coming years in tandem with improvements to foundation models. We share this paper and code primarily to show what is currently possible and hint at what is likely to be possible soon.",
                    "quote": []
                },
                {
                    "text": "Safe Code Execution. The current implementation of The AI Scientist has minimal direct sandboxing in the code, leading to several unexpected and sometimes undesirable outcomes if not appropriately guarded against. For example, in one run, The AI Scientist wrote code in the experiment file that initiated a system call to relaunch itself, causing an uncontrolled increase in Python processes and eventually necessitating manual intervention. In another run, The AI Scientist edited the code to save a checkpoint for every update step, which took up nearly a terabyte of storage. In some cases, when The AI Scientist's experiments exceeded our imposed time limits, it attempted to edit the code to extend the time limit arbitrarily instead of trying to shorten the runtime. While creative, the act of bypassing the experimenter's imposed constraints has potential implications for AI safety. Moreover, The AI Scientist occasionally imported unfamiliar Python libraries, further exacerbating safety concerns. We recommend strict sandboxing when running The AI Scientist, such as containerization, restricted internet access (except for Semantic Scholar), and limitations on storage usage.",
                    "quote": []
                },
                {
                    "text": "At the same time, there were several unexpected positive results from the lack of guardrails. For example, we had forgotten to create the output results directory in the grokking template in our experiments. Each successful run from The AI Scientist that outputted a paper automatically caught this error when it occurred and fixed it. Furthermore, we found that The AI Scientist would occasionally include results and plots that we found surprising, differing significantly from the provided templates. We describe some of these novel algorithm-specific visualizations in Section 6.1.",
                    "quote": []
                },
                {
                    "text": "Broader Impact and Ethical Considerations. While The AI Scientist has the potential to be a valuable tool for researchers, it also carries significant risks of misuse. The ability to automatically generate and submit papers to academic venues could greatly increase the workload for reviewers, potentially overwhelming the peer review process and compromising scientific quality control. Similar concerns have been raised about generative AI in other fields, such as its impact on the arts. Furthermore, if the Automated Reviewer tool was widely adopted by reviewers, it could diminish the quality of reviews and introduce undesirable biases into the evaluation of papers. Because of this, we believe that papers or reviews that are substantially AI-generated must be marked as such for full transparency.",
                    "quote": []
                },
                {
                    "text": "As with most previous technological advances, The AI Scientist has the potential to be used in unethical ways. For example, it could be explicitly deployed to conduct unethical research, or even lead to unintended harm if The AI Scientist conducts unsafe research. Concretely, if it were encouraged to find novel, interesting biological materials and given access to \"cloud labs\" where robots perform wet lab biology experiments, it could (without its overseer's intent) create new, dangerous viruses or poisons that harm people before we can intervene. Even in computers, if tasked to create new, interesting, functional software, it could create dangerous malware. The AI Scientist's current capabilities, which will only improve, reinforce that the machine learning community needs to immediately prioritize learning how to align such systems to explore in a manner that is safe and consistent with our values.",
                    "quote": []
                }
            ]
        },
        {
            "section": {
                "index": "9",
                "name": "Discussion"
            },
            "p": [
                {
                    "text": "In this paper, we introduced The AI Scientist, the first framework designed to fully automate the scientific discovery process, and, as a first demonstration of its capabilities, applied it to machine learning itself. This end-to-end system leverages LLMs to autonomously generate research ideas, implement and execute experiments, search for related works, and produce comprehensive research papers. By integrating stages of ideation, experimentation, and iterative refinement, The AI Scientist aims to replicate the human scientific process in an automated and scalable manner.",
                    "quote": []
                },
                {
                    "text": "Why does writing papers matter? Given our overarching goal to automate scientific discovery, why are we also motivated to have The AI Scientist write papers, like human scientists? For example, previous AI-enabled systems such as FunSearch and GNoME also conduct impressive scientific discovery in restricted domains, but they do not write papers.",
                    "quote": []
                },
                {
                    "text": "There are several reasons why we believe it is fundamentally important for The AI Scientist to write scientific papers to communicate its discoveries. First, writing papers offers a highly interpretable method for humans to benefit from what has been learned. Second, reviewing written papers within the framework of existing machine learning conferences enables us to standardize evaluation. Third, the scientific paper has been the primary medium for disseminating research findings since the dawn of modern science. Since a paper can use natural language, and include plots and code, it can flexibly describe any type of scientific study and discovery. Almost any other conceivable format is locked into a certain kind of data or type of science. Until a superior alternative emerges (or possibly invented by AI), we believe that training The AI Scientist to produce scientific papers is essential for its integration into the broader scientific community.",
                    "quote": []
                },
                {
                    "text": "Costs. Our framework is remarkably versatile and effectively conducts research across various subfields of machine learning, including transformer-based language modeling, neural network learning dynamics, and diffusion modeling. The cost-effectiveness of the system, producing papers with potential conference relevance at an approximate cost of $15 per paper, highlights its ability to democratize research (increase its accessibility) and accelerate scientific progress. Preliminary qualitative analysis, for example in Section 5, suggests that the generated papers can be broadly informative and novel, or at least contain ideas worthy of future study.",
                    "quote": []
                },
                {
                    "text": "The actual compute we allocated for The AI Scientist to conduct its experiments in this work is also incredibly light by today's standards. Notably, our experiments generating hundreds of papers were largely run only using a single 8×NVIDIA H100 node over the course of a week. Massively scaling the search and filtering would likely result in significantly higher-quality papers.",
                    "quote": []
                },
                {
                    "text": "In this project, the bulk of the cost for running The AI Scientist is associated with the LLM API costs for coding and paper writing. In contrast, the costs associated with running the LLM reviewer, as well as the computational expenses for conducting experiments, are negligible due to the constraints we've imposed to keep overall costs down. However, this cost breakdown may change in the future if The AI Scientist is applied to other scientific fields or used for larger-scale computational experiments.",
                    "quote": []
                },
                {
                    "text": "Open vs. Closed Models. To quantitatively evaluate and improve the generated papers, we first created and validated an Automated Paper Reviewer. We show that, although there is significant room for improvement, LLMs are capable of producing reasonably accurate reviews, achieving results comparable to humans across various metrics. Applying this evaluator to the papers generated by The AI Scientist enables us to scale the evaluation of our papers beyond manual inspection.",
                    "quote": []
                },
                {
                    "text": "We find that Sonnet 3.5 consistently produces the best papers, with a few of them even achieving a score that exceeds the threshold for acceptance at a standard machine learning conference from the Automated Paper Reviewer.",
                    "quote": []
                },
                {
                    "text": "However, there is no fundamental reason to expect a single model like Sonnet 3.5 to maintain its lead. We anticipate that all frontier LLMs, including open models, will continue to improve. The competition among LLMs has led to their commoditization and increased capabilities. Therefore, our work aims to be model-agnostic regarding the foundation model provider. In this project, we studied various proprietary LLMs, including GPT-4o and Sonnet, but also explored using open models like DeepSeek and Llama-3. We found that open models offer significant benefits, such as lower costs, guaranteed availability, greater transparency, and flexibility, although slightly worse quality. In the future, we aim to use our proposed discovery process to produce self-improving AI in a closed-loop system using open models.",
                    "quote": []
                },
                {
                    "text": "Future Directions. Direct enhancements to The AI Scientist could include integrating vision capabilities for better plot and figure handling, incorporating human feedback and interaction to refine the AI's outputs, and enabling The AI Scientist to automatically expand the scope of its experiments by pulling in new data and models from the internet, provided this can be done safely. Additionally, The AI Scientist could follow up on its best ideas or even perform research directly on its own code in a self-referential manner. Indeed, significant portions of the code for this project were written by Aider. Expanding the framework to other scientific domains could further amplify its impact, paving the way for a new era of automated scientific discovery. For example, by integrating these technologies with cloud robotics and automation in physical lab spaces provided it can be done safely, The AI Scientist could perform experiments for biology, chemistry, and material sciences.",
                    "quote": []
                },
                {
                    "text": "Crucially, future work should address the reliability and hallucination concerns, potentially through a more in-depth automatic verification of the reported results. This could be done by directly linking code and experiments, or by seeing if an automated verifier can independently reproduce the results.",
                    "quote": []
                },
                {
                    "text": "Conclusion. The introduction of The AI Scientist marks a significant step towards realizing the full potential of AI in scientific research. By automating the discovery process and incorporating an AI-driven review system, we open the door to endless possibilities for innovation and problem-solving in the most challenging areas of science and technology. Ultimately, we envision a fully AI-driven scientific ecosystem including not only AI-driven researchers but also reviewers, area chairs, and entire conferences. However, we do not believe the role of a human scientist will be diminished. We expect the role of scientists will change as we adapt to new technology, and they will be empowered to tackle more ambitious goals. For instance, researchers often have more ideas than they have time to pursue, what if The AI Scientist could take the first explorations on all of them?",
                    "quote": []
                },
                {
                    "text": "While the current iteration of The AI Scientist demonstrates a strong ability to innovate on top of well-established ideas, such as Diffusion Modeling or Transformers, it is an open question whether such systems can ultimately propose genuinely paradigm-shifting ideas. Will future versions of The AI Scientist be capable of proposing ideas as impactful as Diffusion Modeling, or come up with the next Transformer architecture? Will machines ultimately be able to invent concepts as fundamental as the artificial neural network, or information theory? We believe The AI Scientist will make a great companion to human scientists, but only time will tell to the extent to which the nature of human creativity and our moments of serendipitous innovation can be replicated by an open-ended discovery process conducted by artificial agents.",
                    "quote": []
                }
            ]
        }
    ],
    "reference": [
        {
            "index": "b0",
            "title": "Meta-learning curiosity algorithms",
            "author": [
                {
                    "forename": "Ferran",
                    "surname": "Alet",
                    "name": "Ferran Alet",
                    "email": ""
                },
                {
                    "forename": "Martin F",
                    "surname": "Schneider",
                    "name": "Martin F Schneider",
                    "email": ""
                },
                {
                    "forename": "Tomas",
                    "surname": "Lozano-Perez",
                    "name": "Tomas Lozano-Perez",
                    "email": ""
                },
                {
                    "forename": "Leslie Pack",
                    "surname": "Kaelbling",
                    "name": "Leslie Pack Kaelbling",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "arXiv preprint arXiv:2003.05325",
            "date": "2020"
        },
        {
            "index": "b1",
            "title": "Artificial intelligence in scientific writing: a friend or a foe?",
            "author": [
                {
                    "forename": "Signe",
                    "surname": "Altmäe",
                    "name": "Signe Altmäe",
                    "email": ""
                },
                {
                    "forename": "Alberto",
                    "surname": "Sola-Leyva",
                    "name": "Alberto Sola-Leyva",
                    "email": ""
                },
                {
                    "forename": "Andres",
                    "surname": "Salumets",
                    "name": "Andres Salumets",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "Reproductive BioMedicine Online",
            "date": "2023"
        },
        {
            "index": "b2",
            "title": "Model card and evaluations for claude models",
            "author": [
                {
                    "forename": "",
                    "surname": "Anthropic",
                    "name": "Anthropic",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "",
            "date": "2023"
        },
        {
            "index": "b3",
            "title": "The claude 3 model family: Opus, sonnet, haiku",
            "author": [
                {
                    "forename": "",
                    "surname": "Anthropic",
                    "name": "Anthropic",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "",
            "date": "2024"
        },
        {
            "index": "b4",
            "title": "Cloud labs: where robots do the research",
            "author": [
                {
                    "forename": "Carrie",
                    "surname": "Arnold",
                    "name": "Carrie Arnold",
                    "email": ""
                }
            ],
            "doi": "",
            "venue": "Nature",
            "date": "2022"
        }
    ]
}